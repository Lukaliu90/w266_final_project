{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea4333-9dc0-4384-a2cd-c715b132b1d7",
   "metadata": {},
   "source": [
    "# Multi XScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1ee7c-5e35-45c4-8082-46c5ee7a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "## For printing out model summary in PyTorch\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8882d8a6-5dd4-4bd9-8599-c15840ef1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920814-72af-4d58-91bd-aa7dcccd5160",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d1a000-8e57-4144-9253-2779a6dc7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\AppData\\Local\\Temp\\ipykernel_7476\\4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f359-5379-4d2b-8b24-51563c9a247f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17fce3f-f4ff-4ac9-93c9-4a33463d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (C:/Users/JustinTo/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e0e17b0b934291ad6f801ffe46369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24ed-be47-4064-b363-bf872caee6de",
   "metadata": {},
   "source": [
    "## Format dataset to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57be3155-bad5-46d2-953c-3469e2ec2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e9e334-0a0b-4283-b68a-af7bfc46fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = (\n",
    "        example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "        + DOC_SEP\n",
    "        + DOC_SEP.join([x for x in example[\"ref_abstract\"][\"abstract\"] if x])\n",
    "    )\n",
    "    output[\"related_work\"] = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44487cf5-f19d-4d85-a28e-db276fb87df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c94c5ed-4ae3-4940-8523-d988860354ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_dataset_batched at 0x000001E7D53ADAF0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f4ea1797584d61b5cb8ef5bb7d1ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/475 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f246d4ae22430baefed24bfed35830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bd04a01b6e4f21a4b18cd74b62f4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_processed = {}\n",
    "for split in dataset.keys():\n",
    "    dataset_processed[split] = dataset[split].map(\n",
    "        # preprocess_dataset,\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[split].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab3779-c46c-4e93-bdbf-8dc682f1bca8",
   "metadata": {},
   "source": [
    "## Model 1: Default Centrum\n",
    "\n",
    "* Probably need to figure out the distribution of `dataset_processed[\"test\"][\"abstracts\"]` so that we can estimate the best `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328b32ec-684e-4756-94f4-2ee1b01e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c4d3644-e019-42a3-b375-2ff7636ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(host_tokenizer: str):\n",
    "  \"\"\"return the tokenizer and model for LLM training\"\"\"\n",
    "\n",
    "  return (AutoTokenizer.from_pretrained(host_tokenizer, \n",
    "                                        use_cache=False, \n",
    "                                        gradient_checkpointing=True), \n",
    "          AutoModelForSeq2SeqLM.from_pretrained(host_tokenizer, \n",
    "                                                use_cache=False, \n",
    "                                                gradient_checkpointing=True).to(\"cuda\").half())\n",
    "\n",
    "\n",
    "centrum_tokenizer, centrum_model = get_tokenizer(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d4aae9-cdfa-4e85-b494-616ba2f336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrum_tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "centrum_model.resize_token_embeddings(len(centrum_tokenizer))\n",
    "docsep_token_id = centrum_tokenizer.convert_tokens_to_ids(DOC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a9c9a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='ratishsp/Centrum', vocab_size=50265, model_max_len=16384, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrum_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "566b3a70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGeneration(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50267, 768)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50267, 768)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(4096, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50267, 768)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50267, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centrum_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330c855",
   "metadata": {},
   "source": [
    "## Token Length 1024 (no_repeat_ngram = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3acc1cce-7af5-43a4-8bb7-8994bb88bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = {}\n",
    "\n",
    "dataset_tokenized[\"test\"] = centrum_tokenizer(\n",
    "    dataset_processed[\"test\"][\"abstracts\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68868234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract_batched(batch_size=2, start=0, no_repeat_ngram_size=4, max_length=256):\n",
    "    \n",
    "    try:\n",
    "        del test_input_ids, attention_mask, global_attention_mask, predicted_abstract_ids\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    gc.collect()\n",
    "\n",
    "    test_input_ids = dataset_tokenized['test']['input_ids'][start:start+batch_size].to(\"cuda\")\n",
    "    attention_mask = dataset_tokenized['test']['attention_mask'][start:start+batch_size].to(\"cuda\")\n",
    "\n",
    "    global_attention_mask = (test_input_ids == centrum_tokenizer.cls_token_id) | (test_input_ids == docsep_token_id)\n",
    "\n",
    "    predicted_abstract_ids = centrum_model.generate(test_input_ids,\n",
    "                                                    attention_mask=attention_mask, \n",
    "                                                    global_attention_mask=global_attention_mask, \n",
    "                                                    max_length=max_length,\n",
    "                                                    no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                                    num_beams=4)\n",
    "\n",
    "    predicted_abstract = centrum_tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7f0c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 500 now..\n",
      "Handling sample 1000 now..\n",
      "Handling sample 1500 now..\n",
      "Handling sample 2000 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 3000 now..\n",
      "Handling sample 3500 now..\n",
      "Handling sample 4000 now..\n",
      "Handling sample 4500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed, 5093 data points handled.\n"
     ]
    }
   ],
   "source": [
    "## Generating answers\n",
    "test_batch_size = 2\n",
    "no_repeat_ngram_size = 4\n",
    "answers_fixed = []\n",
    "\n",
    "for i in range(0, dataset_processed['test'].num_rows, test_batch_size):\n",
    "    if i%500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    answers_fixed.append(generate_abstract_batched(start=i,\n",
    "                                                   batch_size=test_batch_size,\n",
    "                                                   no_repeat_ngram_size=no_repeat_ngram_size))\n",
    "    \n",
    "print(f\"Completed, {i+1} data points handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9030dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_answers_fixed = []\n",
    "for answer in answers_fixed:\n",
    "    formatted_answers_fixed += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"answers_revised/baselines/Centrum_1024tokens_norepeat4.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_answers_fixed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0582667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the rouge score\n",
    "metric_norepeat4 = rouge.compute(predictions=formatted_answers_fixed,\n",
    "                                 references=[ref for ref in dataset_processed['test']['related_work']],\n",
    "                                 use_stemmer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e18b4675",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2265826960587613, recall=0.43772636218005384, fmeasure=0.2864612831459573), mid=Score(precision=0.22894222834918954, recall=0.44021451741739237, fmeasure=0.2886998899529029), high=Score(precision=0.23152466853877887, recall=0.4427925426116824, fmeasure=0.2909696381737882)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.040909721576971134, recall=0.08008098039670647, fmeasure=0.05185027455132552), mid=Score(precision=0.04179481992509422, recall=0.08173496474357636, fmeasure=0.05290083284921959), high=Score(precision=0.04269202893554369, recall=0.08341225044733727, fmeasure=0.053954731610736106)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.11227537986637294, recall=0.2250679160619483, fmeasure=0.14342061182623692), mid=Score(precision=0.11336789988753487, recall=0.22708980488324895, fmeasure=0.14446791950329646), high=Score(precision=0.1144758750227722, recall=0.22907433802060162, fmeasure=0.14553273116036605)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.11227938607611768, recall=0.22519633134584496, fmeasure=0.14340378973514734), mid=Score(precision=0.11329674420806189, recall=0.22707317589440068, fmeasure=0.14440933969659597), high=Score(precision=0.11438707829615637, recall=0.22882222437739166, fmeasure=0.14543273480572436))}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_norepeat4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5bcd7",
   "metadata": {},
   "source": [
    "## Token Length 1024 (no_repeat_ngram = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b54154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 500 now..\n",
      "Handling sample 1000 now..\n",
      "Handling sample 1500 now..\n",
      "Handling sample 2000 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 3000 now..\n",
      "Handling sample 3500 now..\n",
      "Handling sample 4000 now..\n",
      "Handling sample 4500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed, 5093 data points handled.\n"
     ]
    }
   ],
   "source": [
    "## Generating answers\n",
    "test_batch_size = 2\n",
    "no_repeat_ngram_size = 3\n",
    "answers_fixed = []\n",
    "\n",
    "for i in range(0, dataset_processed['test'].num_rows, test_batch_size):\n",
    "    if i%500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    answers_fixed.append(generate_abstract_batched(start=i,\n",
    "                                                   batch_size=test_batch_size,\n",
    "                                                   no_repeat_ngram_size=no_repeat_ngram_size))\n",
    "    \n",
    "print(f\"Completed, {i+1} data points handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a9d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_answers_fixed = []\n",
    "for answer in answers_fixed:\n",
    "    formatted_answers_fixed += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"answers_revised/baselines/Centrum_1024tokens_norepeat3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_answers_fixed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "788bc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the rouge score\n",
    "metric_norepeat3 = rouge.compute(predictions=formatted_answers_fixed,\n",
    "                                 references=[ref for ref in dataset_processed['test']['related_work']],\n",
    "                                 use_stemmer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "956a268c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.22772499301159588, recall=0.4390357876855743, fmeasure=0.2875707196492456), mid=Score(precision=0.2300670823131827, recall=0.4417820787237462, fmeasure=0.2897487979471861), high=Score(precision=0.23261393435265582, recall=0.4442250579937219, fmeasure=0.29214149159364144)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.04077113757375007, recall=0.07959749558397478, fmeasure=0.05154900659003259), mid=Score(precision=0.041657608261410364, recall=0.08130459685209915, fmeasure=0.05265059264335756), high=Score(precision=0.04254403465352925, recall=0.08299856187724529, fmeasure=0.05373046044217472)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.11234791749286922, recall=0.22510530992302918, fmeasure=0.14332531130561366), mid=Score(precision=0.11339646233716678, recall=0.22706235943254338, fmeasure=0.14431975307778994), high=Score(precision=0.11441829800428202, recall=0.22891465102910727, fmeasure=0.14535622627126812)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.11227152901962982, recall=0.22525628269033263, fmeasure=0.14327097160715616), mid=Score(precision=0.11338961626360788, recall=0.2269513644926014, fmeasure=0.14430399563601576), high=Score(precision=0.11442618463333015, recall=0.22879018665826412, fmeasure=0.1453472418719076))}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_norepeat3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a28890",
   "metadata": {},
   "source": [
    "## Token length 4096 (Centrum does not allow 16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69b23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized_large = {}\n",
    "\n",
    "dataset_tokenized_large[\"test\"] = centrum_tokenizer(\n",
    "    dataset_processed[\"test\"][\"abstracts\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bbd65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract_batched2(batch_size=2, start=0, no_repeat_ngram_size=4, max_length=256):\n",
    "    \n",
    "    try:\n",
    "        del test_input_ids, attention_mask, global_attention_mask, predicted_abstract_ids\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    gc.collect()\n",
    "\n",
    "    test_input_ids = dataset_tokenized_large['test']['input_ids'][start:start+batch_size].to(\"cuda\")\n",
    "    attention_mask = dataset_tokenized_large['test']['attention_mask'][start:start+batch_size].to(\"cuda\")\n",
    "\n",
    "    global_attention_mask = (test_input_ids == centrum_tokenizer.cls_token_id) | (test_input_ids == docsep_token_id)\n",
    "\n",
    "    predicted_abstract_ids = centrum_model.generate(test_input_ids,\n",
    "                                                    attention_mask=attention_mask, \n",
    "                                                    global_attention_mask=global_attention_mask, \n",
    "                                                    max_length=max_length,\n",
    "                                                    no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                                    num_beams=4)\n",
    "\n",
    "    predicted_abstract = centrum_tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1563109d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 500 now..\n",
      "Handling sample 1000 now..\n",
      "Handling sample 1500 now..\n",
      "Handling sample 2000 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 3000 now..\n",
      "Handling sample 3500 now..\n",
      "Handling sample 4000 now..\n",
      "Handling sample 4500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed, 5093 data points handled.\n"
     ]
    }
   ],
   "source": [
    "## Generating answers\n",
    "test_batch_size = 1\n",
    "no_repeat_ngram_size = 4\n",
    "answers_fixed = []\n",
    "\n",
    "for i in range(0, dataset_processed['test'].num_rows, test_batch_size):\n",
    "    if i%500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    answers_fixed.append(generate_abstract_batched2(start=i,\n",
    "                                                    batch_size=test_batch_size,\n",
    "                                                    no_repeat_ngram_size=no_repeat_ngram_size))\n",
    "    \n",
    "print(f\"Completed, {i+1} data points handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d95bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_answers_fixed = []\n",
    "for answer in answers_fixed:\n",
    "    formatted_answers_fixed += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"answers_revised/baselines/Centrum_4096tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_answers_fixed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ad182b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the rouge score\n",
    "metric_4096tokens = rouge.compute(predictions=formatted_answers_fixed,\n",
    "                                  references=[ref for ref in dataset_processed['test']['related_work']],\n",
    "                                  use_stemmer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ece17215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.22612544658583159, recall=0.4361118833312366, fmeasure=0.2856490787565765), mid=Score(precision=0.22838993642640357, recall=0.4386777508413613, fmeasure=0.28776083468712477), high=Score(precision=0.2308064347985721, recall=0.44139529892575774, fmeasure=0.289934362041135)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.040538178391298994, recall=0.07943614423251627, fmeasure=0.051375506410180134), mid=Score(precision=0.04139440297278668, recall=0.08112594717471461, fmeasure=0.05244024798225367), high=Score(precision=0.04230090098514175, recall=0.08282356336360785, fmeasure=0.05353833157856666)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.1124225601329699, recall=0.22517898089951363, fmeasure=0.14339040973864686), mid=Score(precision=0.11341719273858765, recall=0.22713309798623907, fmeasure=0.14441935599279224), high=Score(precision=0.11456579383649804, recall=0.2289604324736872, fmeasure=0.14545339988984682)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.11227161050611298, recall=0.22524420171879592, fmeasure=0.14329049285748952), mid=Score(precision=0.11337952960656272, recall=0.2270635667922466, fmeasure=0.14436838844048822), high=Score(precision=0.11447246385587467, recall=0.22900904102919156, fmeasure=0.14541670597676012))}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_4096tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91f388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior. This article examines the implications of such an agent tracking capability for agent architectures. It specifically focuses on real-time and dynamic environments, where an intelligent agent is faced with the challenge of tracking the highly flexible mix of goal-driven and reactive behaviors of other agents, in real-time. The key implication is that an agent architecture needs to provide direct support for flexible and efficient reasoning about other agents' models. In this article, such support takes the form of an architectural capability to execute the other agent's models, enabling mental simulation of\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_answers_fixed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf61fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Within the MAS community, some work @cite has focused on how artificial AI-based learning agents would fare in communities of similar agents. For example, @cite and @cite show how agents can learn the capabilities of others via repeated interactions, but these agents do not learn to predict what actions other might take. Most of the work in MAS also fails to recognize the possible gains from using explicit agent models to predict agent actions. @cite is an exception and gives another approach for using nested agent models. However, they do not go so far as to try to quantify the advantages of their nested models or show how these could be learned via observations. We believe that our research will bring to the foreground some of the common observations seen in these research areas and help to clarify the implications and utility of learning and using nested agent models.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed['test']['related_work'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5782e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior. ||||| In multi-agent environments, an intelligent agent often needs to interact with other individuals or groups of agents to achieve its goals. Agent tracking is one key capability required for intelligent interaction. It involves monitoring the observable actions of other agents and inferring their unobserved actions, plans, goals and behaviors. This article examines the implications of such an agent tracking capability for agent architectures. It specifically focuses on real-time and dynamic environments, where an intelligent agent is faced with the challenge of tracking the highly flexible mix of goal-driven and reactive behaviors of other agents, in real-time. The key implication is that an agent architecture needs to provide direct support for flexible and efficient reasoning about other agents' models. In this article, such support takes the form of an architectural capability to execute the other agent's models, enabling mental simulation of their behaviors. Other architectural requirements that follow include the capabilities for (pseudo-) simultaneous execution of multiple agent models, dynamic sharing and unsharing of multiple agent models and high bandwidth inter-model communication. We have implemented an agent architecture, an experimental variant of the Soar integrated architecture, that conforms to all of these requirements. Agents based on this architecture have been implemented to execute two different tasks in a real-time, dynamic, multi-agent domain. The article presents experimental results illustrating the agents' dynamic behavior. ||||| I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV. Counteracting institutions, 499. â€” V. Conclusion, 500. ||||| The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed['test']['abstracts'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
