{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb77c69",
   "metadata": {
    "id": "e-k1jXQHLyFU"
   },
   "source": [
    "# W266 Final Project - Qualitative Analysis of Generated Summaries\n",
    "\n",
    "**Description:** \n",
    "\n",
    "- This notebook generations 60 samples for qualitative analysis\n",
    "- Results are set out in a separate Word document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1e41c",
   "metadata": {},
   "source": [
    "# 1. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60a4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pprint import pprint\n",
    "\n",
    "## General plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "## Managing memory\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "## Text processing\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdc57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a85258ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\AppData\\Local\\Temp\\ipykernel_23416\\4286193646.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "## Loading rouge\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b242a",
   "metadata": {},
   "source": [
    "# 2. Loading X-Science Dataset (Test Set only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a654d78",
   "metadata": {},
   "source": [
    "## 2.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7079f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking if GPU is available when running locally\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e460ecf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (C:/Users/JustinTo/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    }
   ],
   "source": [
    "## Loading the dataset\n",
    "xsci_test = load_dataset('multi_x_science_sum', split='test')\n",
    "\n",
    "## For text processing as X-Science have not concatenated the source articles\n",
    "DOC_SEP = \" ||||| \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb5528",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255151e",
   "metadata": {},
   "source": [
    "- Tokenization is not necessary as all the answers from models/baseline to be compared to the test labels are already in text form.\n",
    "- So, we only need to pre-process the X-Science dataset labels to the form we want, e.g. changing the citation numbers to @cite, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6003c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b3261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = (\n",
    "        example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "        + DOC_SEP\n",
    "        + DOC_SEP.join([x for x in example[\"ref_abstract\"][\"abstract\"] if x])\n",
    "    )\n",
    "    output[\"related_work\"] = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88b9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    output[\"main_article\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "        \n",
    "        # Main article added for calculating the degree of copying\n",
    "        output[\"main_article\"].append(abstract)\n",
    "        \n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63adaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_dataset_batched at 0x0000016AB9340700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91488753231148e7bbd5aa504029620a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5093 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xsci_test_processed = xsci_test.map(\n",
    "    # preprocess_dataset,\n",
    "    preprocess_dataset_batched,\n",
    "    remove_columns=xsci_test.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af64184",
   "metadata": {},
   "source": [
    "## 2.3 Separating the Dataset into Short, Medium & Long Input Length Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b965daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokenizer(host_tokenizer: str):\n",
    "    \"\"\"return the tokenizer and model for LLM training\"\"\"\n",
    "\n",
    "    return AutoTokenizer.from_pretrained(host_tokenizer, \n",
    "                                         use_cache=False, \n",
    "                                         gradient_checkpointing=True)\n",
    "\n",
    "\n",
    "centrum_tokenizer = get_tokenizer(\"ratishsp/Centrum\")\n",
    "\n",
    "centrum_tokenizer.add_tokens(DOC_SEP, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96dd295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, number of samples processed is: 5093\n"
     ]
    }
   ],
   "source": [
    "## Estimating the token length of the test data set\n",
    "len_inputs = []\n",
    "len_labels = []\n",
    "num_articles = []\n",
    "\n",
    "for sample in xsci_test_processed['abstracts']:\n",
    "    temp = centrum_tokenizer(sample, return_tensors=\"pt\")\n",
    "    len_inputs.append(temp.input_ids.shape[1])\n",
    "    num_articles.append((sample.count(DOC_SEP)+1))\n",
    "\n",
    "for sample in xsci_test_processed['related_work']:\n",
    "    temp = centrum_tokenizer(sample, return_tensors=\"pt\")\n",
    "    len_labels.append(temp.input_ids.shape[1])\n",
    "                        \n",
    "assert len(len_inputs) == len(len_labels) == len(num_articles)\n",
    "\n",
    "print(f\"Completed, number of samples processed is: {len(len_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5230b4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quartiles in terms of input lengths are: (486.0, 735.0, 1150.0)\n",
      "The quartiles in terms of label lengths are: (96.0, 138.0, 184.0)\n",
      "The quartiles in terms of number of articles are: (2.0, 4.0, 6.0)\n"
     ]
    }
   ],
   "source": [
    "## Showing quartiles\n",
    "q1 = np.quantile(len_inputs, [0,0.25,0.5,0.75,1])\n",
    "q2 = np.quantile(len_labels, [0,0.25,0.5,0.75,1])\n",
    "q3 = np.quantile(num_articles, [0,0.25,0.5,0.75,1])\n",
    "\n",
    "print(f\"The quartiles in terms of input lengths are: {(q1[1], q1[2], q1[3])}\")\n",
    "print(f\"The quartiles in terms of label lengths are: {(q2[1], q2[2], q2[3])}\")\n",
    "print(f\"The quartiles in terms of number of articles are: {(q3[1], q3[2], q3[3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca99144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(range(5094))[np.where(len_inputs < q1[1])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2aaf3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 1273\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 2546\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 1274\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Breaking up test set into three parts: short (<lower quartile); medium (in between quartiles); long (>upper quartile)\n",
    "short_samples = xsci_test_processed.select(np.where(len_inputs < q1[1])[0])\n",
    "medium_samples = xsci_test_processed.select(np.where(np.logical_and(q1[1]<=len_inputs, len_inputs < q1[3]))[0])\n",
    "long_samples = xsci_test_processed.select(np.where(len_inputs >= q1[3])[0])\n",
    "\n",
    "short_samples, medium_samples, long_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c4e473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short': array([   1,    2,    5, ..., 5085, 5086, 5090]),\n",
       " 'medium': array([   0,    3,    4, ..., 5088, 5089, 5091]),\n",
       " 'long': array([   9,   21,   22, ..., 5067, 5079, 5092])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Indice for mapping the index of the subdata sets back to the original\n",
    "indices = {\n",
    "    'short':  np.array(range(5094))[np.where(len_inputs < q1[1])[0]],\n",
    "    'medium': np.array(range(5094))[np.where(np.logical_and(q1[1]<=len_inputs, len_inputs < q1[3]))],\n",
    "    'long':   np.array(range(5094))[np.where(len_inputs >= q1[3])]\n",
    "}\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423526f",
   "metadata": {},
   "source": [
    "## 2.4 Selecting random samples (20 samples each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19927436",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20230409)  # Random seed\n",
    "\n",
    "random_arrays = {\n",
    "    'short':  np.sort(np.random.choice(range(len(indices['short'])), size=20, replace=False)),\n",
    "    'medium': np.sort(np.random.choice(range(len(indices['medium'])), size=20, replace=False)),\n",
    "    'long':   np.sort(np.random.choice(range(len(indices['long'])), size=20, replace=False))\n",
    "}\n",
    "\n",
    "selected_samples = {\n",
    "    'short':  short_samples.select(random_arrays['short']),\n",
    "    'medium': medium_samples.select(random_arrays['medium']),\n",
    "    'long':   long_samples.select(random_arrays['long'])\n",
    "}\n",
    "\n",
    "original_indices = {\n",
    "    'short':  indices['short'][random_arrays['short']],\n",
    "    'medium': indices['medium'][random_arrays['medium']],\n",
    "    'long':   indices['long'][random_arrays['long']]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b601e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short': Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 20\n",
       " }),\n",
       " 'medium': Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 20\n",
       " }),\n",
       " 'long': Dataset({\n",
       "     features: ['related_work', 'abstracts', 'main_article'],\n",
       "     num_rows: 20\n",
       " })}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9222e169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short': array([  43,  483,  556,  599,  620,  686,  828, 1127, 1816, 2179, 2317,\n",
       "        2536, 3027, 3036, 3130, 3157, 3524, 4160, 4191, 4820]),\n",
       " 'medium': array([ 235,  638,  831,  845, 1165, 1542, 1894, 2011, 2697, 2846, 2916,\n",
       "        3404, 3419, 3771, 4046, 4263, 4371, 4717, 4858, 5068]),\n",
       " 'long': array([  76,  485, 1368, 1864, 1872, 1916, 2333, 2542, 2563, 2698, 2741,\n",
       "        2914, 3010, 3045, 3247, 3739, 4038, 4261, 4510, 4713])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "496b7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving results for easier running\n",
    "with open(\"misc_data/random_samples_qualitative_analysis.pkl\", \"wb\") as f:\n",
    "    pickle.dump((original_indices, selected_samples), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da63aae",
   "metadata": {},
   "source": [
    "# 3. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cddd0",
   "metadata": {},
   "source": [
    "## 3.1 Loading Generated Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ffe1f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary mapping to file locations of pickled summaries\n",
    "filepaths = {\n",
    "    'Base LED (16k)':  \"answers_revised/baselines/LED_base_16384tokens.pkl\",\n",
    "    'Large LED (16k)': \"answers_revised/baselines/LED_large_16384tokens.pkl\",\n",
    "    'Centrum (4k)':    \"answers_revised/baselines/Centrum_4096tokens.pkl\",\n",
    "    'Finetuned LED':   \"answers_revised/epoch2/LED_xsci_finetuned_run10.pkl\",\n",
    "    'Finetuned Cent':  \"answers_revised/centrum/Centrum_finetuned_norepeat4_run2.pkl\",\n",
    "    '2-Step (Cent)':   \"misc_data/XSci_test_2step_CENTRUM.pkl\"\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "for model, path in filepaths.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        model_results[model] = pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10de7036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Base LED (16k)', 'Large LED (16k)', 'Centrum (4k)', 'Finetuned LED', 'Finetuned Cent', '2-Step (Cent)'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc07bf",
   "metadata": {},
   "source": [
    "## 3.2 Function for Printing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "61e54552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_results(subset, subset_index):\n",
    "    \n",
    "    original_index = int(original_indices[subset][subset_index])\n",
    "    \n",
    "    print(\"\\033[1m\" + f\"-----Showing Results for: {subset.capitalize()} Sample; Number {subset_index}-----\\n\" + \"\\033[0m\")\n",
    "    print(\"\\033[1m\" + \"Original index: \" + \"\\033[0m\" + f\"{original_index}\")\n",
    "    \n",
    "    ## Abstracts\n",
    "    temp = xsci_test_processed[original_index]['abstracts'].split(DOC_SEP)\n",
    "    print(\"\\033[1m\" + \"\\nAbstracts:\" + \"\\033[0m\")\n",
    "    for idx, abstract in enumerate(temp):\n",
    "        print(\"\\033[1m\" + f\"({idx+1}):\" + \"\\033[0m\")\n",
    "        print(temp[idx])\n",
    "    \n",
    "    ## Labels\n",
    "    print(\"\\033[1m\" + \"\\nLabel:\" + \"\\033[0m\")\n",
    "    print(xsci_test_processed[original_index]['related_work'])\n",
    "    \n",
    "    ## Model Summaries\n",
    "    for model, summary in model_results.items():\n",
    "        print(\"\\033[1m\" + f\"\\n{model}:\" + \"\\033[0m\")\n",
    "        \n",
    "        score = rouge.compute(predictions=[summary[original_index]],\n",
    "                              references=[xsci_test_processed[original_index]['related_work']],\n",
    "                              rouge_types=[\"rouge2\", \"rougeL\"],\n",
    "                              use_stemmer = True)\n",
    "        print(\"\\033[1m\" + \"Rouge scores:\" + \"\\033[0m\")\n",
    "        print(f\"- Rouge 2:\\n {round(score['rouge2'].mid.precision, 4)} (prec) \\n \" +\n",
    "              f\"{round(score['rouge2'].mid.recall, 4)} (recall) \\n \" +\n",
    "              f\"{round(score['rouge2'].mid.fmeasure, 4)} (f-1)\")\n",
    "        print(f\"- Rouge L:\\n {round(score['rougeL'].mid.precision, 4)} (prec) \\n \" +\n",
    "              f\"{round(score['rougeL'].mid.recall, 4)} (recall) \\n \" +\n",
    "              f\"{round(score['rougeL'].mid.fmeasure, 4)} (f-1)\")\n",
    "        print(\"\\033[1m\" + \"Summary:\" + \"\\033[0m\")\n",
    "        print(summary[original_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2dfa7",
   "metadata": {},
   "source": [
    "## 3.3 Showing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9dc1f1e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m-----Showing Results for: Long Sample; Number 19-----\n",
      "\u001b[0m\n",
      "\u001b[1mOriginal index: \u001b[0m4713\n",
      "\u001b[1m\n",
      "Abstracts:\u001b[0m\n",
      "\u001b[1m(1):\u001b[0m\n",
      "Gabor filters (GFs) play an important role in many application areas for the enhancement of various types of images and the extraction of Gabor features. For the purpose of enhancing curved structures in noisy images, we introduce curved GFs that locally adapt their shape to the direction of flow. These curved GFs enable the choice of filter parameters that increase the smoothing power without creating artifacts in the enhanced image. In this paper, curved GFs are applied to the curved ridge and valley structures of low-quality fingerprint images. First, we combine two orientation-field estimation methods in order to obtain a more robust estimation for very noisy images. Next, curved regions are constructed by following the respective local orientation. Subsequently, these curved regions are used for estimating the local ridge frequency. Finally, curved GFs are defined based on curved regions, and they apply the previously estimated orientations and ridge frequencies for the enhancement of low-quality fingerprint images. Experimental results on the FVC2004 databases show improvements of this approach in comparison with state-of-the-art enhancement methods.\n",
      "\u001b[1m(2):\u001b[0m\n",
      "We propose a Gabor-filter-based method for fingerprint recognition in this paper. The method makes use of Gabor filtering technologies and need only to do the core point detection before the feature extraction process without any other pre-processing steps such as smoothing, binarization, thinning, and minutiae detection. The proposed Gabor-filter-based features play a central role in the processes of fingerprint recognition, including local ridge orientation, core point detection, and feature extraction. Experimental results show that the recognition rate of the k-nearest neighbor classifier using the proposed features is 97.2 for a small-scale fingerprint database, and thus that the proposed method is an efficient and reliable approach.\n",
      "\u001b[1m(3):\u001b[0m\n",
      "Fingerprint classification provides an important indexing mechanism in a fingerprint database. An accurate and consistent classification can greatly reduce fingerprint matching time for a large database. We present a fingerprint classification algorithm which is able to achieve an accuracy better than previously reported in the literature. We classify fingerprints into five categories: whorl, right loop, left loop, arch, and tented arch. The algorithm uses a novel representation (FingerCode) and is based on a two-stage classifier to make a classification. It has been tested on 4000 images in the NIST-4 database. For the five-class problem, a classification accuracy of 90 percent is achieved (with a 1.8 percent rejection during the feature extraction phase). For the four-class problem (arch and tented arch combined into one class), we are able to achieve a classification accuracy of 94.8 percent (with 1.8 percent rejection). By incorporating a reject option at the classifier, the classification accuracy can be increased to 96 percent for the five-class classification task, and to 97.8 percent for the four-class classification task after a total of 32.5 percent of the images are rejected.\n",
      "\u001b[1m(4):\u001b[0m\n",
      "In order to ensure that the performance of an automatic fingerprint identification verification system will be robust with respect to the quality of input fingerprint images, it is essential to incorporate a fingerprint enhancement algorithm in the minutiae extraction module. We present a fast fingerprint enhancement algorithm, which can adaptively improve the clarity of ridge and valley structures of input fingerprint images based on the estimated local ridge orientation and frequency. We have evaluated the performance of the image enhancement algorithm using the goodness index of the extracted minutiae and the accuracy of an online fingerprint verification system. Experimental results show that incorporating the enhancement algorithm improves both the goodness index and the verification accuracy.\n",
      "\u001b[1m(5):\u001b[0m\n",
      "Introduces a method for the generation of synthetic fingerprint images. Gabor-like space-variant filters are used for iteratively expanding an initially empty image containing just one or a few seeds. A directional image model, whose inputs are the number and location of the fingerprint cores and deltas, is used for tuning the filters according to the underlying ridge orientation. Very realistic fingerprint images are obtained after the final noising-and-rendering stage.\n",
      "\u001b[1m(6):\u001b[0m\n",
      "Biometrics-based verification, especially fingerprint-based identification, is receiving a lot of attention. There are two major shortcomings of the traditional approaches to fingerprint representation. For a considerable fraction of population, the representations based on explicit detection of complete ridge structures in the fingerprint are difficult to extract automatically. The widely used minutiae-based representation does not utilize a significant component of the rich discriminatory information available in the fingerprints. Local ridge structures cannot be completely characterized by minutiae. Further, minutiae-based matching has difficulty in quickly matching two fingerprint images containing a different number of unregistered minutiae points. The proposed filter-based algorithm uses a bank of Gabor filters to capture both local and global details in a fingerprint as a compact fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the two corresponding FingerCodes and hence is extremely fast. We are able to achieve a verification accuracy which is only marginally inferior to the best results of minutiae-based algorithms published in the open literature. Our system performs better than a state-of-the-art minutiae-based system when the performance requirement of the application system does not demand a very low false acceptance rate. Finally, we show that the matching performance can be improved by combining the decisions of the matchers based on complementary (minutiae-based and filter-based) fingerprint information.\n",
      "\u001b[1m(7):\u001b[0m\n",
      "An important step in fingerprint recognition is the segmentation of the region of interest. In this paper, we present an enhanced approach for fingerprint segmentation based on the response of eight oriented Gabor filters. The performance of the algorithm has been evaluated in terms of decision error trade-off curves of an overall verification system. Experimental results demonstrate the robustness of the proposed method.\n",
      "\u001b[1m\n",
      "Label:\u001b[0m\n",
      "Gabor filterbanks are used for the segmentation @cite and quality estimation @cite of fingerprint images, for core point estimation @cite , classification @cite and fingerprint matching based on Gabor features @cite @cite . GFs are also employed for generating synthetic fingerprints @cite . The use of GF for fingerprint image enhancement was introduced in @cite .\n",
      "\u001b[1m\n",
      "Base LED (16k):\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0236 (prec) \n",
      " 0.0588 (recall) \n",
      " 0.0337 (f-1)\n",
      "- Rouge L:\n",
      " 0.0859 (prec) \n",
      " 0.2115 (recall) \n",
      " 0.1222 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      "based algorithms published in the open literature. Our system performs better than a state-of-the-art minutiae-based system when the performance requirement of the application system does not demand a very low false acceptance rate. Finally, we show that the matching performance can be improved by combining the decisions of the matchers based on complementary (minutiae- based and filter-based) fingerprint information.|||||An important step in fingerprint recognition is the segmentation of the region of interest. In this paper, we present an enhanced approach for fingerprint segmentation based on the response of eight oriented Gabor filters. The performance of the algorithm has been evaluated in terms of decision error trade-off curves of an overall verification system. Experimental results demonstrate the robustness of the proposed method.\n",
      "\u001b[1m\n",
      "Large LED (16k):\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0423 (prec) \n",
      " 0.1176 (recall) \n",
      " 0.0622 (f-1)\n",
      "- Rouge L:\n",
      " 0.1259 (prec) \n",
      " 0.3462 (recall) \n",
      " 0.1846 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      " in this paper, we present a fast fingerprint enhancement algorithm, which can adaptively improve the clarity of ridge and valley structures of input fingerprint images based on the estimated local ridge orientation and frequency. \n",
      " the proposed fingerprint enhancement algorithm is based on the response of eight oriented space-variant filters, which are used for iteratively expanding an initially empty image containing just one or a few seeds. \n",
      " a directional image model, whose inputs are the number and location of the fingerprint cores and deltas, is used for tuning the filters according to the underlying ridge orientation. \n",
      " finally, curved filters are defined based on curved regions, and they apply the previously estimated orientations and ridge frequencies for the enhancement of low-quality fingerprint images. \n",
      " experimental results on the fVC2004 databases show improvements of this approach in comparison with state-of-the-art enhancement methods. \n",
      "\u001b[1m\n",
      "Centrum (4k):\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0145 (prec) \n",
      " 0.0588 (recall) \n",
      " 0.0233 (f-1)\n",
      "- Rouge L:\n",
      " 0.0769 (prec) \n",
      " 0.3077 (recall) \n",
      " 0.1231 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      "Biometrics-based verification, especially fingerprint-based identification, is receiving a lot of attention. There are two major shortcomings of the traditional approaches to fingerprint representation. For a considerable fraction of population, the representations based on explicit detection of complete ridge structures in the fingerprint are difficult to extract automatically. The widely used minutiae-based representation does not utilize a significant component of the rich discriminatory information available in the fingerprints. Local ridge structures cannot be completely characterized by minutiae. Further, minutae-based matching has difficulty in quickly matching two fingerprint images containing a different number of unregistered minutiae points. The proposed filter-based algorithm uses a bank of Gabor filters to capture both local and global details in a fingerprint as a compact fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the two corresponding FingerCodes and hence is extremely fast. We are able to achieve a verification accuracy which is only marginally inferior to the best results of minutiaeâ€“based algorithms published in the open literature. Our system performs better than a state-of-the-art minutia-based system when the performance requirement of the application system does not demand a very low false acceptance rate. Finally, we show that the matching performance\n",
      "\u001b[1m\n",
      "Finetuned LED:\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0794 (prec) \n",
      " 0.1961 (recall) \n",
      " 0.113 (f-1)\n",
      "- Rouge L:\n",
      " 0.1575 (prec) \n",
      " 0.3846 (recall) \n",
      " 0.2235 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      "Fingerprint enhancement methods have been proposed in the literature for the enhancement of feature extraction and feature extraction. For example, @cite proposed a Gabor filter based on the response of eight oriented Gabor filters. In @cite, the authors proposed a method for the generation of synthetic fingerprint images. @cite introduced a method for generating synthetic fingerprint images by using space-variant filters. The authors used a directional image model, whose inputs are the number and location of the fingerprint cores and deltas, is used for tuning the filters according to the underlying ridge orientation and frequency. The authors proposed a fast fingerprint enhancement algorithm, which adaptively improve the clarity of ridge and valley structures of input fingerprint images based on the estimated local ridge orientation and frequencies.\n",
      "\u001b[1m\n",
      "Finetuned Cent:\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0581 (prec) \n",
      " 0.1961 (recall) \n",
      " 0.0897 (f-1)\n",
      "- Rouge L:\n",
      " 0.1098 (prec) \n",
      " 0.3654 (recall) \n",
      " 0.1689 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      "Fingerprint image enhancement has been widely studied in the literature @cite @cite. In @cite, the authors proposed a filter-based fingerprint image enhancement method based on the response of eight oriented Gabor filters. The proposed method makes use of Gabor filtering technologies and need only to do the core point detection before the feature extraction process without any other other pre-processing steps such as smoothing, binarization, thinning, and minutiae detection. @cite proposed an enhanced fingerprint enhancement algorithm based on the estimated local ridge orientation and frequency. The proposed algorithm has been evaluated in terms of decision error trade-off curves of an overall verification system. A fingerprint enhancement algorithm was proposed in @cite to adaptively improve the clarity of ridge and valley structures of input fingerprint images. The proposed fingerprint enhancement algorithm uses a bank of Gabor filters to capture both local and global details in a fingerprint as a compact fixed length FingerCode. The fingerprint matching is based on the Euclidean distance between the corresponding FingerCodes and hence is extremely fast.\n",
      "\u001b[1m\n",
      "2-Step (Cent):\u001b[0m\n",
      "\u001b[1mRouge scores:\u001b[0m\n",
      "- Rouge 2:\n",
      " 0.0809 (prec) \n",
      " 0.2157 (recall) \n",
      " 0.1176 (f-1)\n",
      "- Rouge L:\n",
      " 0.146 (prec) \n",
      " 0.3846 (recall) \n",
      " 0.2116 (f-1)\n",
      "\u001b[1mSummary:\u001b[0m\n",
      "In @cite, the authors proposed a fingerprint enhancement algorithm based on the response of eight oriented Gabor filters. The results have been evaluated on the basis of the response of four oriented Gabor filter. In @cite, the authors proposed an enhanced approach for fingerprint segmentation based on the responses of eight oriented gabor filters. In addition, the proposed method has been evaluated on two different scales @cite @cite. In addition, they did not provide an overview of the methods used for generating synthetic fingerprint images. In addition to generating fingerprint images, the authors in @cite proposed a method to generate synthetic fingerprint images, which is based on the idea that the image can be used to generate synthetic images. In this paper, we use the curved GF filters to enhance the ridge structure of the fingerprint images.\n"
     ]
    }
   ],
   "source": [
    "## Cell for repeated running\n",
    "\n",
    "# subset = 'short'\n",
    "# subset = 'medium'\n",
    "subset = 'long'\n",
    "\n",
    "# Remember: index should be from 0 to 19 (inclusive)\n",
    "subset_index = 19\n",
    "\n",
    "show_model_results(subset=subset, subset_index=subset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133b134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
