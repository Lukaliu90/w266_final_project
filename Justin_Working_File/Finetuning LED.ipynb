{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-k1jXQHLyFU"
   },
   "source": [
    "# W266 Final Project - Finetuning Longformer Encoder Decoder Model\n",
    "\n",
    "**Description:** \n",
    "\n",
    "- This notebook attempts to finetune the LED model for the summarization task for the X-Science dataset\n",
    "- Specifically, the model being finetuned is the checkpoint named \"LED-large-16384-arxiv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QDMDB_KqT8m"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzQBFiaPnr4j",
    "outputId": "684ac0ea-aa87-49f3-d9cb-bc262564043e"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pprint import pprint\n",
    "\n",
    "## For printing out model summary in PyTorch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "## General plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "## Managing memory\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "## Text processing\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking if GPU is available when running locally\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9xudUadMrFA"
   },
   "source": [
    "## 1. Importing Longformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preliminaries\n",
    "\n",
    "- Uses the HuggingFace model (https://huggingface.co/docs/transformers/model_doc/led).  This is the model with both encoder and decoder, and trained on summarization task using the arxiv dataset.\n",
    "- The encoder only version is at https://huggingface.co/docs/transformers/model_doc/longformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing model\n",
    "from transformers import LEDModel, LEDConfig, LEDTokenizer, AutoTokenizer, LEDForConditionalGeneration, EncoderDecoderModel\n",
    "\n",
    "## For training\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
      "\n",
      "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"allenai/led-base-16384\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"allenai/led-base-16384\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"allenai/led-base-16384\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loading check point\n",
    "\n",
    "# GPU version\n",
    "LEDmodel = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\", # large arxiv too large to fit in memory\n",
    "                                                       gradient_checkpointing=True,\n",
    "                                                       use_cache=False) #don't use to half when training\n",
    "LEDtokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "# LEDmodel = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\", # large arxiv too large to fit in memory\n",
    "#                                                        gradient_checkpointing=True,\n",
    "#                                                        use_cache=False).to(\"cuda\") #don't use to half when training\n",
    "# LEDtokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "\n",
    "# CPU version\n",
    "# LEDmodel = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "# LEDtokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JustinTo\\AppData\\Local\\Temp\\ipykernel_5516\\4286193646.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "## Loading rouge\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model summary and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "â”œâ”€LEDModel: 1-1                                    --\n",
      "|    â””â”€Embedding: 2-1                              38,603,520\n",
      "|    â””â”€LEDEncoder: 2-2                             --\n",
      "|    |    â””â”€Embedding: 3-1                         (recursive)\n",
      "|    |    â””â”€LEDLearnedPositionalEmbedding: 3-2     12,582,912\n",
      "|    |    â””â”€ModuleList: 3-3                        53,157,888\n",
      "|    |    â””â”€LayerNorm: 3-4                         1,536\n",
      "|    â””â”€LEDDecoder: 2-3                             --\n",
      "|    |    â””â”€Embedding: 3-5                         (recursive)\n",
      "|    |    â””â”€LEDLearnedPositionalEmbedding: 3-6     786,432\n",
      "|    |    â””â”€ModuleList: 3-7                        56,710,656\n",
      "|    |    â””â”€LayerNorm: 3-8                         1,536\n",
      "â”œâ”€Linear: 1-2                                      38,603,520\n",
      "===========================================================================\n",
      "Total params: 200,448,000\n",
      "Trainable params: 200,448,000\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n",
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "â”œâ”€LEDModel: 1-1                                    --\n",
      "|    â””â”€Embedding: 2-1                              38,603,520\n",
      "|    â””â”€LEDEncoder: 2-2                             --\n",
      "|    |    â””â”€Embedding: 3-1                         (recursive)\n",
      "|    |    â””â”€LEDLearnedPositionalEmbedding: 3-2     12,582,912\n",
      "|    |    â””â”€ModuleList: 3-3                        53,157,888\n",
      "|    |    â””â”€LayerNorm: 3-4                         1,536\n",
      "|    â””â”€LEDDecoder: 2-3                             --\n",
      "|    |    â””â”€Embedding: 3-5                         (recursive)\n",
      "|    |    â””â”€LEDLearnedPositionalEmbedding: 3-6     786,432\n",
      "|    |    â””â”€ModuleList: 3-7                        56,710,656\n",
      "|    |    â””â”€LayerNorm: 3-8                         1,536\n",
      "â”œâ”€Linear: 1-2                                      38,603,520\n",
      "===========================================================================\n",
      "Total params: 200,448,000\n",
      "Trainable params: 200,448,000\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "## Summary of model\n",
    "print(summary(LEDmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDConfig {\n",
       "  \"_name_or_path\": \"allenai/led-base-16384\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"LEDForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_window\": [\n",
       "    1024,\n",
       "    1024,\n",
       "    1024,\n",
       "    1024,\n",
       "    1024,\n",
       "    1024\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_decoder_position_embeddings\": 1024,\n",
       "  \"max_encoder_position_embeddings\": 16384,\n",
       "  \"model_type\": \"led\",\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Seeing the configuration options.\n",
    "LEDmodel.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters for use\n",
    "encoder_max_length = 4096 # max number of tokens in training and validation sets are 4694 and 4183 tokens respectively\n",
    "decoder_max_length = 256 # training set has 3 outliers with >512 tokens for labels, all others have less than 512\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading X-Science Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (C:/Users/JustinTo/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n",
      "Found cached dataset multi_x_science_sum (C:/Users/JustinTo/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n",
      "Found cached dataset multi_x_science_sum (C:/Users/JustinTo/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    }
   ],
   "source": [
    "## Loading the dataset\n",
    "xsci_train = load_dataset('multi_x_science_sum', split='train')\n",
    "xsci_val = load_dataset('multi_x_science_sum', split='validation')\n",
    "xsci_test = load_dataset('multi_x_science_sum', split='test')\n",
    "\n",
    "## For text processing as X-Science have not concatenated the source articles\n",
    "DOC_SEP = \"|||||\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = (\n",
    "        example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "        + DOC_SEP\n",
    "        + DOC_SEP.join([x for x in example[\"ref_abstract\"][\"abstract\"] if x])\n",
    "    )\n",
    "    output[\"related_work\"] = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_dataset_batched at 0x000002777F3C9AF0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7fef680da8496aa0a3f014dc0bc501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30369 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e12bca3d0849939e1a09ec66eaf6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5066 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d766668bf3748e78035df4614725bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5093 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xsci_train_processed = xsci_train.map(\n",
    "    # preprocess_dataset,\n",
    "    preprocess_dataset_batched,\n",
    "    remove_columns=xsci_test.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    )\n",
    "\n",
    "xsci_val_processed = xsci_val.map(\n",
    "    # preprocess_dataset,\n",
    "    preprocess_dataset_batched,\n",
    "    remove_columns=xsci_test.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    )\n",
    "\n",
    "xsci_test_processed = xsci_test.map(\n",
    "    # preprocess_dataset,\n",
    "    preprocess_dataset_batched,\n",
    "    remove_columns=xsci_test.column_names,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 30369\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 5066\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 5093\n",
       " }))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_train_processed, xsci_val_processed, xsci_test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    # tokenize the inputs and labels\n",
    "    inputs = LEDtokenizer(\n",
    "        batch[\"abstracts\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "    )\n",
    "    outputs = LEDtokenizer(\n",
    "        batch[\"related_work\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=decoder_max_length,\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    batch[\"global_attention_mask\"][0][0] = 1\n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == LEDtokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2b0ca0964d466eb220c65338203590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15185 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = xsci_train_processed.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"abstracts\", \"related_work\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fea0e289204ec48eb13069125df2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2533 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = xsci_val_processed.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"abstracts\", \"related_work\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")\n",
    "val_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing initial performance only\n",
    "train_sub_dataset = train_dataset.select(range(1500))\n",
    "val_sub_dataset = val_dataset.select(range(150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "     num_rows: 1500\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "     num_rows: 150\n",
       " }))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sub_dataset, val_sub_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Initial Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3427"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set generate hyperparameters\n",
    "LEDmodel.config.num_beams = 2\n",
    "LEDmodel.config.max_length = 512\n",
    "LEDmodel.config.min_length = 100\n",
    "LEDmodel.config.length_penalty = 2.0\n",
    "LEDmodel.config.early_stopping = True\n",
    "LEDmodel.config.no_repeat_ngram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = LEDtokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = LEDtokenizer.pad_token_id\n",
    "    label_str = LEDtokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# enable fp16 apex training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=5,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=8, # changed from 4\n",
    "    num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 First round of training - 1500 training samples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 1:39:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.586800</td>\n",
       "      <td>3.359481</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.356500</td>\n",
       "      <td>3.316304</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.323000</td>\n",
       "      <td>3.287591</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.418900</td>\n",
       "      <td>3.258099</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.347200</td>\n",
       "      <td>3.240349</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.378300</td>\n",
       "      <td>3.229219</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>3.216768</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.258400</td>\n",
       "      <td>3.213507</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>3.206490</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=3.3692895520117974, metrics={'train_runtime': 5987.8736, 'train_samples_per_second': 0.251, 'train_steps_per_second': 0.016, 'total_flos': 4017904794206208.0, 'train_loss': 3.3692895520117974, 'epoch': 0.99})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights for further iterations\n",
    "torch.save(LEDmodel.state_dict(), \"LED_xsci_finetuned_run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
      "\n",
      "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Running an LED base model loaded to CUDA, and loading in the tuned weights\n",
    "LEDmodel_tuned = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\").to(\"cuda\").half()\n",
    "LEDmodel_tuned.load_state_dict(torch.load(\"LED_xsci_finetuned_run1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs_base = LEDtokenizer(xsci_test_processed['abstracts'],\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=encoder_max_length,\n",
    "                                return_tensors=\"pt\",\n",
    "                                truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract_batched(batch_size=2, start=0, no_repeat_ngram_size=3):\n",
    "    \n",
    "    try:\n",
    "        del test_input_ids, attention_mask, global_attention_mask, predicted_abstract_ids\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    gc.collect()\n",
    "\n",
    "    test_input_ids = test_inputs_base['input_ids'][start:start+batch_size].to(\"cuda\")\n",
    "    attention_mask = test_inputs_base['attention_mask'][start:start+batch_size].to(\"cuda\")\n",
    "\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    predicted_abstract_ids = LEDmodel_tuned.generate(test_input_ids,\n",
    "                                                     attention_mask=attention_mask, \n",
    "                                                     global_attention_mask=global_attention_mask, \n",
    "                                                     max_length=200,\n",
    "                                                     no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                                     num_beams=4)\n",
    "\n",
    "    predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 500 now..\n",
      "Handling sample 1000 now..\n",
      "Handling sample 1500 now..\n",
      "Handling sample 2000 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 3000 now..\n",
      "Handling sample 3500 now..\n",
      "Handling sample 4000 now..\n",
      "Handling sample 4500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed, 5093 data points from the 5093 X-Sci test samples handled.\n"
     ]
    }
   ],
   "source": [
    "## Generating answers\n",
    "answers = []\n",
    "\n",
    "for i in range(0, xsci_test.num_rows, 2):\n",
    "    if i%500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    answers.append(generate_abstract_batched(start=i, batch_size=2))\n",
    "    \n",
    "print(f\"Completed, {i+1} data points from the {xsci_test.num_rows} X-Sci test samples handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_answers = []\n",
    "for answer in answers:\n",
    "    formatted_answers += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"LED_finetuned_xsci_run1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.13057976088324155, recall=0.11599050381417703, fmeasure=0.11343836213344564), mid=Score(precision=0.13288385987516677, recall=0.11826202171628658, fmeasure=0.11543461744076924), high=Score(precision=0.1352587328137354, recall=0.12084246462769582, fmeasure=0.11754179467622794)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.03252974837324809, recall=0.026654380146198038, fmeasure=0.027156282200533975), mid=Score(precision=0.03364807545174513, recall=0.027607158240798617, fmeasure=0.02803290885561131), high=Score(precision=0.03480720809310449, recall=0.028681810808499377, fmeasure=0.029000784145846368)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.10231436482486667, recall=0.09129374953868437, fmeasure=0.08885787178184824), mid=Score(precision=0.10397847511207359, recall=0.09330803026696353, fmeasure=0.09029349583472486), high=Score(precision=0.10579606715064886, recall=0.09535945221472812, fmeasure=0.09193941904152231)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.10230854876996161, recall=0.09113759302163393, fmeasure=0.08876607914791707), mid=Score(precision=0.10402816548249563, recall=0.09329920699438152, fmeasure=0.09032799378775724), high=Score(precision=0.10576523718305954, recall=0.09538330890463499, fmeasure=0.09178853688392513))}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculating the rouge score\n",
    "rouge.compute(predictions=formatted_answers,\n",
    "              references=[ref for ref in xsci_test_processed['related_work']],\n",
    "              use_stemmer = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Second Round of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing initial performance only\n",
    "train_sub_dataset = train_dataset.select(range(1500, 3000))\n",
    "val_sub_dataset = val_dataset.select(range(150, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 1:45:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.358500</td>\n",
       "      <td>3.255856</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.301800</td>\n",
       "      <td>3.219629</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.397700</td>\n",
       "      <td>3.207463</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.395800</td>\n",
       "      <td>3.194153</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.210200</td>\n",
       "      <td>3.186800</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.223800</td>\n",
       "      <td>3.168678</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.397600</td>\n",
       "      <td>3.162593</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.228700</td>\n",
       "      <td>3.155298</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.253400</td>\n",
       "      <td>3.151006</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=3.287283682054089, metrics={'train_runtime': 6325.1958, 'train_samples_per_second': 0.237, 'train_steps_per_second': 0.015, 'total_flos': 4017904794206208.0, 'train_loss': 3.287283682054089, 'epoch': 0.99})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights for further iterations\n",
    "torch.save(LEDmodel.state_dict(), \"LED_xsci_finetuned_run2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Finetuning Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing initial performance only\n",
    "train_sub_dataset = train_dataset.select(range(3000, 4500))\n",
    "val_sub_dataset = val_dataset.select(range(300, 350))  # Reducing val set more to save time as it won't affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:37, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.304800</td>\n",
       "      <td>3.084357</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.211600</td>\n",
       "      <td>3.071871</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.336900</td>\n",
       "      <td>3.055311</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.303400</td>\n",
       "      <td>3.039496</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.232600</td>\n",
       "      <td>3.033954</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.198700</td>\n",
       "      <td>3.018585</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.231600</td>\n",
       "      <td>3.010949</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.169600</td>\n",
       "      <td>3.014882</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.192000</td>\n",
       "      <td>3.003381</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=3.2500229599655315, metrics={'train_runtime': 2627.0283, 'train_samples_per_second': 0.571, 'train_steps_per_second': 0.035, 'total_flos': 4017904794206208.0, 'train_loss': 3.2500229599655315, 'epoch': 0.99})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights for further iterations\n",
    "torch.save(LEDmodel.state_dict(), \"LED_xsci_finetuned_run3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Finetuning Run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing initial performance only\n",
    "train_sub_dataset = train_dataset.select(range(4500, 6000))\n",
    "val_sub_dataset = val_dataset.select(range(350, 400))  # Reducing val set more to save time as it won't affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.232000</td>\n",
       "      <td>3.207462</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.236500</td>\n",
       "      <td>3.189419</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.185800</td>\n",
       "      <td>3.183589</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.233700</td>\n",
       "      <td>3.169704</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.238200</td>\n",
       "      <td>3.165358</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.232200</td>\n",
       "      <td>3.146673</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.211400</td>\n",
       "      <td>3.141774</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.179800</td>\n",
       "      <td>3.135014</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.243800</td>\n",
       "      <td>3.130435</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=3.219583983062416, metrics={'train_runtime': 2681.6972, 'train_samples_per_second': 0.559, 'train_steps_per_second': 0.035, 'total_flos': 4017904794206208.0, 'train_loss': 3.219583983062416, 'epoch': 0.99})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights for further iterations\n",
    "torch.save(LEDmodel.state_dict(), \"LED_xsci_finetuned_run4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Finetuning Run 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For testing initial performance only\n",
    "train_sub_dataset = train_dataset.select(range(6000, 7500))\n",
    "val_sub_dataset = val_dataset.select(range(400, 450))  # Reducing val set more to save time as it won't affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.289400</td>\n",
       "      <td>3.139441</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.216700</td>\n",
       "      <td>3.103977</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.161300</td>\n",
       "      <td>3.086600</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.257600</td>\n",
       "      <td>3.083411</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.145600</td>\n",
       "      <td>3.085605</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.353700</td>\n",
       "      <td>3.075910</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.130800</td>\n",
       "      <td>3.064824</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.224200</td>\n",
       "      <td>3.060600</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.155200</td>\n",
       "      <td>3.054527</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=93, training_loss=3.1918629471973707, metrics={'train_runtime': 2744.9026, 'train_samples_per_second': 0.546, 'train_steps_per_second': 0.034, 'total_flos': 4017904794206208.0, 'train_loss': 3.1918629471973707, 'epoch': 0.99})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving weights for further iterations\n",
    "torch.save(LEDmodel.state_dict(), \"LED_xsci_finetuned_run5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Finetuning Runs 6 to 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 6-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.118200</td>\n",
       "      <td>3.155369</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.238400</td>\n",
       "      <td>3.138356</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.215400</td>\n",
       "      <td>3.133976</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.188300</td>\n",
       "      <td>3.117033</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.181900</td>\n",
       "      <td>3.121855</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.263400</td>\n",
       "      <td>3.103364</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.071000</td>\n",
       "      <td>3.094603</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.196000</td>\n",
       "      <td>3.095176</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.083800</td>\n",
       "      <td>3.087109</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.054500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 7-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.277100</td>\n",
       "      <td>3.163663</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.174300</td>\n",
       "      <td>3.176009</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.187400</td>\n",
       "      <td>3.149466</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.106500</td>\n",
       "      <td>3.145699</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.102400</td>\n",
       "      <td>3.141603</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.080900</td>\n",
       "      <td>3.127262</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.239000</td>\n",
       "      <td>3.122388</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.128600</td>\n",
       "      <td>3.115997</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.134600</td>\n",
       "      <td>3.109286</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 8-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.172900</td>\n",
       "      <td>3.130991</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.159200</td>\n",
       "      <td>3.121824</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.207400</td>\n",
       "      <td>3.116909</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.183000</td>\n",
       "      <td>3.117361</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.091700</td>\n",
       "      <td>3.104712</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.133900</td>\n",
       "      <td>3.091184</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.084800</td>\n",
       "      <td>3.095684</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.156700</td>\n",
       "      <td>3.086277</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.079100</td>\n",
       "      <td>3.081613</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 9-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.142900</td>\n",
       "      <td>3.296781</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.118900</td>\n",
       "      <td>3.282125</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.134000</td>\n",
       "      <td>3.280280</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.046200</td>\n",
       "      <td>3.267339</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.029800</td>\n",
       "      <td>3.261213</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.067200</td>\n",
       "      <td>3.252886</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.083700</td>\n",
       "      <td>3.253846</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.188400</td>\n",
       "      <td>3.243794</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.128900</td>\n",
       "      <td>3.239927</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 10-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.106300</td>\n",
       "      <td>3.068065</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.034400</td>\n",
       "      <td>3.056688</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.109100</td>\n",
       "      <td>3.044571</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.196000</td>\n",
       "      <td>3.039922</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.181000</td>\n",
       "      <td>3.029540</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.108600</td>\n",
       "      <td>3.024995</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.058700</td>\n",
       "      <td>3.012375</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.007700</td>\n",
       "      <td>3.008139</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.031000</td>\n",
       "      <td>3.007747</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 11-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.162100</td>\n",
       "      <td>3.070439</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.178500</td>\n",
       "      <td>3.075189</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.216800</td>\n",
       "      <td>3.061686</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.138600</td>\n",
       "      <td>3.055430</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.094100</td>\n",
       "      <td>3.044333</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.116700</td>\n",
       "      <td>3.043226</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.102300</td>\n",
       "      <td>3.037044</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.132400</td>\n",
       "      <td>3.035568</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.120100</td>\n",
       "      <td>3.029893</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 12-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.201500</td>\n",
       "      <td>3.117205</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.193300</td>\n",
       "      <td>3.107493</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.129300</td>\n",
       "      <td>3.103624</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.154600</td>\n",
       "      <td>3.083095</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.084800</td>\n",
       "      <td>3.092345</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.127700</td>\n",
       "      <td>3.084503</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.070800</td>\n",
       "      <td>3.082038</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.144900</td>\n",
       "      <td>3.077165</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.093400</td>\n",
       "      <td>3.073436</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 13-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.219300</td>\n",
       "      <td>3.110039</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.211900</td>\n",
       "      <td>3.104924</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.230700</td>\n",
       "      <td>3.097950</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.207900</td>\n",
       "      <td>3.096752</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.126100</td>\n",
       "      <td>3.082659</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.091200</td>\n",
       "      <td>3.074965</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.140600</td>\n",
       "      <td>3.066975</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.168000</td>\n",
       "      <td>3.062457</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.102900</td>\n",
       "      <td>3.060525</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6, 14):\n",
    "    \n",
    "    print(f\"\\n-----Starting Run {i}-----\\n\")\n",
    "    \n",
    "    train_sub_dataset = train_dataset.select(range(1500*(i-1), 1500*i))\n",
    "    val_sub_dataset = val_dataset.select(range((150+50*i), (200+50*i)))  \n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    PATH = f\"LED_xsci_finetuned_run{i}\"\n",
    "    \n",
    "    torch.save(LEDmodel.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 14-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.218600</td>\n",
       "      <td>3.045994</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.164200</td>\n",
       "      <td>3.032179</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.027500</td>\n",
       "      <td>3.033917</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.135600</td>\n",
       "      <td>3.016716</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.134800</td>\n",
       "      <td>3.015402</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.967000</td>\n",
       "      <td>3.002485</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.997900</td>\n",
       "      <td>3.003843</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.193400</td>\n",
       "      <td>2.993947</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.186200</td>\n",
       "      <td>2.992142</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 15-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.193700</td>\n",
       "      <td>3.138677</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.160300</td>\n",
       "      <td>3.116251</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.099500</td>\n",
       "      <td>3.118868</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.200800</td>\n",
       "      <td>3.110048</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.075000</td>\n",
       "      <td>3.108420</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.041700</td>\n",
       "      <td>3.098351</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>3.099936</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.065600</td>\n",
       "      <td>3.094401</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.068900</td>\n",
       "      <td>3.090332</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 16-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.120500</td>\n",
       "      <td>2.997821</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.152300</td>\n",
       "      <td>2.983506</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.059200</td>\n",
       "      <td>2.970604</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.086900</td>\n",
       "      <td>2.956535</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.108800</td>\n",
       "      <td>2.957483</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.144900</td>\n",
       "      <td>2.956088</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.093600</td>\n",
       "      <td>2.945897</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.080400</td>\n",
       "      <td>2.937679</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.075600</td>\n",
       "      <td>2.934222</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 17-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.161800</td>\n",
       "      <td>2.996443</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.126000</td>\n",
       "      <td>2.972418</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.181700</td>\n",
       "      <td>2.965862</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.164300</td>\n",
       "      <td>2.969478</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.271100</td>\n",
       "      <td>2.965291</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.107900</td>\n",
       "      <td>2.947655</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.155300</td>\n",
       "      <td>2.945430</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.114700</td>\n",
       "      <td>2.945543</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.230900</td>\n",
       "      <td>2.941980</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 18-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:30, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.332900</td>\n",
       "      <td>3.005607</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.252000</td>\n",
       "      <td>2.987843</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.197000</td>\n",
       "      <td>2.987725</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.228900</td>\n",
       "      <td>2.979223</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.190100</td>\n",
       "      <td>2.964707</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.168700</td>\n",
       "      <td>2.963597</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>2.958445</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.285100</td>\n",
       "      <td>2.950494</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.197300</td>\n",
       "      <td>2.948331</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.055300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 19-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.215500</td>\n",
       "      <td>3.094069</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.193000</td>\n",
       "      <td>3.103274</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.187200</td>\n",
       "      <td>3.081476</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.118900</td>\n",
       "      <td>3.082509</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.208800</td>\n",
       "      <td>3.071808</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.179000</td>\n",
       "      <td>3.067470</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.132100</td>\n",
       "      <td>3.067524</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.087300</td>\n",
       "      <td>3.057874</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.195300</td>\n",
       "      <td>3.053869</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1869\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 116\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 20-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116/116 57:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.319400</td>\n",
       "      <td>3.070155</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.231500</td>\n",
       "      <td>3.074389</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.187600</td>\n",
       "      <td>3.065090</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.181400</td>\n",
       "      <td>3.059839</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.256100</td>\n",
       "      <td>3.048319</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.178400</td>\n",
       "      <td>3.037549</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.170500</td>\n",
       "      <td>3.039893</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.086800</td>\n",
       "      <td>3.033436</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.127300</td>\n",
       "      <td>3.029459</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.174500</td>\n",
       "      <td>3.023694</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.176300</td>\n",
       "      <td>3.021127</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-100\n",
      "Configuration saved in ./checkpoint-100\\config.json\n",
      "Model weights saved in ./checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-100\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-110\n",
      "Configuration saved in ./checkpoint-110\\config.json\n",
      "Model weights saved in ./checkpoint-110\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-110\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-110\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3.8 Finetuning Runs 14 to 20\n",
    "for i in range(14, 21):\n",
    "    \n",
    "    print(f\"\\n-----Starting Run {i}-----\\n\")\n",
    "    \n",
    "    if i == 20:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 30369))\n",
    "    else:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 1500*i))\n",
    "    \n",
    "    val_sub_dataset = val_dataset.select(range((150+50*i), (200+50*i)))  \n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    PATH = f\"LED_xsci_finetuned_run{i}\"\n",
    "    \n",
    "    torch.save(LEDmodel.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interim Tests (on Run 13 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In @cite, the authors presented an approach to the problem of how an agent should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. In this work, the authors propose an agent tracking capability, which is similar to the Soar integrated architecture, but differs in two ways. First, the agents are trained in a real-time, dynamic, multi-agent domain. Second, the agent is trained in the real world domain, and the agent can be trained in real-world scenarios. The authors also provide a framework for the incremental implementation of agent tracking capabilities, and a description of the forms of knowledge required.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[0][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We present our approach to the problem of how an agent, within an economic Multi-Agent System, can determine when it should behave strategically (i.e. learn and use models of other agents), and when it should act as a simple price-taker. We provide a framework for the incremental implementation of modeling capabilities in agents, and a description of the forms of knowledge required. The agents were implemented and different populations simulated in order to learn more about their behavior and the merits of using and learning agent models. Our results show, among other lessons, how savvy buyers can avoid being cheated'' by sellers, how price volatility can be used to quantitatively predict the benefits of deeper models, and how specific types of agent populations influence system behavior.|||||In multi-agent environments, an intelligent agent often needs to interact with other individuals or groups of agents to achieve its goals. Agent tracking is one key capability required for intelligent interaction. It involves monitoring the observable actions of other agents and inferring their unobserved actions, plans, goals and behaviors. This article examines the implications of such an agent tracking capability for agent architectures. It specifically focuses on real-time and dynamic environments, where an intelligent agent is faced with the challenge of tracking the highly flexible mix of goal-driven and reactive behaviors of other agents, in real-time. The key implication is that an agent architecture needs to provide direct support for flexible and efficient reasoning about other agents' models. In this article, such support takes the form of an architectural capability to execute the other agent's models, enabling mental simulation of their behaviors. Other architectural requirements that follow include the capabilities for (pseudo-) simultaneous execution of multiple agent models, dynamic sharing and unsharing of multiple agent models and high bandwidth inter-model communication. We have implemented an agent architecture, an experimental variant of the Soar integrated architecture, that conforms to all of these requirements. Agents based on this architecture have been implemented to execute two different tasks in a real-time, dynamic, multi-agent domain. The article presents experimental results illustrating the agents' dynamic behavior.|||||I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV. Counteracting institutions, 499. â€” V. Conclusion, 500.|||||The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice.\""
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test_processed[0][\"abstracts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Within the MAS community, some work @cite has focused on how artificial AI-based learning agents would fare in communities of similar agents. For example, @cite and @cite show how agents can learn the capabilities of others via repeated interactions, but these agents do not learn to predict what actions other might take. Most of the work in MAS also fails to recognize the possible gains from using explicit agent models to predict agent actions. @cite is an exception and gives another approach for using nested agent models. However, they do not go so far as to try to quantify the advantages of their nested models or show how these could be learned via observations. We believe that our research will bring to the foreground some of the common observations seen in these research areas and help to clarify the implications and utility of learning and using nested agent models.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test_processed[0][\"related_work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 425 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In @cite, the authors proposed a method for automating grasping motion in the animation of virtual actors. The authors propose a hybrid approach using both forward and inverse kinematics to generate realistic looking grasping motion of arbitrary shaped objects. The method is similar to our approach in that it can be adapted to different hand meshes (e.g. human or robotic hands). However, our approach is flexible because it can adapt to different objects. Moreover, our method is able to adapt to various hand meshes, such as human and robotic hands, and it is also easily customizable.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[1][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.|||||Abstract This paper addresses the important issue of automating grasping movement in the animation of virtual actors, and presents a methodology and algorithm to generate realistic looking grasping motion of arbitrary shaped objects. A hybrid approach using both forward and inverse kinematics is proposed. A database of predefined body postures and hand trajectories are generalized to adapt to a specific grasp. The reachable space is divided into small subvolumes, which enables the construction of the database. The paper also addresses some common problems of articulated figure animation. A new approach for body positioning with kinematic constraints on both hands is described. An efficient and accurate manipulation of joint constraints is also presented. Finally, we describe an interpolation algorithm which interpolates between two postures of an articulated figure by moving the end effector along a specific trajectory and maintaining all the joint angles in the feasible range. Results are quite satisfactory, and some are shown in the paper.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test_processed[1][\"abstracts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Grasping action is the most basic component of any interaction and it is composed of three major components @cite . The first one is related to the process of approaching the arm and hand to the target object, considering the overall body movement. The second component focuses on the hand and body pre-shaping before the grasping action. Finally, the last component fits the hand to the geometry of the object by closing each of the fingers until contact is established.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test_processed[2942][\"related_work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1158 to 2048 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The complexity of solving non-cooperative games in a distributed manner has been studied in @cite, where @math is the number of iterations required to find a Nash equilibrium of any game with at least one NE. @math and @math are the two classes of @math -player games with @math or @math, respectively. The problem of solving a non-convex game in distributed manner in which the players communicate with a set of system nodes over noisy communication channels is considered in @math. @math has been extended to the class of all @math games with a continuous action space that admit one NE such that the players' utility functions satisfy a differential constraint.\"]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[2942][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper studies the complexity of solving two classes of non-cooperative games in a distributed manner in which the players communicate with a set of system nodes over noisy communication channels. The complexity of solving each game class is defined as the minimum number of iterations required to find a Nash equilibrium (NE) of any game in that class with @math accuracy. First, we consider the class @math of all @math -player non-cooperative games with a continuous action space that admit at least one NE. Using information-theoretic inequalities, we derive a lower bound on the complexity of solving @math that depends on the Kolmogorov @math -capacity of the constraint set and the total capacity of the communication channels. We also derive a lower bound on the complexity of solving games in @math which depends on the volume and surface area of the constraint set. We next consider the class of all @math -player non-cooperative games with at least one NE such that the players' utility functions satisfy a certain (differential) constraint. We derive lower bounds on the complexity of solving this game class under both Gaussian and non-Gaussian noise models. Our result in the non-Gaussian case is derived by establishing a connection between the Kullback-Leibler distance and Fisher information.|||||We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates. We show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most âˆšd in convergence rate over traditional stochastic gradient methods, where d is the problem dimension. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.|||||We consider derivative-free algorithms for stochastic and nonstochastic convex optimization problems that use only function values rather than gradients. Focusing on nonasymptotic bounds on convergence rates, we show that if pairs of function values are available, algorithms for @math -dimensional optimization that use gradient estimates based on random perturbations suffer a factor of at most @math in convergence rate over traditional stochastic gradient methods. We establish such results for both smooth and nonsmooth cases, sharpening previous analyses that suggested a worse dimension dependence, and extend our results to the case of multiple ( @math ) evaluations. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, establishing the sharpness of our achievable results up to constant (sometimes logarithmic) factors.|||||We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function x â†’ f (x) we consider optimization when one is given access to absolute error oracles that return values in [f (x) - âˆŠ, f (x) + âˆŠ] or relative error oracles that return value in [(1 - âˆŠ)f (x), (1 + âˆŠ)f (x)], for some âˆŠ > 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.|||||We study the intrinsic limitations of sequential convex optimization through the lens of feedback information theory. In the oracle model of optimization, an algorithm queries an oracle for noisy information about the unknown objective function and the goal is to (approximately) minimize every function in a given class using as few queries as possible. We show that, in order for a function to be optimized, the algorithm must be able to accumulate enough information about the objective. This, in turn, puts limits on the speed of optimization under specific assumptions on the oracle and the type of feedback. Our techniques are akin to the ones used in statistical literature to obtain minimax lower bounds on the risks of estimation procedures; the notable difference is that, unlike in the case of i.i.d. data, a sequential optimization algorithm can gather observations in a controlled manner, so that the amount of information at each step is allowed to change in time. In particular, we show that optimization algorithms often obey the law of diminishing returns: the signal-to-noise ratio drops as the optimization algorithm approaches the optimum. To underscore the generality of the tools, we use our approach to derive fundamental lower bounds for a certain active learning problem. Overall, the present work connects the intuitive notions of â€œinformationâ€ in optimization, experimental design, estimation, and active learning to the quantitative notion of Shannon information.|||||This paper provides lower bounds on the convergence rate of Derivative Free Optimization (DFO) with noisy function evaluations, exposing a fundamental and unavoidable gap between the performance of algorithms with access to gradients and those with access to only function evaluations. However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions. A distinctive feature of the algorithm is that it uses only Boolean-valued function comparisons, rather than function evaluations. This makes the algorithm useful in an even wider range of applications, such as optimization based on paired comparisons from human subjects, for example. We also show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same.\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test_processed[2942][\"abstracts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interim Tests (on Run 20 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@cite and @cite provide a framework for the incremental implementation of agent modeling capabilities in a multi-agent setting. Their framework is similar in spirit to ours, but differs from ours in that they do not provide a formal description of how agents should behave in the real-time and dynamic environments. In contrast, our framework is based on the idea that agents should be able to learn and use models of other agents in order to achieve their goals. Our framework is also complementary to ours in the sense that agents are able to use and use their models to learn how to behave in a dynamic environment.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[0][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1158 to 2048 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The complexity of solving non-cooperative games in a distributed manner is defined as the minimum number of iterations required to find a Nash equilibrium (NE) of a given class of strongly convex objective functions @cite. In this paper, we consider the class of games with a continuous action space that admit at least one NE such that the players' utility functions satisfy a certain (differential) constraint. In the case of @math -player games with continuous action spaces that admit @math NEs, we derive a lower bound on the complexity of the class @math of games in which the players have @math SEs. We also derive lower bounds on solving games in the case where the players do not admit a NE. In contrast, in @math, we consider a class of non-convex games with at least @math convex SEs such as @math.\"]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[2942][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after First Epoch (i.e. 20 Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
      "\n",
      "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Running an LED base model loaded to CUDA, and loading in the tuned weights\n",
    "LEDmodel_tuned = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\").to(\"cuda\").half()\n",
    "LEDmodel_tuned.load_state_dict(torch.load(\"LED_xsci_finetuned_run20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5093, 4096])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs_base.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 500 now..\n",
      "Handling sample 1000 now..\n",
      "Handling sample 1500 now..\n",
      "Handling sample 2000 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 3000 now..\n",
      "Handling sample 3500 now..\n",
      "Handling sample 4000 now..\n",
      "Handling sample 4500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed, 5093 data points from the 5093 X-Sci test samples handled.\n"
     ]
    }
   ],
   "source": [
    "## Generating answers\n",
    "answers = []\n",
    "\n",
    "for i in range(0, xsci_test.num_rows, 2):\n",
    "    if i%500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    answers.append(generate_abstract_batched(start=i, batch_size=2))\n",
    "    \n",
    "print(f\"Completed, {i+1} data points from the {xsci_test.num_rows} X-Sci test samples handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_answers = []\n",
    "for answer in answers:\n",
    "    formatted_answers += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"LED_finetuned_xsci_run20.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2189969332825034, recall=0.16469816555046957, fmeasure=0.16370251789160398), mid=Score(precision=0.2221022440221254, recall=0.16803683821206544, fmeasure=0.16627846045873385), high=Score(precision=0.2254366342905602, recall=0.17145377217150184, fmeasure=0.16895833027161908)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.05300207630725514, recall=0.03520468048195691, fmeasure=0.03736891494682047), mid=Score(precision=0.054777220394552795, recall=0.03644890579402382, fmeasure=0.03852526235769757), high=Score(precision=0.056590967667679015, recall=0.037790032619509376, fmeasure=0.03974100181604705)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.16727217245905468, recall=0.12255084229522177, fmeasure=0.12228662127630249), mid=Score(precision=0.16955883500207025, recall=0.1252683193360101, fmeasure=0.12404610628898767), high=Score(precision=0.1719575605698216, recall=0.12765852297468233, fmeasure=0.1257458938296389)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.16748544398512194, recall=0.12273853726786363, fmeasure=0.12236892973311803), mid=Score(precision=0.1695870251867511, recall=0.12515532467839457, fmeasure=0.12402336351088271), high=Score(precision=0.17187286794935105, recall=0.12763337317702927, fmeasure=0.1257430530937679))}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculating the rouge score\n",
    "rouge.compute(predictions=formatted_answers,\n",
    "              references=[ref for ref in xsci_test_processed['related_work']],\n",
    "              use_stemmer = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Second Epoch Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 1 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 46:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.062000</td>\n",
       "      <td>3.095937</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.942300</td>\n",
       "      <td>3.091264</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.947800</td>\n",
       "      <td>3.078426</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.082200</td>\n",
       "      <td>3.068589</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.035400</td>\n",
       "      <td>3.064504</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.086400</td>\n",
       "      <td>3.060841</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.968600</td>\n",
       "      <td>3.048746</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.012400</td>\n",
       "      <td>3.045036</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.143300</td>\n",
       "      <td>3.042347</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 2 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 46:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.939200</td>\n",
       "      <td>2.921397</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.912900</td>\n",
       "      <td>2.939402</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.071300</td>\n",
       "      <td>2.904359</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.096900</td>\n",
       "      <td>2.904894</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.935600</td>\n",
       "      <td>2.892256</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.977000</td>\n",
       "      <td>2.875817</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.193600</td>\n",
       "      <td>2.877270</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.036100</td>\n",
       "      <td>2.870024</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.088900</td>\n",
       "      <td>2.868174</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 3 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.890400</td>\n",
       "      <td>3.130119</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.869500</td>\n",
       "      <td>3.151503</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.025800</td>\n",
       "      <td>3.116408</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.025000</td>\n",
       "      <td>3.119953</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.984000</td>\n",
       "      <td>3.101336</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.977800</td>\n",
       "      <td>3.096196</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.041200</td>\n",
       "      <td>3.082287</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.010700</td>\n",
       "      <td>3.084336</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.043500</td>\n",
       "      <td>3.078574</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 4 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.848300</td>\n",
       "      <td>3.007927</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.911800</td>\n",
       "      <td>3.000872</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.911900</td>\n",
       "      <td>2.972277</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.959900</td>\n",
       "      <td>2.964267</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.011300</td>\n",
       "      <td>2.955741</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.028800</td>\n",
       "      <td>2.949294</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.036400</td>\n",
       "      <td>2.936317</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.037300</td>\n",
       "      <td>2.932335</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.130200</td>\n",
       "      <td>2.928795</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 5 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.908700</td>\n",
       "      <td>3.122052</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.907000</td>\n",
       "      <td>3.103621</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.890800</td>\n",
       "      <td>3.088909</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.037600</td>\n",
       "      <td>3.083971</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.925500</td>\n",
       "      <td>3.076668</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.164300</td>\n",
       "      <td>3.072529</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.964900</td>\n",
       "      <td>3.064049</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.096000</td>\n",
       "      <td>3.058980</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.069800</td>\n",
       "      <td>3.052816</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 6 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 49:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.744400</td>\n",
       "      <td>3.039745</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.947600</td>\n",
       "      <td>3.011190</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.947200</td>\n",
       "      <td>3.019633</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.969100</td>\n",
       "      <td>2.994144</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.985500</td>\n",
       "      <td>2.996237</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.097500</td>\n",
       "      <td>2.981993</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.908100</td>\n",
       "      <td>2.973604</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.078400</td>\n",
       "      <td>2.970840</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.985200</td>\n",
       "      <td>2.965792</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.058300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 7 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.896200</td>\n",
       "      <td>2.908542</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.049400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.872400</td>\n",
       "      <td>2.898990</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.938200</td>\n",
       "      <td>2.886604</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.874000</td>\n",
       "      <td>2.876403</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.911700</td>\n",
       "      <td>2.886275</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.930500</td>\n",
       "      <td>2.868963</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.112100</td>\n",
       "      <td>2.858613</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.026800</td>\n",
       "      <td>2.854752</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.066800</td>\n",
       "      <td>2.849533</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 8 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.837200</td>\n",
       "      <td>3.072921</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.879100</td>\n",
       "      <td>3.075661</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.957800</td>\n",
       "      <td>3.046962</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.977000</td>\n",
       "      <td>3.045879</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.910900</td>\n",
       "      <td>3.045319</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.984000</td>\n",
       "      <td>3.014012</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.968500</td>\n",
       "      <td>3.013730</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.048400</td>\n",
       "      <td>3.009923</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.028300</td>\n",
       "      <td>3.004475</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 9 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 46:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.807100</td>\n",
       "      <td>3.000378</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.839900</td>\n",
       "      <td>2.993233</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.903300</td>\n",
       "      <td>2.977646</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.842400</td>\n",
       "      <td>2.967783</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.861600</td>\n",
       "      <td>2.962765</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.922700</td>\n",
       "      <td>2.951033</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.981300</td>\n",
       "      <td>2.950754</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.107000</td>\n",
       "      <td>2.946230</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.067900</td>\n",
       "      <td>2.940143</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 10 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 43:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.738700</td>\n",
       "      <td>3.086309</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.770900</td>\n",
       "      <td>3.055089</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.876300</td>\n",
       "      <td>3.037665</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.007100</td>\n",
       "      <td>3.028530</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.012600</td>\n",
       "      <td>3.016475</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.984700</td>\n",
       "      <td>3.012596</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.969500</td>\n",
       "      <td>3.003330</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.931700</td>\n",
       "      <td>2.997270</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.977900</td>\n",
       "      <td>2.994570</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 11 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 46:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.829900</td>\n",
       "      <td>3.099360</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.927700</td>\n",
       "      <td>3.084617</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.004900</td>\n",
       "      <td>3.077388</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.938400</td>\n",
       "      <td>3.064973</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.922700</td>\n",
       "      <td>3.050784</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.982000</td>\n",
       "      <td>3.048578</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.007200</td>\n",
       "      <td>3.042363</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.044500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.067100</td>\n",
       "      <td>3.038888</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.076700</td>\n",
       "      <td>3.035746</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 12 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.864000</td>\n",
       "      <td>3.079359</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.930000</td>\n",
       "      <td>3.065106</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.897100</td>\n",
       "      <td>3.073356</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.965800</td>\n",
       "      <td>3.044752</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>3.050087</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.045200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.005600</td>\n",
       "      <td>3.038471</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.976000</td>\n",
       "      <td>3.030725</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.087300</td>\n",
       "      <td>3.025058</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.058400</td>\n",
       "      <td>3.020713</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 13 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 44:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.898000</td>\n",
       "      <td>3.285002</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.969700</td>\n",
       "      <td>3.267649</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.007000</td>\n",
       "      <td>3.262457</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.024700</td>\n",
       "      <td>3.244448</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.976500</td>\n",
       "      <td>3.235155</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>3.221847</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.049800</td>\n",
       "      <td>3.214546</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.107700</td>\n",
       "      <td>3.206937</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.063600</td>\n",
       "      <td>3.203828</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Training for 13 runs first as I have 13 hours available.\n",
    "for i in range(1, 14):\n",
    "    \n",
    "    print(f\"\\n-----Starting Run {i} of Epoch 2-----\\n\")\n",
    "    \n",
    "    if i == 20:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 30369))\n",
    "    else:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 1500*i))\n",
    "    \n",
    "    val_sub_dataset = val_dataset.select(range((50*(i-1)), (50*i)))  \n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    PATH = f\"LED_xsci_finetuned_epoch2_run{i}\"\n",
    "    \n",
    "    torch.save(LEDmodel.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 14 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 49:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.886400</td>\n",
       "      <td>3.032266</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.921000</td>\n",
       "      <td>3.028495</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.811100</td>\n",
       "      <td>3.010232</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.952300</td>\n",
       "      <td>3.004498</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.990100</td>\n",
       "      <td>2.992680</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.849200</td>\n",
       "      <td>2.979079</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.905700</td>\n",
       "      <td>2.979198</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.143700</td>\n",
       "      <td>2.970384</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.146600</td>\n",
       "      <td>2.964620</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 15 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 46:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.876000</td>\n",
       "      <td>3.072990</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.914600</td>\n",
       "      <td>3.054797</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.880200</td>\n",
       "      <td>3.034589</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.024700</td>\n",
       "      <td>3.028608</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.925300</td>\n",
       "      <td>3.019461</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.936700</td>\n",
       "      <td>3.010084</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.980200</td>\n",
       "      <td>3.012640</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.018100</td>\n",
       "      <td>3.007290</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.040600</td>\n",
       "      <td>3.000685</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 16 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 47:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.787100</td>\n",
       "      <td>3.118179</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.913600</td>\n",
       "      <td>3.087421</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.856600</td>\n",
       "      <td>3.085166</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.922300</td>\n",
       "      <td>3.056962</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.964200</td>\n",
       "      <td>3.054320</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.030800</td>\n",
       "      <td>3.041145</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.016200</td>\n",
       "      <td>3.035845</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.043900</td>\n",
       "      <td>3.031094</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.044600</td>\n",
       "      <td>3.026985</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 17 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 45:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.860400</td>\n",
       "      <td>3.104215</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.903200</td>\n",
       "      <td>3.088678</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.985100</td>\n",
       "      <td>3.085711</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.015700</td>\n",
       "      <td>3.073401</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.132500</td>\n",
       "      <td>3.065926</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.999800</td>\n",
       "      <td>3.053473</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.084000</td>\n",
       "      <td>3.039873</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.077800</td>\n",
       "      <td>3.041171</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.224100</td>\n",
       "      <td>3.037147</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 18 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 48:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.980500</td>\n",
       "      <td>3.052346</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.009400</td>\n",
       "      <td>3.041878</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.994300</td>\n",
       "      <td>3.035139</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.066000</td>\n",
       "      <td>3.026803</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.063200</td>\n",
       "      <td>3.018296</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.078800</td>\n",
       "      <td>3.004251</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.100300</td>\n",
       "      <td>2.999377</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.254000</td>\n",
       "      <td>2.991749</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.187100</td>\n",
       "      <td>2.988329</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 93\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 19 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 49:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.889800</td>\n",
       "      <td>3.149838</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.965000</td>\n",
       "      <td>3.144794</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.997300</td>\n",
       "      <td>3.125262</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.954600</td>\n",
       "      <td>3.118775</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.084000</td>\n",
       "      <td>3.119250</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.087100</td>\n",
       "      <td>3.107447</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.079800</td>\n",
       "      <td>3.101096</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.057700</td>\n",
       "      <td>3.092354</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.192900</td>\n",
       "      <td>3.087255</td>\n",
       "      <td>0.056900</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1869\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 116\n",
      "  Number of trainable parameters = 161844480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Starting Run 20 of Epoch 2-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116/116 58:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.985000</td>\n",
       "      <td>2.991040</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.972800</td>\n",
       "      <td>2.979817</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.968200</td>\n",
       "      <td>2.970464</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>2.949457</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.100600</td>\n",
       "      <td>2.953789</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.037400</td>\n",
       "      <td>2.935354</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.063100</td>\n",
       "      <td>2.935560</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.006300</td>\n",
       "      <td>2.932346</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.073400</td>\n",
       "      <td>2.922228</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.148900</td>\n",
       "      <td>2.920949</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.187800</td>\n",
       "      <td>2.916980</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10\\config.json\n",
      "Model weights saved in ./checkpoint-10\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-10\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20\\config.json\n",
      "Model weights saved in ./checkpoint-20\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-20\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30\\config.json\n",
      "Model weights saved in ./checkpoint-30\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-30\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40\\config.json\n",
      "Model weights saved in ./checkpoint-40\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-40\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50\\config.json\n",
      "Model weights saved in ./checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60\\config.json\n",
      "Model weights saved in ./checkpoint-60\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-60\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70\\config.json\n",
      "Model weights saved in ./checkpoint-70\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-70\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-70\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80\\config.json\n",
      "Model weights saved in ./checkpoint-80\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-80\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-80\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-90\n",
      "Configuration saved in ./checkpoint-90\\config.json\n",
      "Model weights saved in ./checkpoint-90\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-100\n",
      "Configuration saved in ./checkpoint-100\\config.json\n",
      "Model weights saved in ./checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-100\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./checkpoint-110\n",
      "Configuration saved in ./checkpoint-110\\config.json\n",
      "Model weights saved in ./checkpoint-110\\pytorch_model.bin\n",
      "tokenizer config file saved in ./checkpoint-110\\tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-110\\special_tokens_map.json\n",
      "Deleting older checkpoint [checkpoint-90] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Training for remaining epochs\n",
    "for i in range(14, 21):\n",
    "    \n",
    "    print(f\"\\n-----Starting Run {i} of Epoch 2-----\\n\")\n",
    "    \n",
    "    if i == 20:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 30369))\n",
    "    else:\n",
    "        train_sub_dataset = train_dataset.select(range(1500*(i-1), 1500*i))\n",
    "    \n",
    "    val_sub_dataset = val_dataset.select(range((50*(i-1)), (50*i)))  \n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=LEDmodel,\n",
    "    tokenizer=LEDtokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_sub_dataset,\n",
    "    eval_dataset=val_sub_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    PATH = f\"LED_xsci_finetuned_epoch2_run{i}\"\n",
    "    \n",
    "    torch.save(LEDmodel.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interim Tests (on Epoch 2 Run 13 Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "C:\\Users\\JustinTo\\anaconda3\\envs\\w266_torch\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"In @cite, the authors describe an agent architecture that allows the agent to execute two different tasks in a real-time, dynamic, multi-agent domain. This architecture is similar to the so-called Soar'' architecture, which is a variant of the Soar integrated architecture. However, the main difference is that the agent is able to perform both the two tasks at once, and the agents are able to do both the same tasks. In contrast, our work focuses on the problem of how an agent should act strategically, and how it should act as a simple price-taker, while the agents can do both.\"]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[0][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1158 to 2048 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The complexity of solving non-cooperative games with at least one NE has been studied in @cite. In this paper, we consider the class @math of all @math -player games with @math NE such that the players' utility functions satisfy a certain (differential) constraint. We also consider the case @math where the players have access to @math and @math, where @math is the number of iterations required to find a Nash equilibrium. In this setting, the problem is to find an @math @math objective function that satisfies a certain @math constraint. The problem of solving @math in a non-linear manner is considered in the context of convex and concave functions, where the goal is to minimize every function in a given class using as few queries as possible. In contrast, in the case of non-convex problems, the complexity of finding a @math optimal function is defined as @math.\"]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = xsci_test_processed[2942][\"abstracts\"]\n",
    "inputs = LEDtokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "global_attention_mask = torch.zeros_like(inputs['attention_mask']).to(\"cuda\")\n",
    "global_attention_mask[:, 0] = 1\n",
    "\n",
    "predicted_abstract_ids = LEDmodel.generate(inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'], \n",
    "                                           global_attention_mask=global_attention_mask, \n",
    "                                           max_length=200, \n",
    "                                           num_beams=4)\n",
    "\n",
    "predicted_abstract = LEDtokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "\n",
    "predicted_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(epoch, run_num, batchsize=2, no_repeat_ngram_size=3):\n",
    "    '''\n",
    "    epoch: int of 1 or 2, corresponding to the training epoch of the LED model\n",
    "    run_num: int from  1 to 20, for calling the saved model checkpoints\n",
    "    '''\n",
    "    print(f\"-----Processing Results for Epoch {epoch} Run {run_num} Model-----\")\n",
    "    \n",
    "    ## Locating model saves and loading it to the common model object\n",
    "    if epoch==1:\n",
    "        PATH = f\"model_saves/epoch1/LED_xsci_finetuned_run{run_num}\"\n",
    "    elif epoch==2:\n",
    "        PATH = f\"model_saves/epoch2/LED_xsci_finetuned_epoch2_run{run_num}\"\n",
    "    \n",
    "    LEDmodel_tuned.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    ## Generating results\n",
    "    answers = []\n",
    "\n",
    "    for i in range(0, xsci_test.num_rows, batchsize):\n",
    "        if i%2500 == 0:\n",
    "            print(f\"Handling sample {i} for epoch {epoch} run {run_num} model now..\")\n",
    "        \n",
    "        answers.append(generate_abstract_batched(start=i, batch_size=batchsize, no_repeat_ngram_size=no_repeat_ngram_size))\n",
    "    \n",
    "    print(f\"Completed, {i+1} data points handled for epoch {epoch} run {run_num} model.\")\n",
    "    \n",
    "    formatted_answers = []\n",
    "    \n",
    "    for answer in answers:\n",
    "        formatted_answers += answer\n",
    "\n",
    "    ## Pickling results\n",
    "    SAVE_PATH = f\"answers_revised/epoch{epoch}/LED_xsci_finetuned_run{run_num}.pkl\"\n",
    "    with open(SAVE_PATH, \"wb\") as f:\n",
    "        pickle.dump(formatted_answers, f)\n",
    "    \n",
    "    return formatted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"./\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024,\n",
      "    1024\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 16384,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\JustinTo/.cache\\huggingface\\hub\\models--allenai--led-base-16384\\snapshots\\38335783885b338d93791936c54bb4be46bebed9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
      "\n",
      "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at allenai/led-base-16384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## Model object to be used by the testing_model function\n",
    "LEDmodel_tuned = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\").to(\"cuda\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Processing Results for Epoch 1 Run 1 Model-----\n",
      "Handling sample 0 for epoch 1 run 1 model now..\n",
      "Handling sample 2500 for epoch 1 run 1 model now..\n",
      "Handling sample 5000 for epoch 1 run 1 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 1 model.\n",
      "-----Processing Results for Epoch 1 Run 2 Model-----\n",
      "Handling sample 0 for epoch 1 run 2 model now..\n",
      "Handling sample 2500 for epoch 1 run 2 model now..\n",
      "Handling sample 5000 for epoch 1 run 2 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 2 model.\n",
      "-----Processing Results for Epoch 1 Run 3 Model-----\n",
      "Handling sample 0 for epoch 1 run 3 model now..\n",
      "Handling sample 2500 for epoch 1 run 3 model now..\n",
      "Handling sample 5000 for epoch 1 run 3 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 3 model.\n",
      "-----Processing Results for Epoch 1 Run 4 Model-----\n",
      "Handling sample 0 for epoch 1 run 4 model now..\n",
      "Handling sample 2500 for epoch 1 run 4 model now..\n",
      "Handling sample 5000 for epoch 1 run 4 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 4 model.\n",
      "-----Processing Results for Epoch 1 Run 5 Model-----\n",
      "Handling sample 0 for epoch 1 run 5 model now..\n",
      "Handling sample 2500 for epoch 1 run 5 model now..\n",
      "Handling sample 5000 for epoch 1 run 5 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 5 model.\n",
      "-----Processing Results for Epoch 1 Run 6 Model-----\n",
      "Handling sample 0 for epoch 1 run 6 model now..\n",
      "Handling sample 2500 for epoch 1 run 6 model now..\n",
      "Handling sample 5000 for epoch 1 run 6 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 6 model.\n",
      "-----Processing Results for Epoch 1 Run 7 Model-----\n",
      "Handling sample 0 for epoch 1 run 7 model now..\n",
      "Handling sample 2500 for epoch 1 run 7 model now..\n",
      "Handling sample 5000 for epoch 1 run 7 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 7 model.\n",
      "-----Processing Results for Epoch 1 Run 8 Model-----\n",
      "Handling sample 0 for epoch 1 run 8 model now..\n",
      "Handling sample 2500 for epoch 1 run 8 model now..\n",
      "Handling sample 5000 for epoch 1 run 8 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 8 model.\n",
      "-----Processing Results for Epoch 1 Run 9 Model-----\n",
      "Handling sample 0 for epoch 1 run 9 model now..\n",
      "Handling sample 2500 for epoch 1 run 9 model now..\n",
      "Handling sample 5000 for epoch 1 run 9 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 9 model.\n",
      "-----Processing Results for Epoch 1 Run 10 Model-----\n",
      "Handling sample 0 for epoch 1 run 10 model now..\n",
      "Handling sample 2500 for epoch 1 run 10 model now..\n",
      "Handling sample 5000 for epoch 1 run 10 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 10 model.\n",
      "-----Processing Results for Epoch 1 Run 11 Model-----\n",
      "Handling sample 0 for epoch 1 run 11 model now..\n",
      "Handling sample 2500 for epoch 1 run 11 model now..\n",
      "Handling sample 5000 for epoch 1 run 11 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 11 model.\n",
      "-----Processing Results for Epoch 1 Run 12 Model-----\n",
      "Handling sample 0 for epoch 1 run 12 model now..\n",
      "Handling sample 2500 for epoch 1 run 12 model now..\n",
      "Handling sample 5000 for epoch 1 run 12 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 12 model.\n",
      "-----Processing Results for Epoch 1 Run 13 Model-----\n",
      "Handling sample 0 for epoch 1 run 13 model now..\n",
      "Handling sample 2500 for epoch 1 run 13 model now..\n",
      "Handling sample 5000 for epoch 1 run 13 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 13 model.\n",
      "-----Processing Results for Epoch 1 Run 14 Model-----\n",
      "Handling sample 0 for epoch 1 run 14 model now..\n",
      "Handling sample 2500 for epoch 1 run 14 model now..\n",
      "Handling sample 5000 for epoch 1 run 14 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 14 model.\n",
      "-----Processing Results for Epoch 1 Run 15 Model-----\n",
      "Handling sample 0 for epoch 1 run 15 model now..\n",
      "Handling sample 2500 for epoch 1 run 15 model now..\n",
      "Handling sample 5000 for epoch 1 run 15 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 15 model.\n",
      "-----Processing Results for Epoch 1 Run 16 Model-----\n",
      "Handling sample 0 for epoch 1 run 16 model now..\n",
      "Handling sample 2500 for epoch 1 run 16 model now..\n",
      "Handling sample 5000 for epoch 1 run 16 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 16 model.\n",
      "-----Processing Results for Epoch 1 Run 17 Model-----\n",
      "Handling sample 0 for epoch 1 run 17 model now..\n",
      "Handling sample 2500 for epoch 1 run 17 model now..\n",
      "Handling sample 5000 for epoch 1 run 17 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 17 model.\n",
      "-----Processing Results for Epoch 1 Run 18 Model-----\n",
      "Handling sample 0 for epoch 1 run 18 model now..\n",
      "Handling sample 2500 for epoch 1 run 18 model now..\n",
      "Handling sample 5000 for epoch 1 run 18 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 18 model.\n",
      "-----Processing Results for Epoch 1 Run 19 Model-----\n",
      "Handling sample 0 for epoch 1 run 19 model now..\n",
      "Handling sample 2500 for epoch 1 run 19 model now..\n",
      "Handling sample 5000 for epoch 1 run 19 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 19 model.\n",
      "-----Processing Results for Epoch 1 Run 20 Model-----\n",
      "Handling sample 0 for epoch 1 run 20 model now..\n",
      "Handling sample 2500 for epoch 1 run 20 model now..\n",
      "Handling sample 5000 for epoch 1 run 20 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 20 model.\n",
      "-----Processing Results for Epoch 2 Run 1 Model-----\n",
      "Handling sample 0 for epoch 2 run 1 model now..\n",
      "Handling sample 2500 for epoch 2 run 1 model now..\n",
      "Handling sample 5000 for epoch 2 run 1 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 1 model.\n",
      "-----Processing Results for Epoch 2 Run 2 Model-----\n",
      "Handling sample 0 for epoch 2 run 2 model now..\n",
      "Handling sample 2500 for epoch 2 run 2 model now..\n",
      "Handling sample 5000 for epoch 2 run 2 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 2 model.\n",
      "-----Processing Results for Epoch 2 Run 3 Model-----\n",
      "Handling sample 0 for epoch 2 run 3 model now..\n",
      "Handling sample 2500 for epoch 2 run 3 model now..\n",
      "Handling sample 5000 for epoch 2 run 3 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 3 model.\n",
      "-----Processing Results for Epoch 2 Run 4 Model-----\n",
      "Handling sample 0 for epoch 2 run 4 model now..\n",
      "Handling sample 2500 for epoch 2 run 4 model now..\n",
      "Handling sample 5000 for epoch 2 run 4 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 4 model.\n",
      "-----Processing Results for Epoch 2 Run 5 Model-----\n",
      "Handling sample 0 for epoch 2 run 5 model now..\n",
      "Handling sample 2500 for epoch 2 run 5 model now..\n",
      "Handling sample 5000 for epoch 2 run 5 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 5 model.\n",
      "-----Processing Results for Epoch 2 Run 6 Model-----\n",
      "Handling sample 0 for epoch 2 run 6 model now..\n",
      "Handling sample 2500 for epoch 2 run 6 model now..\n",
      "Handling sample 5000 for epoch 2 run 6 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 6 model.\n",
      "-----Processing Results for Epoch 2 Run 7 Model-----\n",
      "Handling sample 0 for epoch 2 run 7 model now..\n",
      "Handling sample 2500 for epoch 2 run 7 model now..\n",
      "Handling sample 5000 for epoch 2 run 7 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 7 model.\n",
      "-----Processing Results for Epoch 2 Run 8 Model-----\n",
      "Handling sample 0 for epoch 2 run 8 model now..\n",
      "Handling sample 2500 for epoch 2 run 8 model now..\n",
      "Handling sample 5000 for epoch 2 run 8 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 8 model.\n",
      "-----Processing Results for Epoch 2 Run 9 Model-----\n",
      "Handling sample 0 for epoch 2 run 9 model now..\n",
      "Handling sample 2500 for epoch 2 run 9 model now..\n",
      "Handling sample 5000 for epoch 2 run 9 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 9 model.\n",
      "-----Processing Results for Epoch 2 Run 10 Model-----\n",
      "Handling sample 0 for epoch 2 run 10 model now..\n",
      "Handling sample 2500 for epoch 2 run 10 model now..\n",
      "Handling sample 5000 for epoch 2 run 10 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 10 model.\n",
      "-----Processing Results for Epoch 2 Run 11 Model-----\n",
      "Handling sample 0 for epoch 2 run 11 model now..\n",
      "Handling sample 2500 for epoch 2 run 11 model now..\n",
      "Handling sample 5000 for epoch 2 run 11 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 11 model.\n",
      "-----Processing Results for Epoch 2 Run 12 Model-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 for epoch 2 run 12 model now..\n",
      "Handling sample 2500 for epoch 2 run 12 model now..\n",
      "Handling sample 5000 for epoch 2 run 12 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 12 model.\n",
      "-----Processing Results for Epoch 2 Run 13 Model-----\n",
      "Handling sample 0 for epoch 2 run 13 model now..\n",
      "Handling sample 2500 for epoch 2 run 13 model now..\n",
      "Handling sample 5000 for epoch 2 run 13 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 13 model.\n",
      "-----Processing Results for Epoch 2 Run 14 Model-----\n",
      "Handling sample 0 for epoch 2 run 14 model now..\n",
      "Handling sample 2500 for epoch 2 run 14 model now..\n",
      "Handling sample 5000 for epoch 2 run 14 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 14 model.\n",
      "-----Processing Results for Epoch 2 Run 15 Model-----\n",
      "Handling sample 0 for epoch 2 run 15 model now..\n",
      "Handling sample 2500 for epoch 2 run 15 model now..\n",
      "Handling sample 5000 for epoch 2 run 15 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 15 model.\n",
      "-----Processing Results for Epoch 2 Run 16 Model-----\n",
      "Handling sample 0 for epoch 2 run 16 model now..\n",
      "Handling sample 2500 for epoch 2 run 16 model now..\n",
      "Handling sample 5000 for epoch 2 run 16 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 16 model.\n",
      "-----Processing Results for Epoch 2 Run 17 Model-----\n",
      "Handling sample 0 for epoch 2 run 17 model now..\n",
      "Handling sample 2500 for epoch 2 run 17 model now..\n",
      "Handling sample 5000 for epoch 2 run 17 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 17 model.\n",
      "-----Processing Results for Epoch 2 Run 18 Model-----\n",
      "Handling sample 0 for epoch 2 run 18 model now..\n",
      "Handling sample 2500 for epoch 2 run 18 model now..\n",
      "Handling sample 5000 for epoch 2 run 18 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 18 model.\n",
      "-----Processing Results for Epoch 2 Run 19 Model-----\n",
      "Handling sample 0 for epoch 2 run 19 model now..\n",
      "Handling sample 2500 for epoch 2 run 19 model now..\n",
      "Handling sample 5000 for epoch 2 run 19 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 19 model.\n",
      "-----Processing Results for Epoch 2 Run 20 Model-----\n",
      "Handling sample 0 for epoch 2 run 20 model now..\n",
      "Handling sample 2500 for epoch 2 run 20 model now..\n",
      "Handling sample 5000 for epoch 2 run 20 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 20 model.\n"
     ]
    }
   ],
   "source": [
    "model_list = [(epoch, num) for epoch in [1,2] for num in range(1,21)]\n",
    "\n",
    "for (epoch, num) in model_list:\n",
    "    testing_model(epoch, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Processing Results for Epoch 1 Run 5 Model-----\n",
      "Handling sample 0 for epoch 1 run 5 model now..\n",
      "Handling sample 2500 for epoch 1 run 5 model now..\n",
      "Handling sample 5000 for epoch 1 run 5 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 5 model.\n",
      "-----Processing Results for Epoch 1 Run 10 Model-----\n",
      "Handling sample 0 for epoch 1 run 10 model now..\n",
      "Handling sample 2500 for epoch 1 run 10 model now..\n",
      "Handling sample 5000 for epoch 1 run 10 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 10 model.\n",
      "-----Processing Results for Epoch 1 Run 15 Model-----\n",
      "Handling sample 0 for epoch 1 run 15 model now..\n",
      "Handling sample 2500 for epoch 1 run 15 model now..\n",
      "Handling sample 5000 for epoch 1 run 15 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 15 model.\n",
      "-----Processing Results for Epoch 1 Run 20 Model-----\n",
      "Handling sample 0 for epoch 1 run 20 model now..\n",
      "Handling sample 2500 for epoch 1 run 20 model now..\n",
      "Handling sample 5000 for epoch 1 run 20 model now..\n",
      "Completed, 5093 data points handled for epoch 1 run 20 model.\n",
      "-----Processing Results for Epoch 2 Run 5 Model-----\n",
      "Handling sample 0 for epoch 2 run 5 model now..\n",
      "Handling sample 2500 for epoch 2 run 5 model now..\n",
      "Handling sample 5000 for epoch 2 run 5 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 5 model.\n",
      "-----Processing Results for Epoch 2 Run 10 Model-----\n",
      "Handling sample 0 for epoch 2 run 10 model now..\n",
      "Handling sample 2500 for epoch 2 run 10 model now..\n",
      "Handling sample 5000 for epoch 2 run 10 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 10 model.\n",
      "-----Processing Results for Epoch 2 Run 15 Model-----\n",
      "Handling sample 0 for epoch 2 run 15 model now..\n",
      "Handling sample 2500 for epoch 2 run 15 model now..\n",
      "Handling sample 5000 for epoch 2 run 15 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 15 model.\n",
      "-----Processing Results for Epoch 2 Run 20 Model-----\n",
      "Handling sample 0 for epoch 2 run 20 model now..\n",
      "Handling sample 2500 for epoch 2 run 20 model now..\n",
      "Handling sample 5000 for epoch 2 run 20 model now..\n",
      "Completed, 5093 data points handled for epoch 2 run 20 model.\n"
     ]
    }
   ],
   "source": [
    "## Forgot to set no_repeat in previous attempt\n",
    "model_list = [(epoch, num) for epoch in [1,2] for num in [5, 10, 15, 20]]\n",
    "\n",
    "for (epoch, num) in model_list:\n",
    "    testing_model(epoch, num, batchsize=4, no_repeat_ngram_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex - Experimenting on Model Generation Settings (No Repeat Size  3 or 4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEDmodel_tuned.load_state_dict(torch.load(\"model_saves/epoch2/LED_xsci_finetuned_epoch2_run20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "ans_ngram3 = []\n",
    "\n",
    "for i in range(0, 5093, 4):\n",
    "    if i%2500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    ans_ngram3.append(generate_abstract_batched(start=i, batch_size=4, no_repeat_ngram_size=3))\n",
    "\n",
    "formatted_ans_ngram3 = []\n",
    "for answer in ans_ngram3:\n",
    "    formatted_ans_ngram3 += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"LED_finetuned_xsci_e20run20_ngram3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_ans_ngram3, f)\n",
    "    \n",
    "print(f\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 2500 now..\n",
      "Handling sample 5000 now..\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "ans_ngram4 = []\n",
    "\n",
    "for i in range(0, 5093, 4):\n",
    "    if i%2500 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    ans_ngram4.append(generate_abstract_batched(start=i, batch_size=4, no_repeat_ngram_size=4))\n",
    "\n",
    "formatted_ans_ngram4 = []\n",
    "for answer in ans_ngram4:\n",
    "    formatted_ans_ngram4 += answer\n",
    "\n",
    "## Pickling results\n",
    "with open(\"LED_finetuned_xsci_e20run20_ngram4.pkl\", \"wb\") as f:\n",
    "    pickle.dump(formatted_ans_ngram4, f)\n",
    "    \n",
    "print(f\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.3813711320704865, recall=0.28798908574822707, fmeasure=0.30719964455298093), mid=Score(precision=0.385196298048035, recall=0.29079798345562435, fmeasure=0.30955672894560166), high=Score(precision=0.388919632335763, recall=0.2934464769679644, fmeasure=0.3118527661508592)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.07517289022320789, recall=0.0557909164459903, fmeasure=0.059834220377468564), mid=Score(precision=0.07694911592888685, recall=0.057209942769505126, fmeasure=0.061203507491571295), high=Score(precision=0.07878705713280762, recall=0.05856802489925904, fmeasure=0.06256084049955803)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.213353992233361, recall=0.16000138575075687, fmeasure=0.1704098278035507), mid=Score(precision=0.21565608237447165, recall=0.16165503479061244, fmeasure=0.17176970210299825), high=Score(precision=0.21794042180156895, recall=0.1633619302792455, fmeasure=0.17320074465032362)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.2133320360400375, recall=0.1597467040858027, fmeasure=0.17040595466066732), mid=Score(precision=0.2156162662851328, recall=0.16157298407477533, fmeasure=0.17168502608964112), high=Score(precision=0.21783071210907037, recall=0.16331208127688196, fmeasure=0.1730672860848029))}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=formatted_ans_ngram3,\n",
    "              references=[ref for ref in xsci_test_processed['related_work']],\n",
    "              use_stemmer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.40278879236470455, recall=0.2909581512989938, fmeasure=0.31606615321787757), mid=Score(precision=0.40674574629572097, recall=0.29399932746932667, fmeasure=0.3184340969028486), high=Score(precision=0.41056635675749015, recall=0.29697638599058634, fmeasure=0.3207943230861719)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.08349068030388455, recall=0.0596003896036496, fmeasure=0.0649115511802695), mid=Score(precision=0.08544592979438131, recall=0.06113972400727348, fmeasure=0.06634563059014897), high=Score(precision=0.08743061848498744, recall=0.06261322121911049, fmeasure=0.06776992751551185)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.2277388336232596, recall=0.1632001460074887, fmeasure=0.17710356607022626), mid=Score(precision=0.22999767016672512, recall=0.1650482427490434, fmeasure=0.1785358321652826), high=Score(precision=0.23245825892377, recall=0.16690191236838464, fmeasure=0.18000472850224947)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.22760497668006588, recall=0.16312832829568916, fmeasure=0.17711657563974703), mid=Score(precision=0.22994024894690557, recall=0.16498449995433012, fmeasure=0.17844265861523068), high=Score(precision=0.23221941727824735, recall=0.16671863772023904, fmeasure=0.17990932204688692))}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=formatted_ans_ngram4,\n",
    "              references=[ref for ref in xsci_test_processed['related_work']],\n",
    "              use_stemmer = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': '1903.05238',\n",
       " 'mid': '2963943458',\n",
       " 'abstract': 'Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.',\n",
       " 'related_work': 'Grasping action is the most basic component of any interaction and it is composed of three major components @cite_21 . The first one is related to the process of approaching the arm and hand to the target object, considering the overall body movement. The second component focuses on the hand and body pre-shaping before the grasping action. Finally, the last component fits the hand to the geometry of the object by closing each of the fingers until contact is established.',\n",
       " 'ref_abstract': {'cite_N': ['@cite_21'],\n",
       "  'mid': ['1999329153'],\n",
       "  'abstract': ['Abstract This paper addresses the important issue of automating grasping movement in the animation of virtual actors, and presents a methodology and algorithm to generate realistic looking grasping motion of arbitrary shaped objects. A hybrid approach using both forward and inverse kinematics is proposed. A database of predefined body postures and hand trajectories are generalized to adapt to a specific grasp. The reachable space is divided into small subvolumes, which enables the construction of the database. The paper also addresses some common problems of articulated figure animation. A new approach for body positioning with kinematic constraints on both hands is described. An efficient and accurate manipulation of joint constraints is also presented. Finally, we describe an interpolation algorithm which interpolates between two postures of an articulated figure by moving the end effector along a specific trajectory and maintaining all the joint angles in the feasible range. Results are quite satisfactory, and some are shown in the paper.']}}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': '1903.05238',\n",
       " 'mid': '2963943458',\n",
       " 'abstract': 'Abstract Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes (e.g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On one hand, qualitative evaluation was used in the assessment of abstract aspects, such as motor control, finger movement realism, and interaction realism. On the other hand, for the quantitative evaluation a novel metric has been proposed to visually analyze the performed grips. Performance analysis results indicate that previous experience with our grasping system is not a prerequisite for an enjoyable, natural and intuitive VR interaction experience.',\n",
       " 'related_work': 'Grasping data-driven approaches have existed since a long time ago @cite_21 . These methods are based on large databases of predefined hand poses selected using user criteria or based on grasp taxonomies (i.e. final grasp poses when an object was successfully grasped) which provide us the ability to discriminate between different grasp types.',\n",
       " 'ref_abstract': {'cite_N': ['@cite_21'],\n",
       "  'mid': ['1999329153'],\n",
       "  'abstract': ['Abstract This paper addresses the important issue of automating grasping movement in the animation of virtual actors, and presents a methodology and algorithm to generate realistic looking grasping motion of arbitrary shaped objects. A hybrid approach using both forward and inverse kinematics is proposed. A database of predefined body postures and hand trajectories are generalized to adapt to a specific grasp. The reachable space is divided into small subvolumes, which enables the construction of the database. The paper also addresses some common problems of articulated figure animation. A new approach for body positioning with kinematic constraints on both hands is described. An efficient and accurate manipulation of joint constraints is also presented. Finally, we describe an interpolation algorithm which interpolates between two postures of an articulated figure by moving the end effector along a specific trajectory and maintaining all the joint angles in the feasible range. Results are quite satisfactory, and some are shown in the paper.']}}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "del LEDmodel_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling sample 0 now..\n",
      "Handling sample 4 now..\n",
      "Handling sample 8 now..\n"
     ]
    }
   ],
   "source": [
    "testans = []\n",
    "\n",
    "for i in range(0, 12, 4):\n",
    "    if i%4 == 0:\n",
    "        print(f\"Handling sample {i} now..\")\n",
    "        \n",
    "    testans.append(generate_abstract_batched(start=i, batch_size=4, no_repeat_ngram_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "    num_rows: 30369\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['related_work', 'abstracts'],\n",
       "    num_rows: 30369\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsci_train_processed"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0QDMDB_KqT8m",
    "C9xudUadMrFA",
    "6JSmlLkqtHQ5",
    "C89LpeBj7i4F"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0159a14369cb44f089b90fb2da2e2ca2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02655918f6db46e48dd99afce7d5efdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06a9e61925ff4058b07dc571f4049f16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "088d5810db964888b6355ff4b3006daf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e578056e46b4da69251a0146f7b113f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f345d2303cf455988917e97cb214934": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "103b22a1fe0949ce851df013d05d1d67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1180f7a202054e96b29be676e6d196aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "12ea9af71b74477c8b3345639bd8c3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "138d8cec4be8496aaa9c36de147b6f66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d0c5f1da4ad409e8fa49355313b1941",
      "max": 65,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7744ade8cdca46988dbdc476583c0e0f",
      "value": 65
     }
    },
    "16fe92dd15a148748327d4bf48653881": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18561ee747aa49cf84e87af027488992": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74f09b187b5e4d4eb0ad49068311c196",
       "IPY_MODEL_2ba10c8f3d00411b8e0b9e6ff52aa063",
       "IPY_MODEL_b1c3b65295fd4a22ae96ab2912fc0abf"
      ],
      "layout": "IPY_MODEL_8cf3aa4e4fa3480fb5db01ca3f95d578"
     }
    },
    "18c18207556c4f118b8cb9d6f118f9f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_421584437663421384560a8a12655858",
       "IPY_MODEL_584d42e006024a078b33622a255880bb",
       "IPY_MODEL_7614aa2d66e34e3ea2168e949b8a4eee"
      ],
      "layout": "IPY_MODEL_d28a664d7b1a43678fdbdbc2815d781d"
     }
    },
    "1a44ad75b6a04992836bffb4d47a9a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0a4591b1f2f4538a833de739d0c873b",
      "max": 1392,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c58384d3722e40b6828e29c98a6244f3",
      "value": 1392
     }
    },
    "1ad281ae8521407c89d18b6ee4e9bb43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b62088d67c645949aa5dab4925ec6ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddcef414cf36466c92a511846daec1e1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_23ef8acc67bf4db1b5b18dabd6d9a837",
      "value": "Downloading (â€¦)okenizer_config.json: 100%"
     }
    },
    "1c93c6887fbe425fbbc07ec1501ce91e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d3a366bf60d4fdea5a1d1b0a8a2fce6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2247119016c440668f363b5857596a0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4fb1e9ec774dc6bb7adb9ab8548431",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_741d2ecd9e724e56a300bcf8286dd73b",
      "value": "Downloading (â€¦)neration_config.json: 100%"
     }
    },
    "23ef8acc67bf4db1b5b18dabd6d9a837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24110a18d8cc40e1ad3ab4a44ddb5530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2447714557d14573b2763ab34d1c1e39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2514195ac95d4181afcabe197861068b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c799379c8841b3bd4a2a9cabbc4a34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "269ed10a1fa14376a626a4e3d77f729a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "276ef51f120b43c5945bbc163487eef1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28e6afaaa45a432dbcbd3f628ee4104e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2906bed67ba2498a9c386d74c4f153a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f88d820366ea4205b73a01b787c7497f",
       "IPY_MODEL_51a6700c89b94849b03a061c5e61c23c",
       "IPY_MODEL_f9ec7c8e832e482aa84e085118008fe7"
      ],
      "layout": "IPY_MODEL_5652013d7c0144739e4b2b1b5b9a43c2"
     }
    },
    "2b913ae6919d4b8a82375ac5321bee4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2ba10c8f3d00411b8e0b9e6ff52aa063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67d52d85d5c84b27aa3aa922ae7efb32",
      "max": 1120,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_692afe8728f8430ba9b619409dcbe08d",
      "value": 1120
     }
    },
    "2c4d2dec286e428f8a3910fdd8292f92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c7edaa6e3fb4aef8abb4207a60601f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cb2d4720eaa4a68b6dd4910bc92e93e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2afa23324074ead825abf9f1c12ab71",
       "IPY_MODEL_d18fdc17ada643498962b068957e61a4",
       "IPY_MODEL_9d80f5ab9f924cc487145ecae4b76d1a"
      ],
      "layout": "IPY_MODEL_06a9e61925ff4058b07dc571f4049f16"
     }
    },
    "2d24660c9e2746feb326f76db393e88f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2edd9a26c36945ebb8cc33972af496cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_088d5810db964888b6355ff4b3006daf",
      "max": 892146080,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6150d2af500048be9cab40e37c4035f5",
      "value": 892146080
     }
    },
    "3261b5b856cd46978e13690da45d8e2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "326bb70651d34aefb3e568f6b0e22d17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_712b6a5d35c44433a83b2e9a7c36d729",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d72d974832d54382830a63ec8facca02",
      "value": "Downloading (â€¦)cial_tokens_map.json: 100%"
     }
    },
    "338b2e9a4a1b4322b44b5edb574020ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38194abb46b948048fda0a679926f288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_766141d9dbfe4925a9cfbb06df68bb13",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ca30b603a55b43f7b294746913ef5fb7",
      "value": "Downloading (â€¦)ve/main/spiece.model: 100%"
     }
    },
    "3ac5b60afda54e698248e9416423edab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bd215f11414463ea2a326310b4ef0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0159a14369cb44f089b90fb2da2e2ca2",
      "max": 1912529,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b913ae6919d4b8a82375ac5321bee4a",
      "value": 1912529
     }
    },
    "3dba7a5f4e7b4cdab8b058bf9e9cb169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "421584437663421384560a8a12655858": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25c799379c8841b3bd4a2a9cabbc4a34",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f2b30cd19c7744659111bad4a81aa6bd",
      "value": "Downloading (â€¦)&quot;tf_model.h5&quot;;: 100%"
     }
    },
    "43e27a7e9e1e4378be0520ef47a3734a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f5058fc8794956a8428ca9ddc06c5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45cbb8fe700d4988bdb85c4155dd7268": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "464bb290c51d45d68d29bec268b0f90e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4704e6458cb54b36bc8e76923cf13b15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_269ed10a1fa14376a626a4e3d77f729a",
      "max": 6270,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d24660c9e2746feb326f76db393e88f",
      "value": 6270
     }
    },
    "4773d98e83ee438983ca3f3b338d8257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8f1419ce7c14f7998903674dd22024e",
       "IPY_MODEL_e7a1b4dfc0d04682b7dd0dbf970da6d9",
       "IPY_MODEL_77054765426947adaec7ad9ec6f52203"
      ],
      "layout": "IPY_MODEL_5fc12d8ec4c7494bb8b34f2e3b172bb5"
     }
    },
    "4832cf81dbd54b78816766e57f5b6325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485015da79684256be43c75a1cf01966": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "492115ece3b64a89a0ac65a492b82add": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49526ddcba0f4ac7bfa3c34e334df5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16fe92dd15a148748327d4bf48653881",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_02655918f6db46e48dd99afce7d5efdc",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "49748fda66674cf9bdbf5d8270568af0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4503cf67f4e4c0e93eb13252b608a3c",
       "IPY_MODEL_a4bba2d6bb7a43e084dcd290a8f418b5",
       "IPY_MODEL_ceeb2e0997c4453799c3e9cab0fe8077"
      ],
      "layout": "IPY_MODEL_4b3afc82da4047b88a4ebe0a75b12066"
     }
    },
    "4a9e7db9e26040cd81642b559d661952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b3afc82da4047b88a4ebe0a75b12066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c6fc49bc8b44c60a47b3a041928c4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e31fa7e86924da08e2bfb05794e99aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eaff597a65045eb827fa4949aa6f343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5039835bd0084a51912bb1a12a958faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "51a6700c89b94849b03a061c5e61c23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e6aa2e02e9743fd8488b9f6edc9fba2",
      "max": 147,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_492115ece3b64a89a0ac65a492b82add",
      "value": 147
     }
    },
    "52c4971d737445f1af77868ea329d033": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9baadaef783a42a3a5e51795d57706f4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4a9e7db9e26040cd81642b559d661952",
      "value": " 1.39k/1.39k [00:00&lt;00:00, 43.9kB/s]"
     }
    },
    "546def1aae7f48c2ae189e52b1067bea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5652013d7c0144739e4b2b1b5b9a43c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "584d42e006024a078b33622a255880bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f39af3d82c944db0b4fa773a703de606",
      "max": 2279695208,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b19488ea8824edb8735e720932cd89f",
      "value": 2279695208
     }
    },
    "585f8caac1804ace8944873c42da1146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_326bb70651d34aefb3e568f6b0e22d17",
       "IPY_MODEL_138d8cec4be8496aaa9c36de147b6f66",
       "IPY_MODEL_f9152823c04a4998a4f53fb9dbe3d19e"
      ],
      "layout": "IPY_MODEL_3dba7a5f4e7b4cdab8b058bf9e9cb169"
     }
    },
    "59399ffaee294c72ababd164ed4a6dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49526ddcba0f4ac7bfa3c34e334df5e3",
       "IPY_MODEL_1a44ad75b6a04992836bffb4d47a9a14",
       "IPY_MODEL_52c4971d737445f1af77868ea329d033"
      ],
      "layout": "IPY_MODEL_b14a7edf59af4cd6b9869ca6a1211117"
     }
    },
    "5b19488ea8824edb8735e720932cd89f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c9d306c880147299dab19c1e785f792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd89f1949644a70b41e7ddd327f0b6f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d809b096b7344d7ba074aa633520f67a",
      "value": " 792k/792k [00:00&lt;00:00, 5.33MB/s]"
     }
    },
    "5d30ab6db0834ba79de0c7fdd274f673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd89f1949644a70b41e7ddd327f0b6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eae9087fde444ccac09f1c8b4552536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38194abb46b948048fda0a679926f288",
       "IPY_MODEL_3bd215f11414463ea2a326310b4ef0b4",
       "IPY_MODEL_d28a35ceb30a454fae8593017bfa9a98"
      ],
      "layout": "IPY_MODEL_d2d7a69fb0de4e7eb6dcdbc94fb8dc38"
     }
    },
    "5fc12d8ec4c7494bb8b34f2e3b172bb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6150d2af500048be9cab40e37c4035f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "629fbe09bd9e435faa3e877c5cc90002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "648f386240b24d828955e2deaa238465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2812e195b174c95bf56811c5054e31a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2c4d2dec286e428f8a3910fdd8292f92",
      "value": "Downloading (â€¦)okenizer_config.json: 100%"
     }
    },
    "64eee87e73924271a849c3f3712f7681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "656b0b61c2394539bfbe77acb0efd371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6579c47eff21431db8ac8e2981d1f82f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "669ac6ddf900466486c2faff63d8898c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c7edaa6e3fb4aef8abb4207a60601f0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e8169982e2c8441bafd699e19b340a00",
      "value": "Downloading (â€¦)&quot;tf_model.h5&quot;;: 100%"
     }
    },
    "67d52d85d5c84b27aa3aa922ae7efb32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "692afe8728f8430ba9b619409dcbe08d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a9616699803464a8442c9c6e0b626e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d0c5f1da4ad409e8fa49355313b1941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e69e479dde44d2ea4bde6eda1264523": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "710bf9e376784313a383578b487846df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "712b6a5d35c44433a83b2e9a7c36d729": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "739138a2d9de4f3db72bfe7bb150692b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741d2ecd9e724e56a300bcf8286dd73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74f09b187b5e4d4eb0ad49068311c196": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_276ef51f120b43c5945bbc163487eef1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fea28d3215af4df2846eeb885ccb6a3e",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "75484201a2f44da7801d3595b647dc2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "756318c8f0fd49069cd7635b17f7f168": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7614aa2d66e34e3ea2168e949b8a4eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95b7a158d81c4a0199d2b1722a83803b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_64eee87e73924271a849c3f3712f7681",
      "value": " 2.28G/2.28G [00:41&lt;00:00, 58.5MB/s]"
     }
    },
    "766141d9dbfe4925a9cfbb06df68bb13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769df0ae198a43b6baaacbddd4c54c61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcbf31199f6f4ea5b5203072afc7c660",
       "IPY_MODEL_4704e6458cb54b36bc8e76923cf13b15",
       "IPY_MODEL_bef2feb8bbde4f7fb4a7a585fcbe8a8d"
      ],
      "layout": "IPY_MODEL_0f345d2303cf455988917e97cb214934"
     }
    },
    "76b9271a9822407ab9f0c3ed97d7d819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6579c47eff21431db8ac8e2981d1f82f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4eaff597a65045eb827fa4949aa6f343",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 36.9kB/s]"
     }
    },
    "77054765426947adaec7ad9ec6f52203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0c5c49a71b84f7ba017810a608353ea",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_78aa8a2450c2474786f157f2d004a050",
      "value": " 1.91M/1.91M [00:00&lt;00:00, 11.1MB/s]"
     }
    },
    "7744ade8cdca46988dbdc476583c0e0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78aa8a2450c2474786f157f2d004a050": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79d7c83d7961458394e5b8cc8d63d1e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c0ec7f4b0774123933ec959122665ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_338b2e9a4a1b4322b44b5edb574020ae",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3261b5b856cd46978e13690da45d8e2a",
      "value": " 259/259 [00:00&lt;00:00, 8.75kB/s]"
     }
    },
    "7ffe93a700b24f189a0739a2154eb5ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "831882be87be44a1b5343e2fd5f439b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9652511a13574bfebe686997f1539b15",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_12ea9af71b74477c8b3345639bd8c3bb",
      "value": " 88.0/88.0 [00:00&lt;00:00, 3.24kB/s]"
     }
    },
    "8829805b91154b73afaa050309305777": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cf3aa4e4fa3480fb5db01ca3f95d578": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d4fb1e9ec774dc6bb7adb9ab8548431": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e6aa2e02e9743fd8488b9f6edc9fba2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94c1990e6034419b9efe900c323c66c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "951c106900d14ea0b8f41977345e610d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95b7a158d81c4a0199d2b1722a83803b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960ea3181be64aebb819de6ea21e8725": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdf41896f526407d9f20c35dc3742f89",
       "IPY_MODEL_e289f9cda35e4cd783ddb74b0ff82375",
       "IPY_MODEL_76b9271a9822407ab9f0c3ed97d7d819"
      ],
      "layout": "IPY_MODEL_44f5058fc8794956a8428ca9ddc06c5c"
     }
    },
    "9652511a13574bfebe686997f1539b15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9baadaef783a42a3a5e51795d57706f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c32ed5d6e0a407aa0c306a4410d0fa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d80f5ab9f924cc487145ecae4b76d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ac5b60afda54e698248e9416423edab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0e578056e46b4da69251a0146f7b113f",
      "value": " 2.28G/2.28G [00:53&lt;00:00, 51.5MB/s]"
     }
    },
    "a0c6c0c3443e4f2d85d2333f692cc73a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a15e66d58cfd443984f8e9cb4d7727ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d3a366bf60d4fdea5a1d1b0a8a2fce6",
      "max": 88,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2eb10395edc41a893c1647a4a361cd6",
      "value": 88
     }
    },
    "a2c91c993af4442aa1fc3eefbd59651a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a49f45b5baac4019bf775439ca6fa54d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4bba2d6bb7a43e084dcd290a8f418b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_464bb290c51d45d68d29bec268b0f90e",
      "max": 65,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee3f43e0221a44118010a37912b6574c",
      "value": 65
     }
    },
    "a7f8bbaa80604f77aa332db3e0477a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4832cf81dbd54b78816766e57f5b6325",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1180f7a202054e96b29be676e6d196aa",
      "value": 791656
     }
    },
    "a8cda5eff16e46c68a1c3fea5635d5ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab50ea2ad8254de782dd7bcf6a7a0d04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0c5c49a71b84f7ba017810a608353ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b14a7edf59af4cd6b9869ca6a1211117": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b16aab12c4f84277bbdfb15d60c43147": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1c3b65295fd4a22ae96ab2912fc0abf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_546def1aae7f48c2ae189e52b1067bea",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_103b22a1fe0949ce851df013d05d1d67",
      "value": " 1.12k/1.12k [00:00&lt;00:00, 38.6kB/s]"
     }
    },
    "b4503cf67f4e4c0e93eb13252b608a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ad281ae8521407c89d18b6ee4e9bb43",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_710bf9e376784313a383578b487846df",
      "value": "Downloading (â€¦)cial_tokens_map.json: 100%"
     }
    },
    "b581db63cbd345da9b2e46111eb37c97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc71e26dd72c43f19f37de59591432c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bef2feb8bbde4f7fb4a7a585fcbe8a8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75484201a2f44da7801d3595b647dc2a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d6e66e7b8a0a49b495f2d3ccd79c9b54",
      "value": " 6.27k/6.27k [00:00&lt;00:00, 258kB/s]"
     }
    },
    "c15d073c2ece4cde83fae3c878b1c5fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2812e195b174c95bf56811c5054e31a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c411dc3b553e469e8065064318ffca25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b62088d67c645949aa5dab4925ec6ad",
       "IPY_MODEL_f988ccaf6afa40a3bdea7e4a91269806",
       "IPY_MODEL_f1166cffa1b844deb2af68a6e0b6e4b1"
      ],
      "layout": "IPY_MODEL_8829805b91154b73afaa050309305777"
     }
    },
    "c58384d3722e40b6828e29c98a6244f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca30b603a55b43f7b294746913ef5fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca53d418b94e4caf893f10cb508d86c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b16aab12c4f84277bbdfb15d60c43147",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f32cd421bdc04ce3abd95d286258be14",
      "value": "Downloading (â€¦)ve/main/spiece.model: 100%"
     }
    },
    "cb358eccde1f4bbc8d5f6ac93275f401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_648f386240b24d828955e2deaa238465",
       "IPY_MODEL_a15e66d58cfd443984f8e9cb4d7727ee",
       "IPY_MODEL_831882be87be44a1b5343e2fd5f439b6"
      ],
      "layout": "IPY_MODEL_656b0b61c2394539bfbe77acb0efd371"
     }
    },
    "ceeb2e0997c4453799c3e9cab0fe8077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8cda5eff16e46c68a1c3fea5635d5ad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_739138a2d9de4f3db72bfe7bb150692b",
      "value": " 65.0/65.0 [00:00&lt;00:00, 2.46kB/s]"
     }
    },
    "d0a4591b1f2f4538a833de739d0c873b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d18fdc17ada643498962b068957e61a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0c6c0c3443e4f2d85d2333f692cc73a",
      "max": 2275327883,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5039835bd0084a51912bb1a12a958faa",
      "value": 2275327883
     }
    },
    "d28a35ceb30a454fae8593017bfa9a98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_629fbe09bd9e435faa3e877c5cc90002",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d7c93df9700944838d001fa1f9872f8f",
      "value": " 1.91M/1.91M [00:00&lt;00:00, 9.37MB/s]"
     }
    },
    "d28a664d7b1a43678fdbdbc2815d781d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2d7a69fb0de4e7eb6dcdbc94fb8dc38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6e66e7b8a0a49b495f2d3ccd79c9b54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d72d974832d54382830a63ec8facca02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d768aff6bb1c459da4045fe9c88a0714": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c93df9700944838d001fa1f9872f8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d809b096b7344d7ba074aa633520f67a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d84bc17d0fe244acadebeaf917196da0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9f63bfebc9742948f3aef1a8710357c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcbf31199f6f4ea5b5203072afc7c660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_951c106900d14ea0b8f41977345e610d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b581db63cbd345da9b2e46111eb37c97",
      "value": "Downloading builder script: 100%"
     }
    },
    "ddcef414cf36466c92a511846daec1e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e289f9cda35e4cd783ddb74b0ff82375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a49f45b5baac4019bf775439ca6fa54d",
      "max": 1208,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2c91c993af4442aa1fc3eefbd59651a",
      "value": 1208
     }
    },
    "e2eb10395edc41a893c1647a4a361cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3e546fd09704bc788a791350f61a8e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_669ac6ddf900466486c2faff63d8898c",
       "IPY_MODEL_2edd9a26c36945ebb8cc33972af496cd",
       "IPY_MODEL_e6f30f97d6ff42cc884dbb464c7969d0"
      ],
      "layout": "IPY_MODEL_2514195ac95d4181afcabe197861068b"
     }
    },
    "e6f30f97d6ff42cc884dbb464c7969d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c32ed5d6e0a407aa0c306a4410d0fa2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_485015da79684256be43c75a1cf01966",
      "value": " 892M/892M [00:07&lt;00:00, 120MB/s]"
     }
    },
    "e7504cab44f442df860ed34739ced0db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2247119016c440668f363b5857596a0a",
       "IPY_MODEL_f1b256d2eedb418895f5919ea89252e0",
       "IPY_MODEL_7c0ec7f4b0774123933ec959122665ce"
      ],
      "layout": "IPY_MODEL_7ffe93a700b24f189a0739a2154eb5ba"
     }
    },
    "e7a1b4dfc0d04682b7dd0dbf970da6d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9f63bfebc9742948f3aef1a8710357c",
      "max": 1912529,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_756318c8f0fd49069cd7635b17f7f168",
      "value": 1912529
     }
    },
    "e8169982e2c8441bafd699e19b340a00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8f1419ce7c14f7998903674dd22024e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c93c6887fbe425fbbc07ec1501ce91e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4e31fa7e86924da08e2bfb05794e99aa",
      "value": "Downloading (â€¦)ve/main/spiece.model: 100%"
     }
    },
    "ecd0fcb30a6d4a4e93ee8cf8caa45fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca53d418b94e4caf893f10cb508d86c0",
       "IPY_MODEL_a7f8bbaa80604f77aa332db3e0477a62",
       "IPY_MODEL_5c9d306c880147299dab19c1e785f792"
      ],
      "layout": "IPY_MODEL_2447714557d14573b2763ab34d1c1e39"
     }
    },
    "ee3f43e0221a44118010a37912b6574c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1166cffa1b844deb2af68a6e0b6e4b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a9616699803464a8442c9c6e0b626e1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ab50ea2ad8254de782dd7bcf6a7a0d04",
      "value": " 87.0/87.0 [00:00&lt;00:00, 3.21kB/s]"
     }
    },
    "f1b256d2eedb418895f5919ea89252e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d768aff6bb1c459da4045fe9c88a0714",
      "max": 259,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24110a18d8cc40e1ad3ab4a44ddb5530",
      "value": 259
     }
    },
    "f2afa23324074ead825abf9f1c12ab71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d30ab6db0834ba79de0c7fdd274f673",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_94c1990e6034419b9efe900c323c66c3",
      "value": "Downloading (â€¦)&quot;pytorch_model.bin&quot;;: 100%"
     }
    },
    "f2b30cd19c7744659111bad4a81aa6bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f2c6e8c5634b40beadac4a6a7a633005": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f32cd421bdc04ce3abd95d286258be14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f39af3d82c944db0b4fa773a703de606": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f88d820366ea4205b73a01b787c7497f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45cbb8fe700d4988bdb85c4155dd7268",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f2c6e8c5634b40beadac4a6a7a633005",
      "value": "Downloading (â€¦)neration_config.json: 100%"
     }
    },
    "f9152823c04a4998a4f53fb9dbe3d19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c15d073c2ece4cde83fae3c878b1c5fb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4c6fc49bc8b44c60a47b3a041928c4cb",
      "value": " 65.0/65.0 [00:00&lt;00:00, 2.86kB/s]"
     }
    },
    "f988ccaf6afa40a3bdea7e4a91269806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc71e26dd72c43f19f37de59591432c3",
      "max": 87,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e69e479dde44d2ea4bde6eda1264523",
      "value": 87
     }
    },
    "f9ec7c8e832e482aa84e085118008fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79d7c83d7961458394e5b8cc8d63d1e9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_28e6afaaa45a432dbcbd3f628ee4104e",
      "value": " 147/147 [00:00&lt;00:00, 5.46kB/s]"
     }
    },
    "fdf41896f526407d9f20c35dc3742f89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43e27a7e9e1e4378be0520ef47a3734a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d84bc17d0fe244acadebeaf917196da0",
      "value": "Downloading (â€¦)lve/main/config.json: 100%"
     }
    },
    "fea28d3215af4df2846eeb885ccb6a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
