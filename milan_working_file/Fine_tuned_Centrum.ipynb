{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\milan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Preprocessing for Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (C:/Users/milan/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n",
      "100%|██████████| 3/3 [00:00<00:00, 158.28it/s]\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH_ENC = 4096\n",
    "MAX_LENGTH_DEC = 256\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\milan\\.cache\\huggingface\\datasets\\multi_x_science_sum\\default\\1.1.0\\2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729\\cache-36ea612be6b764b4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\milan\\.cache\\huggingface\\datasets\\multi_x_science_sum\\default\\1.1.0\\2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729\\cache-521ec846b4403907.arrow\n",
      "Loading cached processed dataset at C:\\Users\\milan\\.cache\\huggingface\\datasets\\multi_x_science_sum\\default\\1.1.0\\2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729\\cache-bfe27b0fca3375bf.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(example):\n",
    "\n",
    "    abstracts = example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "    related_work = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    ref_abstracts = filter(bool, example[\"ref_abstract\"][\"abstract\"])\n",
    "    output = {\n",
    "        \"abstracts\": f\"{abstracts}{DOC_SEP}{DOC_SEP.join(ref_abstracts)}\",\n",
    "        \"related_work\": related_work\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def preprocess_dataset_batched(example):\n",
    "    abstracts = [\n",
    "        abstract.split(\"| Abstract: \")[-1] + DOC_SEP + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        for abstract, ref_abstract in zip(example[\"abstract\"], example[\"ref_abstract\"])\n",
    "    ]\n",
    "    related_work = [pat.sub(\"@cite\", rw) for rw in example[\"related_work\"]]\n",
    "    output = {\n",
    "        \"abstracts\": abstracts,\n",
    "        \"related_work\": related_work,\n",
    "    }\n",
    "    return output\n",
    "\n",
    "dataset_processed = {}\n",
    "for split in dataset.keys():\n",
    "    dataset_processed[split] = dataset[split].map(\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[split].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 30369\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 5093\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['related_work', 'abstracts'],\n",
       "     num_rows: 5066\n",
       " })}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\config.json\n",
      "Model config LEDConfig {\n",
      "  \"_name_or_path\": \"ratishsp/Centrum\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"LEDForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_decoder_position_embeddings\": 1024,\n",
      "  \"max_encoder_position_embeddings\": 4096,\n",
      "  \"model_type\": \"led\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 50266\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\milan/.cache\\huggingface\\hub\\models--ratishsp--Centrum\\snapshots\\e9e32bd7ab7f460c1786f42e6e7f4f5697ace02d\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LEDForConditionalGeneration.\n",
      "\n",
      "All the weights of LEDForConditionalGeneration were initialized from the model checkpoint at ratishsp/Centrum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LEDForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "def get_tokenizer(host_tokenizer: str):\n",
    "  \"\"\"return the tokenizer and model for LLM training\"\"\"\n",
    "\n",
    "  return (AutoTokenizer.from_pretrained(host_tokenizer, \n",
    "                                        use_cache=False, \n",
    "                                        gradient_checkpointing=True), \n",
    "          AutoModelForSeq2SeqLM.from_pretrained(host_tokenizer, \n",
    "                                                use_cache=False, \n",
    "                                                gradient_checkpointing=True))\n",
    "\n",
    "\n",
    "centrum_tokenizer, centrum_model = get_tokenizer(\"ratishsp/Centrum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='ratishsp/Centrum', vocab_size=50265, model_max_len=16384, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}) LEDForConditionalGeneration(\n",
      "  (led): LEDModel(\n",
      "    (shared): Embedding(50266, 768, padding_idx=1)\n",
      "    (encoder): LEDEncoder(\n",
      "      (embed_tokens): Embedding(50266, 768, padding_idx=1)\n",
      "      (embed_positions): LEDLearnedPositionalEmbedding(4096, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x LEDEncoderLayer(\n",
      "          (self_attn): LEDEncoderAttention(\n",
      "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): LEDDecoder(\n",
      "      (embed_tokens): Embedding(50266, 768, padding_idx=1)\n",
      "      (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x LEDDecoderLayer(\n",
      "          (self_attn): LEDDecoderAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): LEDDecoderAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50266, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(centrum_tokenizer, centrum_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def tokenize_dataset_batched(example):\n",
    "    # Tokenizer input\n",
    "    input_encoding = centrum_tokenizer(\n",
    "        example[\"abstracts\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_ENC,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Tokenizer output\n",
    "    output_encoding = centrum_tokenizer(\n",
    "        example[\"related_work\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_DEC,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Modify output encoding to ignore padding in loss function\n",
    "    # torch ignore -100 in loss function computation\n",
    "    labels = output_encoding[\"input_ids\"].clone()\n",
    "    labels[labels == centrum_tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # Global attention with vectorized operations (optimized for GPU)\n",
    "    input_ids = input_encoding[\"input_ids\"]\n",
    "    docsep_token_id = centrum_tokenizer.convert_tokens_to_ids(DOC_SEP)\n",
    "    global_attention_mask = (input_ids == centrum_tokenizer.cls_token_id) | (input_ids == docsep_token_id)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_encoding[\"input_ids\"],\n",
    "        \"attention_mask\": input_encoding[\"attention_mask\"],\n",
    "        \"global_attention_mask\": global_attention_mask.float(),\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "centrum_tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "centrum_model.resize_token_embeddings(len(centrum_tokenizer))\n",
    "docsep_token_id = centrum_tokenizer.convert_tokens_to_ids(DOC_SEP)\n",
    "\n",
    "dataset_tokenized = {}\n",
    "for split in dataset_processed.keys():\n",
    "    dataset_tokenized[split] = (\n",
    "        dataset_processed[split]\n",
    "        .select(range(len(dataset_processed[split])))\n",
    "        .map(\n",
    "            tokenize_dataset_batched,\n",
    "            remove_columns=dataset_processed[split].column_names,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "     num_rows: 30369\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "     num_rows: 5093\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
       "     num_rows: 5066\n",
       " })}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = centrum_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = centrum_tokenizer.pad_token_id\n",
    "    label_str = centrum_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    fp16=True,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=250,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      " 13%|█▎        | 758/5694 [54:56<5:57:49,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=centrum_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30369\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3798\n",
      "  Number of trainable parameters = 152408832\n",
      " 13%|█▎        | 500/3798 [1:43:37<2:15:43,  2.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4396, 'learning_rate': 0.0001859639233370913, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 1000/3798 [2:04:57<1:55:21,  2.47s/it]***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2059, 'learning_rate': 0.00015777903043968434, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 26%|██▋       | 1000/3798 [2:07:39<1:55:21,  2.47s/it]Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1048591136932373, 'eval_runtime': 162.5592, 'eval_samples_per_second': 31.164, 'eval_steps_per_second': 1.95, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-1000\\pytorch_model.bin\n",
      " 39%|███▉      | 1500/3798 [2:28:19<1:34:49,  2.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1152, 'learning_rate': 0.00012959413754227734, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2000/3798 [2:48:49<1:13:14,  2.44s/it]***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9804, 'learning_rate': 0.00010140924464487037, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 53%|█████▎    | 2000/3798 [2:51:32<1:13:14,  2.44s/it]Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9712271690368652, 'eval_runtime': 162.9232, 'eval_samples_per_second': 31.094, 'eval_steps_per_second': 1.946, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-2000\\pytorch_model.bin\n",
      " 66%|██████▌   | 2500/3798 [3:12:08<53:20,  2.47s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7474, 'learning_rate': 7.328072153325818e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 3000/3798 [10:20:27<10:41:14, 48.21s/it] ***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7161, 'learning_rate': 4.509582863585119e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 79%|███████▉  | 3000/3798 [10:23:25<10:41:14, 48.21s/it]Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.889575481414795, 'eval_runtime': 178.1377, 'eval_samples_per_second': 28.439, 'eval_steps_per_second': 1.78, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./checkpoint-3000\\pytorch_model.bin\n",
      " 92%|█████████▏| 3500/3798 [10:46:45<14:01,  2.82s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6719, 'learning_rate': 1.6910935738444194e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3798/3798 [11:00:36<00:00,  2.04s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 3798/3798 [11:00:36<00:00, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39636.919, 'train_samples_per_second': 1.532, 'train_steps_per_second': 0.096, 'train_loss': 2.958127825808814, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3798, training_loss=2.958127825808814, metrics={'train_runtime': 39636.919, 'train_samples_per_second': 1.532, 'train_steps_per_second': 0.096, 'train_loss': 2.958127825808814, 'epoch': 2.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(centrum_model.state_dict(), \"centrum_xsci_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c22a1a313b5b99f8d6d4a168f63c77d589ec218f12dd769d82016c203a145817"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
