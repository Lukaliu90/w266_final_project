{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hgw3GTXLLw0"
      },
      "source": [
        "## ðŸ¤— Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ðŸ¤—"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7-QHmRiAMB9"
      },
      "source": [
        "The *Longformer Encoder-Decoder (LED)* was recently added as an extension to [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n",
        "\n",
        "In this notebook we will finetune *LED* for Summarization on [Pubmed](https://huggingface.co/datasets/viewer/?dataset=scientific_papers). *Pubmed* is a long-range summarization dataset, which makes it a good candidate for LED. LED will be finetuned up to an input length of 8K tokens on a single GPU.\n",
        "\n",
        "We will leverage ðŸ¤—`Seq2SeqTrainer`, gradient checkpointing and as usual ðŸ¤—`datasets`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B19PhgrCHM1"
      },
      "source": [
        "First, let's try to get a GPU with at least 15GB RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JnoCUoL-2Jz"
      },
      "outputs": [],
      "source": [
        "# crash colab to get more RAM\n",
        "# !kill -9 -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ11Va-qCp96"
      },
      "source": [
        "To check that we are having enough RAM we can run the following command.\n",
        "If the randomely allocated GPU is too small, the above cells can be run \n",
        "to crash the notebook hoping to get a better GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9IkphgF-90-",
        "outputId": "818225be-eab3-4cbf-f4bf-4d81ffa1e65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Mar 25 16:17:24 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 528.33       Driver Version: 528.33       CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   34C    P5    23W / 320W |   1303MiB / 10240MiB |     54%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1824    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      2472    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      3144    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
            "|    0   N/A  N/A      3544    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A      4360    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A      6388    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A      7792    C+G   ...1.0.4\\OverwolfBrowser.exe    N/A      |\n",
            "|    0   N/A  N/A      8012    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
            "|    0   N/A  N/A      8356    C+G   ...zilla Firefox\\firefox.exe    N/A      |\n",
            "|    0   N/A  N/A      8748    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A      8916    C+G   ...zilla Firefox\\firefox.exe    N/A      |\n",
            "|    0   N/A  N/A      9912    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
            "|    0   N/A  N/A     11556    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
            "|    0   N/A  N/A     12168    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
            "|    0   N/A  N/A     12472    C+G   ...86)\\Overwolf\\Overwolf.exe    N/A      |\n",
            "|    0   N/A  N/A     14080    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     14396    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     14812    C+G   ...oft\\OneDrive\\OneDrive.exe    N/A      |\n",
            "|    0   N/A  N/A     16740    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
            "|    0   N/A  N/A     17168    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSLE5TdC7RL"
      },
      "source": [
        "Next, we install ðŸ¤—Transformers, ðŸ¤—Datasets, and `rouge_score`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u9twLMqYEzT"
      },
      "source": [
        "Let's start by loading and preprocessing the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ODLQ8MUJfmi4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from functools import partial\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slSiCZFybmtG",
        "outputId": "6d4c31fb-4127-4977-dfc1-587accb230e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_6012\\1095751166.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge = load_metric(\"rouge\")\n"
          ]
        }
      ],
      "source": [
        "# Load the metric scoring object early\n",
        "rouge = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEZ20fa9DP4W"
      },
      "source": [
        "Next, we download the pubmed train and validation dataset ([click to see on ðŸ¤—Datasets Hub](https://huggingface.co/datasets/scientific_papers)). This can take a couple of minutes **â˜•** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ydpzesKycf42"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Optional\n",
        "def get_dataset(data: str, host: Optional[str] = None) -> Tuple:\n",
        "  \"\"\"Getting the training and validation data for our models\"\"\"\n",
        "\n",
        "  train_dataset = load_dataset(data, host, split=\"train\")\n",
        "  val_dataset = load_dataset(data, host, split=\"validation\")\n",
        "\n",
        "  return (train_dataset, val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWM_4AChFnpz"
      },
      "source": [
        "It's always a good idea to take a look at some data samples. Let's do that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iES1LXilfMxt",
        "outputId": "68ea7a0e-9655-42a3-d9fe-94751d3766be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset multi_x_science_sum (C:/Users/milan/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n",
            "Found cached dataset multi_x_science_sum (C:/Users/milan/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
          ]
        }
      ],
      "source": [
        "led_train, led_val = get_dataset(data=\"scientific_papers\", host=\"pubmed\")\n",
        "print(led_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eaz72rzWGBAK",
        "outputId": "71ee9313-19a6-4444-8bd0-a7ef027f0129"
      },
      "outputs": [],
      "source": [
        "def show_random_elements(dataset, num_examples=4):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jpUr9QeebZ-n"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Non-consecutive added token '<doc-sep>' found. Should have index 50266 but has index 50265 in saved vocabulary.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[39mreturn\u001b[39;00m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(host_tokenizer)\n\u001b[0;32m      9\u001b[0m \u001b[39m# led_tokenizer = get_tokenizer(\"allenai/led-base-16384\")\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m centrum_tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mratishsp/Centrum\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(host_tokenizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tokenizer\u001b[39m(host_tokenizer: \u001b[39mstr\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"return the tokenizer for LLM training\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m   \u001b[39mreturn\u001b[39;00m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(host_tokenizer)\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:385\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1762\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1759\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1760\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(file_path, resolved_vocab_files[file_id]))\n\u001b[1;32m-> 1762\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained(\n\u001b[0;32m   1763\u001b[0m     resolved_vocab_files, pretrained_model_name_or_path, init_configuration, \u001b[39m*\u001b[39minit_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   1764\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1869\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     added_tok_encoder_sorted \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39msorted\u001b[39m(added_tok_encoder\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m]))\n\u001b[0;32m   1868\u001b[0m     \u001b[39mfor\u001b[39;00m token, index \u001b[39min\u001b[39;00m added_tok_encoder_sorted:\n\u001b[1;32m-> 1869\u001b[0m         \u001b[39massert\u001b[39;00m index \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(tokenizer), (\n\u001b[0;32m   1870\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNon-consecutive added token \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m found. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1871\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShould have index \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(tokenizer)\u001b[39m}\u001b[39;00m\u001b[39m but has index \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m in saved vocabulary.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1872\u001b[0m         )\n\u001b[0;32m   1873\u001b[0m         tokenizer\u001b[39m.\u001b[39madd_tokens(token, special_tokens\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(token \u001b[39min\u001b[39;00m special_tokens))\n\u001b[0;32m   1875\u001b[0m \u001b[39m# Check all our special tokens are registered as \"no split\" token (we don't cut them) and are in the vocab\u001b[39;00m\n",
            "\u001b[1;31mAssertionError\u001b[0m: Non-consecutive added token '<doc-sep>' found. Should have index 50266 but has index 50265 in saved vocabulary."
          ]
        }
      ],
      "source": [
        "# Non-consecutive added token '<doc-sep>' found. Should have index 50266 but has index 50265 in saved vocabulary (Centrum).\n",
        "\n",
        "def get_tokenizer(host_tokenizer: str):\n",
        "  \"\"\"return the tokenizer for LLM training\"\"\"\n",
        "\n",
        "  return AutoTokenizer.from_pretrained(host_tokenizer)\n",
        "\n",
        "\n",
        "led_tokenizer = get_tokenizer(\"allenai/led-base-16384\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhQeQg3oCcL-"
      },
      "source": [
        "Note that for the sake of this notebook, we finetune the \"smaller\" LED checkpoint [\"allenai/led-base-16384\"](https://huggingface.co/allenai/led-base-16384). Better performance can however be attained by finetuning [\"allenai/led-large-16384\"](https://huggingface.co/allenai/led-large-16384) at the cost of a higher required GPU RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_SAv1JE40f"
      },
      "source": [
        "Now, let's write down the input data processing function that will be used to map each data sample to the correct model format.\n",
        "As explained earlier `article` represents here our input data and `abstract` is the target data. The datasamples are thus tokenized up to the respective maximum lengths of 8192 and 512.\n",
        "\n",
        "In addition to the usual `attention_mask`, LED can make use of an additional `global_attention_mask` defining which input tokens are attended globally and which are attended only locally, just as it's the case of [Longformer](https://huggingface.co/transformers/model_doc/longformer.html). For more information on Longformer's self-attention, please take a look at the corresponding [docs](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention). For summarization, we follow recommendations of the [paper](https://arxiv.org/abs/2004.05150) and use global attention only for the very first token. Finally, we make sure that no loss is computed on padded tokens by setting their index to `-100`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lEcAaZhNY8ge"
      },
      "outputs": [],
      "source": [
        "# Setting up input/output parameters\n",
        "max_input_length = 8192\n",
        "max_output_length = 512\n",
        "batch_size = 2\n",
        "\n",
        "def process_data_to_model_inputs(batch, model_tokenizer):\n",
        "    # tokenize the inputs and labels\n",
        "    inputs = model_tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length,\n",
        "    )\n",
        "    outputs = model_tokenizer(\n",
        "        batch[\"abstract\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_output_length,\n",
        "    )\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "\n",
        "    # create 0 global_attention_mask lists\n",
        "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
        "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
        "    ]\n",
        "\n",
        "    # since above lists are references, the following line changes the 0 index for all samples\n",
        "    batch[\"global_attention_mask\"][0][0] = 1\n",
        "    batch[\"labels\"] = outputs.input_ids\n",
        "\n",
        "    # We have to make sure that the PAD token is ignored\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == model_tokenizer.pad_token_id else token for token in labels]\n",
        "        for labels in batch[\"labels\"]\n",
        "    ]\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyseRmC5IcXj"
      },
      "source": [
        "For the sake of this notebook, we will reduce the training and validation data \n",
        "to a dummy dataset of sizes 250 and 25 respectively. For a full training run, those lines should be commented out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNzZZACtIoCD"
      },
      "source": [
        "Great, having defined the mapping function, let's preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55,
          "referenced_widgets": [
            "5c684056797043269f6a1044f3826672",
            "6f77c6d3513942dbb2a829e79aba202e",
            "c8c49882d9f648f2b7bda229ea09ffc2",
            "2ad956eb35ee47929b6d63a8a1a80e58",
            "da1538e0ae2140329fe745b006cda1fd",
            "09abcfbf26b84b54878aacf9d2dd9cda",
            "f80b3c216a534bd1aac3366981801bf5",
            "a06cc8a0357b42d1920f8c76772ef33c",
            "13d698207227436e9f3b6ea750f80ace",
            "91ba04ee706f40698ff6a2525e6692bb",
            "f267a6cfbf404b9a86870e1787c46bfa"
          ]
        },
        "id": "8RClMjZOZCPO",
        "outputId": "fee235d1-e9f8-4800-d5a8-bd25b72870cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\milan\\.cache\\huggingface\\datasets\\scientific_papers\\pubmed\\1.1.1\\306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f\\cache-efe70b533a7ae5c8.arrow\n",
            "Loading cached processed dataset at C:\\Users\\milan\\.cache\\huggingface\\datasets\\scientific_papers\\pubmed\\1.1.1\\306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f\\cache-64007b77b81ec3a8.arrow\n"
          ]
        }
      ],
      "source": [
        "def prep_and_convert_data(train: datasets.arrow_dataset.Dataset, validation: datasets.arrow_dataset.Dataset, train_range: Optional[int] = None, validation_range: Optional[int] = None) -> Tuple:\n",
        "  \"\"\"Processing the training and validation dataset to be trained\"\"\"\n",
        "\n",
        "  processed_model_data = partial(process_data_to_model_inputs, model_tokenizer=led_tokenizer)\n",
        "\n",
        "  if train_range and validation_range:\n",
        "    train_dataset = train.select(range(train_range))\n",
        "    val_dataset = validation.select(range(validation_range))\n",
        "  else:\n",
        "    train_dataset = train\n",
        "    val_dataset = validation\n",
        "\n",
        "  train_dataset = train_dataset.map(\n",
        "      processed_model_data,\n",
        "      batched=True,\n",
        "      batch_size=batch_size,\n",
        "      remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        "  )\n",
        "  val_dataset = val_dataset.map(\n",
        "    processed_model_data,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"abstract\", \"section_names\"],\n",
        "  )\n",
        "  train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "  )\n",
        "  val_dataset.set_format(\n",
        "      type=\"torch\",\n",
        "      columns=[\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\"],\n",
        "  )\n",
        "\n",
        "  return (train_dataset, val_dataset)\n",
        "\n",
        "\n",
        "train_dataset, val_dataset = prep_and_convert_data(train=led_train, validation=led_val, train_range=200, validation_range=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJQlLDKJHIK"
      },
      "source": [
        "We've decided to stick to the smaller model `\"allenai/led-base-16384\"` for the sake of this notebook. In addition, we directly enable gradient checkpointing and disable the caching mechanism to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYd-jwjaD5h",
        "outputId": "bf6b034c-cb0f-4da1-a1cb-fda58d555a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'global_attention_mask', 'labels'],\n",
            "    num_rows: 40\n",
            "}) tensor([[    0,  3618,     8,  ...,  -100,  -100,  -100],\n",
            "        [    0,  3618,   627,  ...,  -100,  -100,  -100],\n",
            "        [    0,  1437, 50118,  ...,  -100,  -100,  -100],\n",
            "        ...,\n",
            "        [    0, 31892,   741,  ...,  -100,  -100,  -100],\n",
            "        [    0, 14701,   876,  ...,  -100,  -100,  -100],\n",
            "        [    0, 37788, 10395,  ...,  -100,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "print(val_dataset, val_dataset['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R-UfEo0Zadpl"
      },
      "outputs": [],
      "source": [
        "def get_model(model_host: str):\n",
        "  \"\"\"Get either the LED or Centrum model\"\"\"\n",
        "\n",
        "  return AutoModelForSeq2SeqLM.from_pretrained(model_host, gradient_checkpointing=True, use_cache=False)\n",
        "\n",
        "led = get_model(model_host=\"allenai/led-base-16384\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOaX6eRJXkM"
      },
      "source": [
        "During training, we want to evaluate the model on Rouge, the most common metric used in summarization, to make sure the model is indeed improving during training. For this, we set fitting generation parameters. We'll use beam search with a small beam of just 2 to save memory. Also, we force the model to generate at least 100 tokens, but no more than 512. In addition, some other generation parameters are set that have been found helpful for generation. For more information on those parameters, please take a look at the [docs](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kPnNi_tWaklV"
      },
      "outputs": [],
      "source": [
        "# set generate hyperparameters\n",
        "led.config.num_beams = 2\n",
        "led.config.max_length = 512\n",
        "led.config.min_length = 100\n",
        "led.config.length_penalty = 2.0\n",
        "led.config.early_stopping = True\n",
        "led.config.no_repeat_ngram_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1ahmf7mcZzU7"
      },
      "outputs": [],
      "source": [
        "# Compute metrics for rouge\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = led_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = led_tokenizer.pad_token_id\n",
        "    label_str = led_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrX2gBnYLAna"
      },
      "source": [
        "Now, we're ready to start training. Let's import the `Seq2SeqTrainer` and `Seq2SeqTrainingArguments`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga-fCOB4LI4W"
      },
      "source": [
        "In contrast to the usual `Trainer`, the `Seq2SeqTrainer` makes it possible to use the `generate()` function during evaluation. This should be enabled with `predict_with_generate=True`. Because our GPU RAM is limited, we make use of gradient accumulation by setting `gradient_accumulation_steps=4` to have an effective `batch_size` of 2 * 4 = 8.\n",
        "\n",
        "Other training arguments can be read upon in the [docs](https://huggingface.co/transformers/main_classes/trainer.html?highlight=trainingarguments#transformers.TrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mMehwI4QZqB_"
      },
      "outputs": [],
      "source": [
        "# enable fp16 apex training\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    output_dir=\"./\",\n",
        "    logging_steps=5,\n",
        "    eval_steps=10,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZCY-C5mLzXY"
      },
      "source": [
        "The training arguments, along with the model, tokenizer, datasets and the `compute_metrics` function can then be passed to the `Seq2SeqTrainer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6WPyTYO_JfHW"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=led,\n",
        "    tokenizer=led_tokenizer,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZxSe_afL9TH"
      },
      "source": [
        "and we can start training. This will take about ~35min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "g4zkCpeQa2NN",
        "outputId": "8f66c8a1-f81b-40ff-edca-834033d525fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|â–ˆâ–ˆ        | 5/25 [00:46<03:06,  9.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.3828, 'learning_rate': 4e-05, 'epoch': 0.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [01:45<02:37, 10.47s/it]c:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9623, 'learning_rate': 3e-05, 'epoch': 0.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:Using default tokenizer.\n",
            "                                               \n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [19:46<02:37, 10.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.699963331222534, 'eval_rouge2_precision': 0.0798, 'eval_rouge2_recall': 0.1698, 'eval_rouge2_fmeasure': 0.1029, 'eval_runtime': 1080.5846, 'eval_samples_per_second': 0.037, 'epoch': 0.4}\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 770.00 MiB (GPU 0; 10.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 8.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:888\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    886\u001b[0m         tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m    887\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m    889\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_flos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfloating_point_ops(inputs)\n\u001b[0;32m    891\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[0;32m    892\u001b[0m     \u001b[39m# last step in epoch but step is always smaller than gradient_accumulation_steps\u001b[39;00m\n\u001b[0;32m    893\u001b[0m     steps_in_epoch \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m    894\u001b[0m     \u001b[39mand\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m steps_in_epoch\n\u001b[0;32m    895\u001b[0m ):\n\u001b[0;32m    896\u001b[0m     \u001b[39m# Gradient clipping\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py:1259\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1256\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_amp:\n\u001b[1;32m-> 1259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   1260\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   1261\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\checkpoint.py:157\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[1;34m(ctx, *args)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs_with_grad) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnone of output has requires_grad=True,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m this checkpoint() is not necessary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(outputs_with_grad, args_with_grad)\n\u001b[0;32m    158\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inp\u001b[39m.\u001b[39mgrad \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inp, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    159\u001b[0m               \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m detached_inputs)\n\u001b[0;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m) \u001b[39m+\u001b[39m grads\n",
            "File \u001b[1;32mc:\\Users\\milan\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 770.00 MiB (GPU 0; 10.00 GiB total capacity; 5.15 GiB already allocated; 0 bytes free; 8.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs1gYYb1MJOr"
      },
      "source": [
        "This completes the fine-tuning tutorial for LED. This training script with some small changes was used to train [this](https://huggingface.co/patrickvonplaten/led-large-16384-pubmed) checkpoint, called `\" patrickvonplaten/led-large-16384-pubmed\"` on a single GPU for ca. 3 days. Evaluating `\" patrickvonplaten/led-large-16384-pubmed\"` on Pubmed's test data gives a Rouge-2 score of **19.33** which is around 1 Rouge-2 point below SOTA performance on Pubmed.\n",
        "\n",
        "In the Appendix below, the condensed training and evaluation scripts that were used locally to finetune `\" patrickvonplaten/led-large-16384-pubmed\"` are attached."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09abcfbf26b84b54878aacf9d2dd9cda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13d698207227436e9f3b6ea750f80ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ad956eb35ee47929b6d63a8a1a80e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91ba04ee706f40698ff6a2525e6692bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f267a6cfbf404b9a86870e1787c46bfa",
            "value": " 44/50 [00:00&lt;00:00, 75.53 examples/s]"
          }
        },
        "5c684056797043269f6a1044f3826672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f77c6d3513942dbb2a829e79aba202e",
              "IPY_MODEL_c8c49882d9f648f2b7bda229ea09ffc2",
              "IPY_MODEL_2ad956eb35ee47929b6d63a8a1a80e58"
            ],
            "layout": "IPY_MODEL_da1538e0ae2140329fe745b006cda1fd"
          }
        },
        "6f77c6d3513942dbb2a829e79aba202e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09abcfbf26b84b54878aacf9d2dd9cda",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f80b3c216a534bd1aac3366981801bf5",
            "value": "Map:  88%"
          }
        },
        "91ba04ee706f40698ff6a2525e6692bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06cc8a0357b42d1920f8c76772ef33c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8c49882d9f648f2b7bda229ea09ffc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a06cc8a0357b42d1920f8c76772ef33c",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13d698207227436e9f3b6ea750f80ace",
            "value": 50
          }
        },
        "da1538e0ae2140329fe745b006cda1fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "f267a6cfbf404b9a86870e1787c46bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f80b3c216a534bd1aac3366981801bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
