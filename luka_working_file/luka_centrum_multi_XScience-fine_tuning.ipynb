{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c480166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rouge_score in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.24.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd2b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: evaluate in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (1.24.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2023.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.12.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: dill in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: pandas in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea4333-9dc0-4384-a2cd-c715b132b1d7",
   "metadata": {},
   "source": [
    "# Multi XScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1ee7c-5e35-45c4-8082-46c5ee7a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cf2b8a-046a-4db6-907c-789a2dd50c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c647b3-567f-4e53-9167-f54ab731c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8882d8a6-5dd4-4bd9-8599-c15840ef1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH_ENC = 4096\n",
    "MAX_LENGTH_DEC = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920814-72af-4d58-91bd-aa7dcccd5160",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d1a000-8e57-4144-9253-2779a6dc7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/_8_tbmdx5036fk6n1h6v9b680000gn/T/ipykernel_16993/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f359-5379-4d2b-8b24-51563c9a247f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17fce3f-f4ff-4ac9-93c9-4a33463d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (/Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d5d025e8174cbe8e701c7f59df863a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbc5af7-4c41-4d9a-9ac2-fb7a8842d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106f88f2-3ce7-44b4-ad41-998a39db1ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': 'math9912167',\n",
       " 'mid': '1631980677',\n",
       " 'abstract': 'Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.',\n",
       " 'related_work': 'Two other generalizations that can be considered are invariants of graphs in 3-manifolds, and invariants associated to other flat connections @cite_16 . We will analyze these in future work. Among other things, there should be a general relation between flat bundles and links in 3-manifolds on the one hand and finite covers and branched covers on the other hand @cite_26 .',\n",
       " 'ref_abstract': {'cite_N': ['@cite_16', '@cite_26'],\n",
       "  'mid': ['1481005306', '1641082372'],\n",
       "  'abstract': ['This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.',\n",
       "   'Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24ed-be47-4064-b363-bf872caee6de",
   "metadata": {},
   "source": [
    "## Format dataset to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57be3155-bad5-46d2-953c-3469e2ec2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e9e334-0a0b-4283-b68a-af7bfc46fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = (\n",
    "        example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "        + DOC_SEP\n",
    "        + DOC_SEP.join([x for x in example[\"ref_abstract\"][\"abstract\"] if x])\n",
    "    )\n",
    "    output[\"related_work\"] = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44487cf5-f19d-4d85-a28e-db276fb87df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c94c5ed-4ae3-4940-8523-d988860354ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30369 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_processed = {}\n",
    "for split in dataset.keys():\n",
    "    dataset_processed[split] = dataset[split].map(\n",
    "        # preprocess_dataset,\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[split].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab3779-c46c-4e93-bdbf-8dc682f1bca8",
   "metadata": {},
   "source": [
    "## Model 2: Fine-tune Centrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "328b32ec-684e-4756-94f4-2ee1b01e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4d3644-e019-42a3-b375-2ff7636ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d4aae9-cdfa-4e85-b494-616ba2f336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe87bc86-629a-4c89-9dd8-feaa769a3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_batched(example):\n",
    "    # Tokenizer input\n",
    "    output = tokenizer(\n",
    "        example[\"abstracts\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_ENC,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Tokenizer output\n",
    "    output[\"labels\"] = (\n",
    "        tokenizer(\n",
    "            example[\"related_work\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH_DEC,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        .input_ids\n",
    "    )\n",
    "    # Tokenizer output ignore padding in loss function\n",
    "    # torch ignore -100 in loss function computation\n",
    "    output[\"labels\"] = [\n",
    "        [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in labels\n",
    "        ]\n",
    "        for labels in output[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    # Global attention\n",
    "    output[\"global_attention_mask\"] = np.array(\n",
    "        [\n",
    "            [\n",
    "                1 if token in (tokenizer.cls_token_id, docsep_token_id) else 0 \n",
    "                for token in each\n",
    "            ]\n",
    "            for each in output[\"input_ids\"]\n",
    "        ], \n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f618211a-192a-49a7-a26b-c67b41cd9c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = {}\n",
    "\n",
    "for split in dataset_processed.keys():\n",
    "    dataset_tokenized[split] = (\n",
    "        dataset_processed[split]\n",
    "        .select(range(128))\n",
    "        .map(\n",
    "            tokenize_dataset_batched,\n",
    "            remove_columns=dataset_processed[split].column_names,\n",
    "            batched=True,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49cdae6-7a8a-424e-a5c3-34e113e38ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Set up the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    # use_mps_device=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0d57be4-0ce9-4505-8877-798b534a1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c196c644-b949-448c-847d-3adad17f5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 128\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 152408832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 45:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=4.0296125411987305, metrics={'train_runtime': 2837.5996, 'train_samples_per_second': 0.135, 'train_steps_per_second': 0.008, 'total_flos': 1036878656569344.0, 'train_loss': 4.0296125411987305, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16560c43-bf65-434f-a03d-bb8356b605ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 0/8 [00:00<?, ?it/s]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/8 [01:31<10:40, 91.46s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                      | 2/8 [03:07<09:23, 93.92s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             | 3/8 [04:39<07:46, 93.31s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                    | 4/8 [06:11<06:10, 92.67s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 5/8 [07:34<04:28, 89.35s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 6/8 [09:02<02:57, 88.87s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 7/8 [10:34<01:29, 89.74s/it]Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [12:07<00:00, 90.88s/it]\n"
     ]
    }
   ],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=torch.as_tensor(input_ids),\n",
    "            attention_mask=torch.as_tensor(attention_mask),\n",
    "            global_attention_mask=torch.as_tensor(global_attention_mask),\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59bff4ac-0551-4eb4-b3eb-c7bef357c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f993ff1-9d3f-4e66-89f7-48aa62fe4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f082e6e2-7bc3-45ce-b8dd-a1100e632acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "026fb5e2-ca46-469c-972c-768e73c06a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2933487638578587, recall=0.29200709832037636, fmeasure=0.2815715983753184), mid=Score(precision=0.31049575936025475, recall=0.30798508735871666, fmeasure=0.29432358485593413), high=Score(precision=0.3280951076358564, recall=0.3250701177873674, fmeasure=0.30592633047634554)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.04350232238478724, recall=0.04349789242499366, fmeasure=0.041407873852706695), mid=Score(precision=0.05183538594707471, recall=0.052840695590428914, fmeasure=0.04971452590462587), high=Score(precision=0.06038594419192698, recall=0.06344514610297083, fmeasure=0.058800723232261924)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15518199067772578, recall=0.15768831899322766, fmeasure=0.15026841424341691), mid=Score(precision=0.16485430398692857, recall=0.16904587700574214, fmeasure=0.15823517427974215), high=Score(precision=0.1745321488156862, recall=0.18136510192458355, fmeasure=0.16781062293071153)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.1552374114897623, recall=0.15677383438490985, fmeasure=0.15032775385130467), mid=Score(precision=0.1652847381019507, recall=0.1690507117462037, fmeasure=0.158490594758643), high=Score(precision=0.17454030425769723, recall=0.18160941089779872, fmeasure=0.16678198199626665))}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"][:128],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26fb22-d99e-47ab-b1c2-75e3a045792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeefe6-db00-4dc9-96a3-a6cf23045246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512dd50-d7e1-4f5c-90f2-ded6717a7c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1820-10e4-42d9-b079-292b9ee5c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9075b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36d115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28983863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143abd45-2e28-4c48-be98-35a8f615a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, EncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load pre-trained Longformer tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Load the pre-trained Longformer encoder and decoder\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('allenai/longformer-base-4096', 'allenai/longformer-base-4096')\n",
    "\n",
    "# Define your training and validation data\n",
    "train_data = ...\n",
    "val_data = ...\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9c7f-ba05-4328-872a-78ac9b4e596c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2bd306-2b45-4eac-b529-66d448e911a5",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32720f9-1878-4fac-83dc-89b2506f3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "led_output_model1 = model.generate(\n",
    "    **dataset_tokenized[\"test\"],\n",
    "    # input_ids=dataset_tokenized[\"test\"][\"input_ids\"][:n],\n",
    "    # attention_mask=dataset_tokenized[\"test\"][\"attention_mask\"][:n],\n",
    "    # global_attention_mask=dataset_tokenized[\"test\"][\"global_attention_mask\"][:n],\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=128,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6968c4ce-2231-493e-b5d1-8fa8168c1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [3:11:58<00:00, 143.98s/it]\n"
     ]
    }
   ],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8104ad4-368c-4597-b7fd-870edbc0fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a610a7bb-d9e0-43a6-96bd-f507154efd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5208799d-1f41-4035-8ad3-a0d1e941c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the Oculus Touch motion controllers. Our approach is flexible because it can be adapted to different hand meshes (e-g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "415f1b9a-17e4-451f-a61a-94392acae155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2982484009370646, recall=0.3057877687297647, fmeasure=0.2877549383290699), mid=Score(precision=0.3011000320975565, recall=0.308186529440017, fmeasure=0.28974742885739196), high=Score(precision=0.30423223116720244, recall=0.31058012470155627, fmeasure=0.2917896849338474)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.046860562392848866, recall=0.04742509910853629, fmeasure=0.04476525721466253), mid=Score(precision=0.04805986457733996, recall=0.048611032584090635, fmeasure=0.0458330562332894), high=Score(precision=0.04916571328652274, recall=0.0498130873004002, fmeasure=0.046871122190935706)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15540080811555254, recall=0.1636225890270709, fmeasure=0.15142010390078578), mid=Score(precision=0.15687893432752667, recall=0.16523845626801453, fmeasure=0.1524776573706927), high=Score(precision=0.15834376408384634, recall=0.16680563034698537, fmeasure=0.15364196168214198)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.15538972251299768, recall=0.16342358445792937, fmeasure=0.15136482317934177), mid=Score(precision=0.15683121294129587, recall=0.16520268561441434, fmeasure=0.15246824909372558), high=Score(precision=0.1583816066756327, recall=0.1666711839189804, fmeasure=0.15354401584643315))}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
