{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea4333-9dc0-4384-a2cd-c715b132b1d7",
   "metadata": {},
   "source": [
    "# Multi XScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1ee7c-5e35-45c4-8082-46c5ee7a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cf2b8a-046a-4db6-907c-789a2dd50c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c647b3-567f-4e53-9167-f54ab731c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8882d8a6-5dd4-4bd9-8599-c15840ef1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH_ENC = 512  #4096\n",
    "MAX_LENGTH_DEC = 256\n",
    "# N = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920814-72af-4d58-91bd-aa7dcccd5160",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d1a000-8e57-4144-9253-2779a6dc7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/_8_tbmdx5036fk6n1h6v9b680000gn/T/ipykernel_24524/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f359-5379-4d2b-8b24-51563c9a247f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17fce3f-f4ff-4ac9-93c9-4a33463d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (/Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab153c13054496bb50b45c48f38cd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbc5af7-4c41-4d9a-9ac2-fb7a8842d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106f88f2-3ce7-44b4-ad41-998a39db1ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': 'math9912167',\n",
       " 'mid': '1631980677',\n",
       " 'abstract': 'Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.',\n",
       " 'related_work': 'Two other generalizations that can be considered are invariants of graphs in 3-manifolds, and invariants associated to other flat connections @cite_16 . We will analyze these in future work. Among other things, there should be a general relation between flat bundles and links in 3-manifolds on the one hand and finite covers and branched covers on the other hand @cite_26 .',\n",
       " 'ref_abstract': {'cite_N': ['@cite_16', '@cite_26'],\n",
       "  'mid': ['1481005306', '1641082372'],\n",
       "  'abstract': ['This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.',\n",
       "   'Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24ed-be47-4064-b363-bf872caee6de",
   "metadata": {},
   "source": [
    "## Format dataset to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57be3155-bad5-46d2-953c-3469e2ec2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44487cf5-f19d-4d85-a28e-db276fb87df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c740d31-b794-4faf-b719-879bc8d69e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-f3232cb45e6040e6.arrow\n",
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-da1a80df5f4bf131.arrow\n",
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-238661e6a318eda9.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_processed = (\n",
    "    dataset\n",
    "    # .filter(lambda _, idx: idx < N, with_indices=True)\n",
    "    .map(\n",
    "        # preprocess_dataset,\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364904ca-acc1-408f-b1ba-086533ef0d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6302833-c174-4c80-9486-97d79ac22e64",
   "metadata": {},
   "source": [
    "## Model 2: Fine-tune Centrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "328b32ec-684e-4756-94f4-2ee1b01e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4d3644-e019-42a3-b375-2ff7636ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d4aae9-cdfa-4e85-b494-616ba2f336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe87bc86-629a-4c89-9dd8-feaa769a3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_batched(example):\n",
    "    # Tokenizer input\n",
    "    output = tokenizer(\n",
    "        example[\"abstracts\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_ENC,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Tokenizer output\n",
    "    output[\"labels\"] = (\n",
    "        tokenizer(\n",
    "            example[\"related_work\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH_DEC,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=False,\n",
    "        )\n",
    "        .input_ids\n",
    "    )\n",
    "    # Tokenizer output ignore padding in loss function\n",
    "    # torch ignore -100 in loss function computation\n",
    "    output[\"labels\"] = [\n",
    "        [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in labels\n",
    "        ]\n",
    "        for labels in output[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    # Global attention\n",
    "    output[\"global_attention_mask\"] = np.array(\n",
    "        [\n",
    "            [\n",
    "                1 if token in (tokenizer.cls_token_id, docsep_token_id) else 0 \n",
    "                for token in each\n",
    "            ]\n",
    "            for each in output[\"input_ids\"]\n",
    "        ], \n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e322891b-7ef5-4199-bc8c-012a0bf7f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-9c485eb4fb0ff28b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-e4d2ef224e30a11e.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = (\n",
    "    dataset_processed\n",
    "    .map(\n",
    "        tokenize_dataset_batched,\n",
    "        remove_columns=dataset_processed[\"train\"].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c3a8dc-8fcd-4deb-9692-9d4776bfa9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e49cdae6-7a8a-424e-a5c3-34e113e38ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    # predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # eval_steps=BATCH_SIZE//2,\n",
    "    eval_steps=10,\n",
    "    # save_steps=BATCH_SIZE//2,\n",
    "    save_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    # warmup_steps=BATCH_SIZE,\n",
    "    # warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    # logging_steps=BATCH_SIZE//4,\n",
    "    logging_steps=10,\n",
    "    # use_mps_device=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0d57be4-0ce9-4505-8877-798b534a1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f3118-0b83-48eb-bdb5-67971491ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30369\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 950\n",
      "  Number of trainable parameters = 152408832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 91/950 3:54:34 < 37:44:02, 0.01 it/s, Epoch 0.19/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.970200</td>\n",
       "      <td>3.686373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.715300</td>\n",
       "      <td>3.594553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.653600</td>\n",
       "      <td>3.545779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.636500</td>\n",
       "      <td>3.511709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.582500</td>\n",
       "      <td>3.486015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.559800</td>\n",
       "      <td>3.462022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.536400</td>\n",
       "      <td>3.451576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.528700</td>\n",
       "      <td>3.434519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/80 06:29 < 12:30, 0.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-10\n",
      "Configuration saved in ./checkpoint-10/config.json\n",
      "Configuration saved in ./checkpoint-10/generation_config.json\n",
      "Model weights saved in ./checkpoint-10/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-20\n",
      "Configuration saved in ./checkpoint-20/config.json\n",
      "Configuration saved in ./checkpoint-20/generation_config.json\n",
      "Model weights saved in ./checkpoint-20/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-30\n",
      "Configuration saved in ./checkpoint-30/config.json\n",
      "Configuration saved in ./checkpoint-30/generation_config.json\n",
      "Model weights saved in ./checkpoint-30/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-40\n",
      "Configuration saved in ./checkpoint-40/config.json\n",
      "Configuration saved in ./checkpoint-40/generation_config.json\n",
      "Model weights saved in ./checkpoint-40/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-50\n",
      "Configuration saved in ./checkpoint-50/config.json\n",
      "Configuration saved in ./checkpoint-50/generation_config.json\n",
      "Model weights saved in ./checkpoint-50/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-60\n",
      "Configuration saved in ./checkpoint-60/config.json\n",
      "Configuration saved in ./checkpoint-60/generation_config.json\n",
      "Model weights saved in ./checkpoint-60/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-70\n",
      "Configuration saved in ./checkpoint-70/config.json\n",
      "Configuration saved in ./checkpoint-70/generation_config.json\n",
      "Model weights saved in ./checkpoint-70/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./checkpoint-80\n",
      "Configuration saved in ./checkpoint-80/config.json\n",
      "Configuration saved in ./checkpoint-80/generation_config.json\n",
      "Model weights saved in ./checkpoint-80/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5066\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8667f-e432-48bc-8b6d-d9193d065b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.log_metrics(\"train\", train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcda7e9-dcc4-4cae-8b13-d37a6be2051e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f65dfe-f21c-473a-8ca6-53fc271efebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf08618-50e2-42f3-88f2-6688e5a6b85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a12483-9a74-4284-8069-e29198ce63ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffba24-03c4-428b-a370-1f3702d1b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd76dd9-9268-4e76-aab7-380dae1cd08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf5a50-d690-4414-aa76-e7b56ac685ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84bf99-ba84-4864-9bb2-6c8bfeb57793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d0550-d5d8-47b0-a801-bf26f2b39ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c6b63-e725-4b1b-b549-2c980a7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c196c644-b949-448c-847d-3adad17f5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4\n",
      "  Number of trainable parameters = 152408832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 03:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.253900</td>\n",
       "      <td>4.312594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.969000</td>\n",
       "      <td>4.312594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-2\n",
      "Configuration saved in ./checkpoint-2/config.json\n",
      "Configuration saved in ./checkpoint-2/generation_config.json\n",
      "Model weights saved in ./checkpoint-2/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-4\n",
      "Configuration saved in ./checkpoint-4/config.json\n",
      "Configuration saved in ./checkpoint-4/generation_config.json\n",
      "Model weights saved in ./checkpoint-4/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 10s, sys: 7min 41s, total: 16min 52s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cb6e2af-250e-4deb-8c9a-7fb2d8c91b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =    80472GF\n",
      "  train_loss               =     2.1585\n",
      "  train_runtime            = 0:03:58.61\n",
      "  train_samples_per_second =      0.134\n",
      "  train_steps_per_second   =      0.017\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf3410-750e-4ff9-97fd-60f9bbd8e4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7873e7-8f42-4275-9ee6-6fcb4f9f8eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d0a7b-94f7-48da-85e5-756fc102be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16560c43-bf65-434f-a03d-bb8356b605ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=torch.as_tensor(input_ids),\n",
    "            attention_mask=torch.as_tensor(attention_mask),\n",
    "            global_attention_mask=torch.as_tensor(global_attention_mask),\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59bff4ac-0551-4eb4-b3eb-c7bef357c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f993ff1-9d3f-4e66-89f7-48aa62fe4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f082e6e2-7bc3-45ce-b8dd-a1100e632acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "026fb5e2-ca46-469c-972c-768e73c06a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2933487638578587, recall=0.29200709832037636, fmeasure=0.2815715983753184), mid=Score(precision=0.31049575936025475, recall=0.30798508735871666, fmeasure=0.29432358485593413), high=Score(precision=0.3280951076358564, recall=0.3250701177873674, fmeasure=0.30592633047634554)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.04350232238478724, recall=0.04349789242499366, fmeasure=0.041407873852706695), mid=Score(precision=0.05183538594707471, recall=0.052840695590428914, fmeasure=0.04971452590462587), high=Score(precision=0.06038594419192698, recall=0.06344514610297083, fmeasure=0.058800723232261924)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15518199067772578, recall=0.15768831899322766, fmeasure=0.15026841424341691), mid=Score(precision=0.16485430398692857, recall=0.16904587700574214, fmeasure=0.15823517427974215), high=Score(precision=0.1745321488156862, recall=0.18136510192458355, fmeasure=0.16781062293071153)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.1552374114897623, recall=0.15677383438490985, fmeasure=0.15032775385130467), mid=Score(precision=0.1652847381019507, recall=0.1690507117462037, fmeasure=0.158490594758643), high=Score(precision=0.17454030425769723, recall=0.18160941089779872, fmeasure=0.16678198199626665))}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26fb22-d99e-47ab-b1c2-75e3a045792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeefe6-db00-4dc9-96a3-a6cf23045246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512dd50-d7e1-4f5c-90f2-ded6717a7c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1820-10e4-42d9-b079-292b9ee5c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9075b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36d115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28983863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143abd45-2e28-4c48-be98-35a8f615a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, EncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load pre-trained Longformer tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Load the pre-trained Longformer encoder and decoder\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('allenai/longformer-base-4096', 'allenai/longformer-base-4096')\n",
    "\n",
    "# Define your training and validation data\n",
    "train_data = ...\n",
    "val_data = ...\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Create a Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9c7f-ba05-4328-872a-78ac9b4e596c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2bd306-2b45-4eac-b529-66d448e911a5",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32720f9-1878-4fac-83dc-89b2506f3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "led_output_model1 = model.generate(\n",
    "    **dataset_tokenized[\"test\"],\n",
    "    # input_ids=dataset_tokenized[\"test\"][\"input_ids\"][:n],\n",
    "    # attention_mask=dataset_tokenized[\"test\"][\"attention_mask\"][:n],\n",
    "    # global_attention_mask=dataset_tokenized[\"test\"][\"global_attention_mask\"][:n],\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=128,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6968c4ce-2231-493e-b5d1-8fa8168c1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [3:11:58<00:00, 143.98s/it]\n"
     ]
    }
   ],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8104ad4-368c-4597-b7fd-870edbc0fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a610a7bb-d9e0-43a6-96bd-f507154efd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5208799d-1f41-4035-8ad3-a0d1e941c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the Oculus Touch motion controllers. Our approach is flexible because it can be adapted to different hand meshes (e-g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "415f1b9a-17e4-451f-a61a-94392acae155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2982484009370646, recall=0.3057877687297647, fmeasure=0.2877549383290699), mid=Score(precision=0.3011000320975565, recall=0.308186529440017, fmeasure=0.28974742885739196), high=Score(precision=0.30423223116720244, recall=0.31058012470155627, fmeasure=0.2917896849338474)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.046860562392848866, recall=0.04742509910853629, fmeasure=0.04476525721466253), mid=Score(precision=0.04805986457733996, recall=0.048611032584090635, fmeasure=0.0458330562332894), high=Score(precision=0.04916571328652274, recall=0.0498130873004002, fmeasure=0.046871122190935706)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15540080811555254, recall=0.1636225890270709, fmeasure=0.15142010390078578), mid=Score(precision=0.15687893432752667, recall=0.16523845626801453, fmeasure=0.1524776573706927), high=Score(precision=0.15834376408384634, recall=0.16680563034698537, fmeasure=0.15364196168214198)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.15538972251299768, recall=0.16342358445792937, fmeasure=0.15136482317934177), mid=Score(precision=0.15683121294129587, recall=0.16520268561441434, fmeasure=0.15246824909372558), high=Score(precision=0.1583816066756327, recall=0.1666711839189804, fmeasure=0.15354401584643315))}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
