{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea4333-9dc0-4384-a2cd-c715b132b1d7",
   "metadata": {},
   "source": [
    "# Multi XScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1ee7c-5e35-45c4-8082-46c5ee7a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AdamW, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cf2b8a-046a-4db6-907c-789a2dd50c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c647b3-567f-4e53-9167-f54ab731c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8882d8a6-5dd4-4bd9-8599-c15840ef1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH_ENC = 512  #4096\n",
    "MAX_LENGTH_DEC = 256\n",
    "# N = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920814-72af-4d58-91bd-aa7dcccd5160",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d1a000-8e57-4144-9253-2779a6dc7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fs/_8_tbmdx5036fk6n1h6v9b680000gn/T/ipykernel_31513/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f359-5379-4d2b-8b24-51563c9a247f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17fce3f-f4ff-4ac9-93c9-4a33463d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (/Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462381670bc649e9820aca6ae9a8c22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbc5af7-4c41-4d9a-9ac2-fb7a8842d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106f88f2-3ce7-44b4-ad41-998a39db1ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': 'math9912167',\n",
       " 'mid': '1631980677',\n",
       " 'abstract': 'Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.',\n",
       " 'related_work': 'Two other generalizations that can be considered are invariants of graphs in 3-manifolds, and invariants associated to other flat connections @cite_16 . We will analyze these in future work. Among other things, there should be a general relation between flat bundles and links in 3-manifolds on the one hand and finite covers and branched covers on the other hand @cite_26 .',\n",
       " 'ref_abstract': {'cite_N': ['@cite_16', '@cite_26'],\n",
       "  'mid': ['1481005306', '1641082372'],\n",
       "  'abstract': ['This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrodâ€“Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.',\n",
       "   'Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24ed-be47-4064-b363-bf872caee6de",
   "metadata": {},
   "source": [
    "## Format dataset to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57be3155-bad5-46d2-953c-3469e2ec2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44487cf5-f19d-4d85-a28e-db276fb87df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c740d31-b794-4faf-b719-879bc8d69e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-f3232cb45e6040e6.arrow\n",
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-da1a80df5f4bf131.arrow\n",
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-238661e6a318eda9.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_processed = (\n",
    "    dataset\n",
    "    # .filter(lambda _, idx: idx < N, with_indices=True)\n",
    "    .map(\n",
    "        # preprocess_dataset,\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364904ca-acc1-408f-b1ba-086533ef0d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['related_work', 'abstracts'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6302833-c174-4c80-9486-97d79ac22e64",
   "metadata": {},
   "source": [
    "## Model 2: Fine-tune Centrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "328b32ec-684e-4756-94f4-2ee1b01e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\"\n",
    "CHECKPOINT_MODEL = \"./checkpoint_512/checkpoint-950\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c4d3644-e019-42a3-b375-2ff7636ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d4aae9-cdfa-4e85-b494-616ba2f336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe87bc86-629a-4c89-9dd8-feaa769a3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_batched(example):\n",
    "    # Tokenizer input\n",
    "    output = tokenizer(\n",
    "        example[\"abstracts\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_ENC,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Tokenizer output\n",
    "    output[\"labels\"] = (\n",
    "        tokenizer(\n",
    "            example[\"related_work\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH_DEC,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=False,\n",
    "        )\n",
    "        .input_ids\n",
    "    )\n",
    "    # Tokenizer output ignore padding in loss function\n",
    "    # torch ignore -100 in loss function computation\n",
    "    output[\"labels\"] = [\n",
    "        [\n",
    "            -100 if token == tokenizer.pad_token_id else token\n",
    "            for token in labels\n",
    "        ]\n",
    "        for labels in output[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    # Global attention\n",
    "    output[\"global_attention_mask\"] = np.array(\n",
    "        [\n",
    "            [\n",
    "                1 if token in (tokenizer.cls_token_id, docsep_token_id) else 0 \n",
    "                for token in each\n",
    "            ]\n",
    "            for each in output[\"input_ids\"]\n",
    "        ], \n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e322891b-7ef5-4199-bc8c-012a0bf7f515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-9c485eb4fb0ff28b.arrow\n",
      "Loading cached processed dataset at /Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729/cache-b20b09fa3e8c6b66.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tokenized = (\n",
    "    dataset_processed\n",
    "    .map(\n",
    "        tokenize_dataset_batched,\n",
    "        remove_columns=dataset_processed[\"train\"].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5c3a8dc-8fcd-4deb-9692-9d4776bfa9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'global_attention_mask'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de8327-c3b2-48aa-babc-318a1a7953b1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c960e6-9333-4d91-a1bd-bec086e47c52",
   "metadata": {},
   "source": [
    "## evaluation with sample 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12b0f600-1852-40d8-b02a-0bca3f00247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 for evaluation\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "eval_idx = {}\n",
    "for split in dataset_processed.keys():\n",
    "    eval_idx[split] = rng.permutation(dataset_processed[split].shape[0])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69095e81-a260-418c-9078-84b84749312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [26:46<00:00, 100.43s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [24:56<00:00, 93.56s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [24:33<00:00, 92.07s/it]\n"
     ]
    }
   ],
   "source": [
    "model_pred = {}\n",
    "\n",
    "for split in dataset_tokenized.keys():\n",
    "    model_pred[split] = []\n",
    "    each_dataset = dataset_tokenized[split]\n",
    "    for i in tqdm(range(0, len(eval_idx[split]), BATCH_SIZE)):\n",
    "        \n",
    "        idx = eval_idx[split][i:i+BATCH_SIZE]\n",
    "\n",
    "        input_ids = torch.tensor(each_dataset[\"input_ids\"])[idx]\n",
    "        attention_mask = torch.tensor(each_dataset[\"attention_mask\"])[idx]\n",
    "        global_attention_mask = (\n",
    "            torch.tensor(each_dataset[\"global_attention_mask\"])[idx]\n",
    "        )\n",
    "\n",
    "        model_output = (\n",
    "            model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                global_attention_mask=global_attention_mask,\n",
    "                no_repeat_ngram_size=3,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model_pred[split].extend(\n",
    "            tokenizer.batch_decode(\n",
    "                model_output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "127b2b3a-653b-433c-9315-fae1bf9d515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neural representation learning @cite is a well-studied area in computer vision, and has been applied to many computer vision tasks, such as clustering, classification, visualization, and word sense disambiguation. In this paper, we propose a new approach for learning graph embeddings, which is based on structural measures of node similarities for generation of training data. The model learns nodes that are able to approximate a given measure such as the shortest path distance or any other. Our work differs from previous work in that our model learns low dimensional vectors to represent vertices appearing in a graph and, unlike existing work,',\n",
       " 'In @cite, the authors propose a type inference algorithm for Datalog with negation. The algorithm is based on a type system where equalities are tracked, and present a type-based algorithm. The results show that it is optimal for object-oriented Datalogy without negation, in the sense that the inferred type is as tight as possible.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdab42b0-b192-48f0-bada-2736622d899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.34746422014300804, recall=0.273249424007993, fmeasure=0.28604606562659757), mid=Score(precision=0.3559469499811089, recall=0.27943006179438545, fmeasure=0.2904711339670817), high=Score(precision=0.36410203631150506, recall=0.286097831511261, fmeasure=0.2954159014835912)), 'rouge2': AggregateScore(low=Score(precision=0.05959426010148445, recall=0.04487425598194418, fmeasure=0.04742389772735309), mid=Score(precision=0.06308240138774394, recall=0.04758573403068603, fmeasure=0.04994374901741308), high=Score(precision=0.06657248003259754, recall=0.05063040323261355, fmeasure=0.052788689654841556)), 'rougeL': AggregateScore(low=Score(precision=0.19306343998654202, recall=0.15141867755368166, fmeasure=0.15794940024314422), mid=Score(precision=0.1980041613985281, recall=0.15521645582490146, fmeasure=0.16071709105215864), high=Score(precision=0.2029374466991714, recall=0.15935784407547057, fmeasure=0.1636477424420036)), 'rougeLsum': AggregateScore(low=Score(precision=0.1933078051270838, recall=0.15151529632756722, fmeasure=0.1580730203456568), mid=Score(precision=0.1976885013564819, recall=0.15527969803705144, fmeasure=0.1607137203982148), high=Score(precision=0.20274546219037803, recall=0.1592257952694192, fmeasure=0.1633996496967782))}\n",
      "\n",
      "test\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.35241507593925475, recall=0.2722166805165999, fmeasure=0.287924775180605), mid=Score(precision=0.3599142995165337, recall=0.2783178340159952, fmeasure=0.29216347907756157), high=Score(precision=0.3666581142483567, recall=0.2844883557098049, fmeasure=0.2965787464719626)), 'rouge2': AggregateScore(low=Score(precision=0.05924164583482199, recall=0.04428034406206693, fmeasure=0.047249854059172126), mid=Score(precision=0.06307165038023865, recall=0.04703535646869152, fmeasure=0.050021687949888295), high=Score(precision=0.06695061872198414, recall=0.04993733566901862, fmeasure=0.05280633008538996)), 'rougeL': AggregateScore(low=Score(precision=0.1926311492038692, recall=0.14914925727988537, fmeasure=0.15684513400926045), mid=Score(precision=0.19797010587237396, recall=0.15272098791773264, fmeasure=0.1597507537244048), high=Score(precision=0.20282391601552827, recall=0.1564805827269019, fmeasure=0.1625630242278389)), 'rougeLsum': AggregateScore(low=Score(precision=0.1930594299194224, recall=0.14898764211159912, fmeasure=0.15694763752814223), mid=Score(precision=0.1977665598508927, recall=0.1527057113006126, fmeasure=0.15965287602634756), high=Score(precision=0.20293003480206254, recall=0.15684465817233195, fmeasure=0.16251184706882502))}\n",
      "\n",
      "validation\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.3471215897806331, recall=0.2747686632198652, fmeasure=0.28814210652922423), mid=Score(precision=0.3548344932661947, recall=0.28091173819038284, fmeasure=0.29285947182909944), high=Score(precision=0.362249314746841, recall=0.2874029426846766, fmeasure=0.2973031380354137)), 'rouge2': AggregateScore(low=Score(precision=0.060177678576269204, recall=0.04672211213657784, fmeasure=0.049025931737064626), mid=Score(precision=0.06377372382841755, recall=0.04929467588306663, fmeasure=0.051624074816610875), high=Score(precision=0.06781715163016734, recall=0.05239423099898111, fmeasure=0.054559184359191555)), 'rougeL': AggregateScore(low=Score(precision=0.18934851129820474, recall=0.14968995520974243, fmeasure=0.15662931081803602), mid=Score(precision=0.19463604114966265, recall=0.15391208596420308, fmeasure=0.15977906522933727), high=Score(precision=0.19979011945217282, recall=0.15790674458928952, fmeasure=0.1630134917630748)), 'rougeLsum': AggregateScore(low=Score(precision=0.18949031991222115, recall=0.15012999543648944, fmeasure=0.1569182196742331), mid=Score(precision=0.19466011588740834, recall=0.15404230525630597, fmeasure=0.15982249161112583), high=Score(precision=0.19969620349779046, recall=0.15825919680702707, fmeasure=0.16318282452347913))}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in dataset_tokenized.keys():\n",
    "    \n",
    "    model_pred[split] = [each.strip() for each in model_pred[split]]\n",
    "    \n",
    "    scores = rouge.compute(\n",
    "        predictions=model_pred[split],\n",
    "        references=(\n",
    "            np.array(dataset_processed[split][\"related_work\"])\n",
    "            [eval_idx[split]]\n",
    "        ),\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    print(split)\n",
    "    print(scores)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6613dd-5f9a-4ca1-a33a-e50f1e770e3b",
   "metadata": {},
   "source": [
    "### train sample 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaf08618-50e2-42f3-88f2-6688e5a6b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 for evaluation\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "eval_idx = {}\n",
    "for split in dataset_processed.keys():\n",
    "    eval_idx[split] = rng.permutation(dataset_processed[split].shape[0])[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4a12483-9a74-4284-8069-e29198ce63ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [52:18<00:00, 98.07s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [48:18<00:00, 90.56s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [48:13<00:00, 90.41s/it]\n"
     ]
    }
   ],
   "source": [
    "model_pred = {}\n",
    "\n",
    "for split in dataset_tokenized.keys():\n",
    "    model_pred[split] = []\n",
    "    each_dataset = dataset_tokenized[split]\n",
    "    for i in tqdm(range(0, len(eval_idx[split]), BATCH_SIZE)):\n",
    "        \n",
    "        idx = eval_idx[split][i:i+BATCH_SIZE]\n",
    "\n",
    "        input_ids = torch.tensor(each_dataset[\"input_ids\"])[idx]\n",
    "        attention_mask = torch.tensor(each_dataset[\"attention_mask\"])[idx]\n",
    "        global_attention_mask = (\n",
    "            torch.tensor(each_dataset[\"global_attention_mask\"])[idx]\n",
    "        )\n",
    "\n",
    "        model_output = (\n",
    "            model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                global_attention_mask=global_attention_mask,\n",
    "                no_repeat_ngram_size=3,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model_pred[split].extend(\n",
    "            tokenizer.batch_decode(\n",
    "                model_output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfffba24-03c4-428b-a370-1f3702d1b95b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.35391011334189376, recall=0.2737611459771506, fmeasure=0.28916646809152835), mid=Score(precision=0.35940752474614257, recall=0.2783476730762118, fmeasure=0.2924012530012049), high=Score(precision=0.36499001737976594, recall=0.282727809774108, fmeasure=0.29584997001026225)), 'rouge2': AggregateScore(low=Score(precision=0.06142128512453346, recall=0.046026066629214435, fmeasure=0.048951201528433894), mid=Score(precision=0.06388302825783397, recall=0.04801530560445494, fmeasure=0.050789875545480236), high=Score(precision=0.06669488169547416, recall=0.05021880024980211, fmeasure=0.05288513509201786)), 'rougeL': AggregateScore(low=Score(precision=0.1953893435988529, recall=0.1513425921569353, fmeasure=0.15910107951683983), mid=Score(precision=0.19862920523131505, recall=0.15400884197487658, fmeasure=0.16101314096447117), high=Score(precision=0.2018729706985693, recall=0.15688351413995336, fmeasure=0.16308482736658378)), 'rougeLsum': AggregateScore(low=Score(precision=0.19518140233824233, recall=0.15139849143445627, fmeasure=0.15905461831082468), mid=Score(precision=0.19876256731390507, recall=0.15396909037932313, fmeasure=0.161027246500726), high=Score(precision=0.2020439637947414, recall=0.1565336083457926, fmeasure=0.16298355035937634))}\n",
      "\n",
      "test\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.3596097667865363, recall=0.2753383624068532, fmeasure=0.2916092843522179), mid=Score(precision=0.36533210732115406, recall=0.27964653867196165, fmeasure=0.29509916552690074), high=Score(precision=0.3709484538709875, recall=0.28407465082747874, fmeasure=0.29836428835415424)), 'rouge2': AggregateScore(low=Score(precision=0.06234024169653126, recall=0.046318861240637055, fmeasure=0.04945663670864288), mid=Score(precision=0.06515996728609941, recall=0.048643744292474034, fmeasure=0.05158698276937197), high=Score(precision=0.06800021406313732, recall=0.051124535734234845, fmeasure=0.05367807557918804)), 'rougeL': AggregateScore(low=Score(precision=0.19789212546397666, recall=0.15116216269421157, fmeasure=0.1596378092856674), mid=Score(precision=0.20137038725613776, recall=0.15393839845225016, fmeasure=0.1616856358826487), high=Score(precision=0.20497982784579374, recall=0.15731723868194109, fmeasure=0.16379316304311084)), 'rougeLsum': AggregateScore(low=Score(precision=0.19758341301830049, recall=0.15119345815402824, fmeasure=0.15960714720005892), mid=Score(precision=0.2012389689444588, recall=0.1541032642900534, fmeasure=0.16166622952616977), high=Score(precision=0.20505548526750259, recall=0.1567725264430528, fmeasure=0.1636929923552409))}\n",
      "\n",
      "validation\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.3520784143147363, recall=0.275230213554294, fmeasure=0.2894446229522656), mid=Score(precision=0.35767558275897027, recall=0.27949312035587315, fmeasure=0.29269083437868676), high=Score(precision=0.3628649083444355, recall=0.2837885153118326, fmeasure=0.29585815065162097)), 'rouge2': AggregateScore(low=Score(precision=0.06010670248805438, recall=0.04604839357575193, fmeasure=0.04845859494222918), mid=Score(precision=0.06268948146008022, recall=0.04808862125158701, fmeasure=0.05046638977370599), high=Score(precision=0.06515383961626078, recall=0.05021060485192308, fmeasure=0.052400971539358764)), 'rougeL': AggregateScore(low=Score(precision=0.19310085435806343, recall=0.15075764755373294, fmeasure=0.1581549380959952), mid=Score(precision=0.19659708213441068, recall=0.1535585524118599, fmeasure=0.1600824939523483), high=Score(precision=0.2000567990118851, recall=0.15644027162486204, fmeasure=0.16226782925063155)), 'rougeLsum': AggregateScore(low=Score(precision=0.19290925224805464, recall=0.15082825851687992, fmeasure=0.15789676862284605), mid=Score(precision=0.19664986934589546, recall=0.15363532143838154, fmeasure=0.1600866786225818), high=Score(precision=0.19999560001571876, recall=0.1564585042293452, fmeasure=0.16244910365816095))}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in dataset_tokenized.keys():\n",
    "    \n",
    "    model_pred[split] = [each.strip() for each in model_pred[split]]\n",
    "    \n",
    "    scores = rouge.compute(\n",
    "        predictions=model_pred[split],\n",
    "        references=(\n",
    "            np.array(dataset_processed[split][\"related_work\"])\n",
    "            [eval_idx[split]]\n",
    "        ),\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    print(split)\n",
    "    print(scores)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd76dd9-9268-4e76-aab7-380dae1cd08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf5a50-d690-4414-aa76-e7b56ac685ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84bf99-ba84-4864-9bb2-6c8bfeb57793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d0550-d5d8-47b0-a801-bf26f2b39ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "499097b5-45ff-4d6e-ab31-35f91ca0070b",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c196c644-b949-448c-847d-3adad17f5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4\n",
      "  Number of trainable parameters = 152408832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 03:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.253900</td>\n",
       "      <td>4.312594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.969000</td>\n",
       "      <td>4.312594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-2\n",
      "Configuration saved in ./checkpoint-2/config.json\n",
      "Configuration saved in ./checkpoint-2/generation_config.json\n",
      "Model weights saved in ./checkpoint-2/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./checkpoint-4\n",
      "Configuration saved in ./checkpoint-4/config.json\n",
      "Configuration saved in ./checkpoint-4/generation_config.json\n",
      "Model weights saved in ./checkpoint-4/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 10s, sys: 7min 41s, total: 16min 52s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cb6e2af-250e-4deb-8c9a-7fb2d8c91b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               =    80472GF\n",
      "  train_loss               =     2.1585\n",
      "  train_runtime            = 0:03:58.61\n",
      "  train_samples_per_second =      0.134\n",
      "  train_steps_per_second   =      0.017\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", train_result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf3410-750e-4ff9-97fd-60f9bbd8e4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7873e7-8f42-4275-9ee6-6fcb4f9f8eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d0a7b-94f7-48da-85e5-756fc102be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16560c43-bf65-434f-a03d-bb8356b605ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=torch.as_tensor(input_ids),\n",
    "            attention_mask=torch.as_tensor(attention_mask),\n",
    "            global_attention_mask=torch.as_tensor(global_attention_mask),\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59bff4ac-0551-4eb4-b3eb-c7bef357c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f993ff1-9d3f-4e66-89f7-48aa62fe4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f082e6e2-7bc3-45ce-b8dd-a1100e632acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the VR handheld controllers (e.g. Oculus Touch motion controllers). Our approach is flexible because it can be adapted to different hand meshes and it is also easily customizable. Moreover, it enables interaction with different objects regardless their geometries. In order to validate our']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "026fb5e2-ca46-469c-972c-768e73c06a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2933487638578587, recall=0.29200709832037636, fmeasure=0.2815715983753184), mid=Score(precision=0.31049575936025475, recall=0.30798508735871666, fmeasure=0.29432358485593413), high=Score(precision=0.3280951076358564, recall=0.3250701177873674, fmeasure=0.30592633047634554)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.04350232238478724, recall=0.04349789242499366, fmeasure=0.041407873852706695), mid=Score(precision=0.05183538594707471, recall=0.052840695590428914, fmeasure=0.04971452590462587), high=Score(precision=0.06038594419192698, recall=0.06344514610297083, fmeasure=0.058800723232261924)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15518199067772578, recall=0.15768831899322766, fmeasure=0.15026841424341691), mid=Score(precision=0.16485430398692857, recall=0.16904587700574214, fmeasure=0.15823517427974215), high=Score(precision=0.1745321488156862, recall=0.18136510192458355, fmeasure=0.16781062293071153)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.1552374114897623, recall=0.15677383438490985, fmeasure=0.15032775385130467), mid=Score(precision=0.1652847381019507, recall=0.1690507117462037, fmeasure=0.158490594758643), high=Score(precision=0.17454030425769723, recall=0.18160941089779872, fmeasure=0.16678198199626665))}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26fb22-d99e-47ab-b1c2-75e3a045792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeefe6-db00-4dc9-96a3-a6cf23045246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512dd50-d7e1-4f5c-90f2-ded6717a7c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1820-10e4-42d9-b079-292b9ee5c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9075b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36d115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9c7f-ba05-4328-872a-78ac9b4e596c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e2bd306-2b45-4eac-b529-66d448e911a5",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32720f9-1878-4fac-83dc-89b2506f3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "led_output_model1 = model.generate(\n",
    "    **dataset_tokenized[\"test\"],\n",
    "    # input_ids=dataset_tokenized[\"test\"][\"input_ids\"][:n],\n",
    "    # attention_mask=dataset_tokenized[\"test\"][\"attention_mask\"][:n],\n",
    "    # global_attention_mask=dataset_tokenized[\"test\"][\"global_attention_mask\"][:n],\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=128,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6968c4ce-2231-493e-b5d1-8fa8168c1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [3:11:58<00:00, 143.98s/it]\n"
     ]
    }
   ],
   "source": [
    "led_output_model1 = []\n",
    "for i in tqdm(range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE)):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8104ad4-368c-4597-b7fd-870edbc0fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a610a7bb-d9e0-43a6-96bd-f507154efd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5208799d-1f41-4035-8ad3-a0d1e941c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence, that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. â€” II. The model with automobiles as an example, 489. â€” III. Examples and applications, 492. â€” IV.\",\n",
       " 'â€œInteraction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the Oculus Touch motion controllers. Our approach is flexible because it can be adapted to different hand meshes (e-g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "415f1b9a-17e4-451f-a61a-94392acae155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2982484009370646, recall=0.3057877687297647, fmeasure=0.2877549383290699), mid=Score(precision=0.3011000320975565, recall=0.308186529440017, fmeasure=0.28974742885739196), high=Score(precision=0.30423223116720244, recall=0.31058012470155627, fmeasure=0.2917896849338474)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.046860562392848866, recall=0.04742509910853629, fmeasure=0.04476525721466253), mid=Score(precision=0.04805986457733996, recall=0.048611032584090635, fmeasure=0.0458330562332894), high=Score(precision=0.04916571328652274, recall=0.0498130873004002, fmeasure=0.046871122190935706)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.15540080811555254, recall=0.1636225890270709, fmeasure=0.15142010390078578), mid=Score(precision=0.15687893432752667, recall=0.16523845626801453, fmeasure=0.1524776573706927), high=Score(precision=0.15834376408384634, recall=0.16680563034698537, fmeasure=0.15364196168214198)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.15538972251299768, recall=0.16342358445792937, fmeasure=0.15136482317934177), mid=Score(precision=0.15683121294129587, recall=0.16520268561441434, fmeasure=0.15246824909372558), high=Score(precision=0.1583816066756327, recall=0.1666711839189804, fmeasure=0.15354401584643315))}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
