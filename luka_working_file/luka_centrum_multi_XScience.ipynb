{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c480166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rouge_score in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.24.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from nltk->rouge_score) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd2b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: evaluate in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (1.24.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2023.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.12.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: dill in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: pandas in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luka/miniconda/envs/nlp_project/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b598292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f4dbf2-a9d1-4623-a6eb-b7a1a2f04dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff744bc7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3ac697",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa9399",
   "metadata": {},
   "source": [
    "## Load multi_news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6ab210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_news (/Users/luka/.cache/huggingface/datasets/multi_news/default/1.0.0/2f1f69a2bedc8ad1c5d8ae5148e4755ee7095f465c1c01ae8f85454342065a72)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3286ae467f55478884f32c77859cfa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('multi_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382a93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['document', 'summary'],\n",
       " 'validation': ['document', 'summary'],\n",
       " 'test': ['document', 'summary']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb542b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "input_documents = dataset[\"train\"][\"document\"][:n]\n",
    "input_summaries = dataset[\"train\"][\"summary\"][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6078f289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['National Archives \\n \\n Yes, it’s that time again, folks. It’s the first Friday of the month, when for one ever-so-brief moment the interests of Wall Street, Washington and Main Street are all aligned on one thing: Jobs. \\n \\n A fresh update on the U.S. employment situation for January hits the wires at 8:30 a.m. New York time offering one of the most important snapshots on how the economy fared during the previous month. Expectations are for 203,000 new jobs to be created, according to economists polled by Dow Jones Newswires, compared to 227,000 jobs added in February. The unemployment rate is expected to hold steady at 8.3%. \\n \\n Here at MarketBeat HQ, we’ll be offering color commentary before and after the data crosses the wires. Feel free to weigh-in yourself, via the comments section. And while you’re here, why don’t you sign up to follow us on Twitter. \\n \\n Enjoy the show. ||||| Employers pulled back sharply on hiring last month, a reminder that the U.S. economy may not be growing fast enough to sustain robust job growth. The unemployment rate dipped, but mostly because more Americans stopped looking for work. \\n \\n The Labor Department says the economy added 120,000 jobs in March, down from more than 200,000 in each of the previous three months. \\n \\n The unemployment rate fell to 8.2 percent, the lowest since January 2009. The rate dropped because fewer people searched for jobs. The official unemployment tally only includes those seeking work. \\n \\n The economy has added 858,000 jobs since December _ the best four months of hiring in two years. But Federal Reserve Chairman Ben Bernanke has cautioned that the current hiring pace is unlikely to continue without more consumer spending.',\n",
       " 'LOS ANGELES (AP) — In her first interview since the NBA banned her estranged husband, Shelly Sterling says she will fight to keep her share of the Los Angeles Clippers and plans one day to divorce Donald Sterling. \\n \\n (Click Prev or Next to continue viewing images.) \\n \\n ADVERTISEMENT (Click Prev or Next to continue viewing images.) \\n \\n Los Angeles Clippers co-owner Shelly Sterling, below, watches the Clippers play the Oklahoma City Thunder along with her attorney, Pierce O\\'Donnell, in the first half of Game 3 of the Western Conference... (Associated Press) \\n \\n Shelly Sterling spoke to Barbara Walters, and ABC News posted a short story with excerpts from the conversation Sunday. \\n \\n NBA Commissioner Adam Silver has banned Donald Sterling for making racist comments and urged owners to force Sterling to sell the team. Silver added that no decisions had been made about the rest of Sterling\\'s family. \\n \\n According to ABC\\'s story, Shelly Sterling told Walters: \"I will fight that decision.\" \\n \\n Sterling also said that she \"eventually\" will divorce her husband, and that she hadn\\'t yet done so due to financial considerations. ||||| Shelly Sterling said today that \"eventually, I am going to\" divorce her estranged husband, Donald Sterling, and if the NBA tries to force her to sell her half of the Los Angeles Clippers, she would \"absolutely\" fight to keep her stake in the team. \\n \\n \"I will fight that decision,\" she told ABC News\\' Barbara Walters today in an exclusive interview. \"To be honest with you, I\\'m wondering if a wife of one of the owners, and there\\'s 30 owners, did something like that, said those racial slurs, would they oust the husband? Or would they leave the husband in?\" \\n \\n Sterling added that the Clippers franchise is her \"passion\" and \"legacy to my family.\" \\n \\n \"I\\'ve been with the team for 33 years, through the good times and the bad times,\" she added. \\n \\n These comments come nearly two weeks after NBA Commissioner Adam Silver announced a lifetime ban and a $2.5 million fine for Donald Sterling on April 29, following racist comments from the 80-year-old, which were caught on tape and released to the media. \\n \\n Read: Barbara Walters\\' Exclusive Interview With V. Stiviano \\n \\n Being estranged from her husband, Shelly Sterling said she would \"have to accept\" whatever punishment the NBA handed down to him, but that her stake in the team should be separate. \\n \\n \"I was shocked by what he said. And -- well, I guess whatever their decision is -- we have to live with it,\" she said. \"But I don\\'t know why I should be punished for what his actions were.\" \\n \\n An NBA spokesman said this evening that league rules would not allow her tol hold on to her share. \\n \\n \"Under the NBA Constitution, if a controlling owner\\'s interest is terminated by a 3/4 vote, all other team owners\\' interests are automatically terminated as well,\" NBA spokesman Mike Bass said. \"It doesn\\'t matter whether the owners are related as is the case here. These are the rules to which all NBA owners agreed to as a condition of owning their team.\" \\n \\n Sherry Sterling\\'s lawyer, Pierce O\\'Donnell, disputed the league\\'s reading of its constitution. \\n \\n \"We do not agree with the league\\'s self-serving interpretation of its constitution, its application to Shelly Sterling or its validity under these unique circumstances,\" O\\'Donnell said in a statement released this evening in reposnse the NBA. \"We live in a nation of laws. California law and the United States Constitution trump any such interpretation.\" \\n \\n If the league decides to force Donald Sterling to sell his half of the team, Shelly Sterling doesn\\'t know what he will do, but the possibility of him transferring full ownership to her is something she \"would love him to\" consider. \\n \\n Related: NBA Bans Clippers Owner Donald Sterling For Life \\n \\n \"I haven\\'t discussed it with him or talked to him about it,\" she said. \\n \\n The lack of communication between Rochelle and Donald Sterling led Walters to question whether she plans to file for divorce. \\n \\n \"For the last 20 years, I\\'ve been seeing attorneys for a divorce,\" she said, laughing. \"In fact, I have here-- I just filed-- I was going to file the petition. I signed the petition for a divorce. And it came to almost being filed. And then, my financial advisor and my attorney said to me, \\'Not now.\\'\" \\n \\n Sterling added that she thinks the stalling of the divorce stems from \"financial arrangements.\" \\n \\n But she said \"Eventually, I\\'m going to.\" \\n \\n She also told Walters she thinks her estranged husband is suffering from \"the onset of dementia.\" \\n \\n Since Donald Sterling\\'s ban, several celebrities have said they would be willing to buy the team from Sterling, including Oprah Winfrey and Magic Johnson. Sterling remains the owner, though his ban means he can have nothing to do with running the team and can\\'t attend any games. \\n \\n Silver announced Friday that former Citigroup chairman and former Time Warner chairman Richard Parsons has been named interim CEO of the team, but nothing concrete in terms of ownership or whether Sterling will be forced to sell the team. Parsons will now take over the basic daily operations for the team and oversee the team\\'s president. \\n \\n Read: What You Need to Know This Week About Donald Sterling \\n \\n ABC News contacted Donald Sterling for comment on his wife\\'s interview, but he declined.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3c47cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['– The unemployment rate dropped to 8.2% last month, but the economy only added 120,000 jobs, when 203,000 new jobs had been predicted, according to today\\'s jobs report. Reaction on the Wall Street Journal\\'s MarketBeat Blog was swift: \"Woah!!! Bad number.\" The unemployment rate, however, is better news; it had been expected to hold steady at 8.3%. But the AP notes that the dip is mostly due to more Americans giving up on seeking employment.',\n",
       " '– Shelly Sterling plans \"eventually\" to divorce her estranged husband Donald, she tells Barbara Walters at ABC News. As for her stake in the Los Angeles Clippers, she plans to keep it, the AP notes. Sterling says she would \"absolutely\" fight any NBA decision to force her to sell the team. The team is her \"legacy\" to her family, she says. \"To be honest with you, I\\'m wondering if a wife of one of the owners … said those racial slurs, would they oust the husband? Or would they leave the husband in?\"']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_summaries[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95f386",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "First 3 sentences from each news\n",
    "\n",
    "Room of improvements\n",
    "* How to properly get the first 3 sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080b2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_baseline = [\n",
    "    \".\".join([\". \".join(each.split(\". \")[:3]) for each in sequence.split(\"|||||\")]) + \".\"\n",
    "    for sequence in input_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbf08ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['National Archives \\n \\n Yes, it’s that time again, folks. It’s the first Friday of the month, when for one ever-so-brief moment the interests of Wall Street, Washington and Main Street are all aligned on one thing: Jobs. \\n \\n A fresh update on the U.S. Employers pulled back sharply on hiring last month, a reminder that the U.S. economy may not be growing fast enough to sustain robust job growth. The unemployment rate dipped, but mostly because more Americans stopped looking for work.',\n",
       " 'LOS ANGELES (AP) — In her first interview since the NBA banned her estranged husband, Shelly Sterling says she will fight to keep her share of the Los Angeles Clippers and plans one day to divorce Donald Sterling. \\n \\n (Click Prev or Next to continue viewing images.) \\n \\n ADVERTISEMENT (Click Prev or Next to continue viewing images.) \\n \\n Los Angeles Clippers co-owner Shelly Sterling, below, watches the Clippers play the Oklahoma City Thunder along with her attorney, Pierce O\\'Donnell, in the first half of Game 3 of the Western Conference... (Associated Press) \\n \\n Shelly Sterling spoke to Barbara Walters, and ABC News posted a short story with excerpts from the conversation Sunday. Shelly Sterling said today that \"eventually, I am going to\" divorce her estranged husband, Donald Sterling, and if the NBA tries to force her to sell her half of the Los Angeles Clippers, she would \"absolutely\" fight to keep her stake in the team. \\n \\n \"I will fight that decision,\" she told ABC News\\' Barbara Walters today in an exclusive interview. \"To be honest with you, I\\'m wondering if a wife of one of the owners, and there\\'s 30 owners, did something like that, said those racial slurs, would they oust the husband? Or would they leave the husband in?\" \\n \\n Sterling added that the Clippers franchise is her \"passion\" and \"legacy to my family.\" \\n \\n \"I\\'ve been with the team for 33 years, through the good times and the bad times,\" she added.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_baseline[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada0d356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.43601002287852186,\n",
       " 'rouge2': 0.14265791909311992,\n",
       " 'rougeL': 0.19956066282373178,\n",
       " 'rougeLsum': 0.2729012505428157}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=preds_baseline, references=input_summaries, use_stemmer=True\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502e823",
   "metadata": {},
   "source": [
    "# Model 1: Default Centrum\n",
    "\n",
    "Boooo!!!\n",
    "\n",
    "Notes\n",
    "* rouge includes padding in calculation\n",
    "\n",
    "Room of improvements\n",
    "* Trim after end\n",
    "* Weird special characters (Fixed)\n",
    "* max_length?\n",
    "* beam search? (Fixed)\n",
    "* skip repeated n gram (Fixed)\n",
    "\n",
    "tokenizer.pad_token_idtokenizer.pad_token_id?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c34c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\"\n",
    "DOC_SEP_ = \"|||||\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187eb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "748aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(DOC_SEP_, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692a988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(\n",
    "    input_documents,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "546e61df-2d53-440b-9923-9daee1b73415",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"global_attention_mask\"] = [\n",
    "    [\n",
    "        1 if token in (tokenizer.cls_token_id, docsep_token_id) else 0 \n",
    "        for token in each\n",
    "    ]\n",
    "    for each in batch[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c351a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "led_output = model.generate(\n",
    "    **batch,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=512,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d5e4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_model1 = tokenizer.batch_decode(\n",
    "    led_output,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e626f75-d339-473d-ae1b-3fde2bd2eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_model1 = [each.strip() for each in preds_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab787569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The unemployment rate fell to 8.2 percent, the lowest since January 2009. The rate dropped because fewer people searched for jobs. The official unemployment tally only includes those seeking work. The economy has added 858,000 jobs since December — the best four months of hiring in two years. But Federal Reserve Chairman Ben Bernanke has cautioned that the current hiring pace is unlikely to continue without more consumer spending. The unemployment rate is expected to hold steady at 8.3 percent. Here at MarketBeat HQ, we’ll be offering color commentary before and after the data crosses the wires. Feel free to weigh-in yourself, via the comments section. And while you’re here, why don’t you sign up to follow us on Twitter.',\n",
       " 'LOS ANGELES — In her first interview since the NBA banned her estranged husband, Shelly Sterling says she will fight to keep her share of the Los Angeles Clippers and plans one day to divorce Donald Sterling. “I will fight that decision,” she told ABC News’ Barbara Walters today in an exclusive interview. \"To be honest with you, I’m wondering if a wife of one of the owners, and there’s 30 owners, did something like that, said those racial slurs, would they oust the husband? Or would they leave the husband in?” The Clippers franchise is her “passion” and “legacy to my family.” She’d been with the team for 33 years, through the good times and the bad times, she added. The comments come nearly two weeks after NBA Commissioner Adam Silver announced a lifetime ban and a $2.5 million fine for Donald Sterling on April 29, following racist comments from the 80-year-old, which were caught on tape and released to the media. NBA commissioner Adam Silver has banned Donald Sterling for making racist comments and urged owners to force Sterling to sell the team. Silver added that no decisions had been made about the rest of Sterling\\'s family. In her interview with Walters, Sterling also said that she “eventually” will divorce her husband, and that she hadn’t yet done so due to financial considerations. In an interview with Barbara Walters, she also said she would fight the decision to keep the Clippers.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1be7b6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.4221617749790788,\n",
       " 'rouge2': 0.1418855987672803,\n",
       " 'rougeL': 0.20409749797205406,\n",
       " 'rougeLsum': 0.20413145617293332}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=preds_model1, references=input_summaries, use_stemmer=True\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34e504-11ca-47de-bc88-9694cb61afa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ea4333-9dc0-4384-a2cd-c715b132b1d7",
   "metadata": {},
   "source": [
    "# Multi XScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1ee7c-5e35-45c4-8082-46c5ee7a5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cf2b8a-046a-4db6-907c-789a2dd50c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c647b3-567f-4e53-9167-f54ab731c72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8882d8a6-5dd4-4bd9-8599-c15840ef1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"multi_x_science_sum\"\n",
    "DOC_SEP = \" ||||| \"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20920814-72af-4d58-91bd-aa7dcccd5160",
   "metadata": {},
   "source": [
    "## Set up evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d1a000-8e57-4144-9253-2779a6dc7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156f359-5379-4d2b-8b24-51563c9a247f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17fce3f-f4ff-4ac9-93c9-4a33463d32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_x_science_sum (/Users/luka/.cache/huggingface/datasets/multi_x_science_sum/default/1.1.0/2876ec0401f8f5c5acf7f4857dbc8d6229a390ab428321ab848f03f14b7f9729)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf316e35a443f5a588f3ad27693e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbc5af7-4c41-4d9a-9ac2-fb7a8842d417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 30369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5093\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['aid', 'mid', 'abstract', 'related_work', 'ref_abstract'],\n",
       "        num_rows: 5066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106f88f2-3ce7-44b4-ad41-998a39db1ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid': 'math9912167',\n",
       " 'mid': '1631980677',\n",
       " 'abstract': 'Author(s): Kuperberg, Greg; Thurston, Dylan P. | Abstract: We give a purely topological definition of the perturbative quantum invariants of links and 3-manifolds associated with Chern-Simons field theory. Our definition is as close as possible to one given by Kontsevich. We will also establish some basic properties of these invariants, in particular that they are universally finite type with respect to algebraically split surgery and with respect to Torelli surgery. Torelli surgery is a mutual generalization of blink surgery of Garoufalidis and Levine and clasper surgery of Habiro.',\n",
       " 'related_work': 'Two other generalizations that can be considered are invariants of graphs in 3-manifolds, and invariants associated to other flat connections @cite_16 . We will analyze these in future work. Among other things, there should be a general relation between flat bundles and links in 3-manifolds on the one hand and finite covers and branched covers on the other hand @cite_26 .',\n",
       " 'ref_abstract': {'cite_N': ['@cite_16', '@cite_26'],\n",
       "  'mid': ['1481005306', '1641082372'],\n",
       "  'abstract': ['This note is a sequel to our earlier paper of the same title [4] and describes invariants of rational homology 3-spheres associated to acyclic orthogonal local systems. Our work is in the spirit of the Axelrod–Singer papers [1], generalizes some of their results, and furnishes a new setting for the purely topological implications of their work.',\n",
       "   'Recently, Mullins calculated the Casson-Walker invariant of the 2-fold cyclic branched cover of an oriented link in S^3 in terms of its Jones polynomial and its signature, under the assumption that the 2-fold branched cover is a rational homology 3-sphere. Using elementary principles, we provide a similar calculation for the general case. In addition, we calculate the LMO invariant of the p-fold branched cover of twisted knots in S^3 in terms of the Kontsevich integral of the knot.']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a24ed-be47-4064-b363-bf872caee6de",
   "metadata": {},
   "source": [
    "## Format dataset to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57be3155-bad5-46d2-953c-3469e2ec2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(\"@cite_[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e9e334-0a0b-4283-b68a-af7bfc46fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = (\n",
    "        example[\"abstract\"].split(\"| Abstract: \")[-1]\n",
    "        + DOC_SEP\n",
    "        + DOC_SEP.join([x for x in example[\"ref_abstract\"][\"abstract\"] if x])\n",
    "    )\n",
    "    output[\"related_work\"] = pat.sub(\"@cite\", example[\"related_work\"])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44487cf5-f19d-4d85-a28e-db276fb87df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_batched(example):\n",
    "    output = {}\n",
    "    output[\"abstracts\"] = []\n",
    "    output[\"related_work\"] = []\n",
    "    \n",
    "    for abstract, ref_abstract in zip(\n",
    "        example[\"abstract\"], example[\"ref_abstract\"]\n",
    "    ):\n",
    "        output[\"abstracts\"].append(\n",
    "            abstract.split(\"| Abstract: \")[-1]\n",
    "            + DOC_SEP\n",
    "            + DOC_SEP.join([x for x in ref_abstract[\"abstract\"] if x])\n",
    "        )\n",
    "    for related_work in example[\"related_work\"]:\n",
    "        output[\"related_work\"].append(pat.sub(\"@cite\", related_work))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c94c5ed-4ae3-4940-8523-d988860354ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30369 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_processed = {}\n",
    "for split in dataset.keys():\n",
    "    dataset_processed[split] = dataset[split].map(\n",
    "        # preprocess_dataset,\n",
    "        preprocess_dataset_batched,\n",
    "        remove_columns=dataset[split].column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c506e5-2b6b-4171-9cc9-64eeb7f1cc7e",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "First 3 sentences from each news\n",
    "\n",
    "Room of improvements\n",
    "* How to properly get the first 3 sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd5ba2d-16e2-4486-a779-6b67b2264663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['related_work', 'abstracts'],\n",
       "    num_rows: 5093\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a5bc89-c337-4188-8d4b-b146bef06631",
   "metadata": {},
   "outputs": [],
   "source": [
    "punkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7810cc55-3a75-43f0-8930-4600eea138f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_baselines = [\n",
    "    \" \".join(\n",
    "        [\n",
    "            \" \".join(punkt_tokenizer.tokenize(each)[:2]).strip()\n",
    "            for each in element[\"abstracts\"].split(\"|||||\")\n",
    "        ]\n",
    "    ).strip()\n",
    "    for element in dataset_processed[\"test\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d6cc54-e94f-4f49-9ecb-ebad641fcef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.29248682507658086,\n",
       " 'rouge2': 0.054016188777851686,\n",
       " 'rougeL': 0.14639129491285108,\n",
       " 'rougeLsum': 0.1463775641142901}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=test_pred_baselines,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab3779-c46c-4e93-bdbf-8dc682f1bca8",
   "metadata": {},
   "source": [
    "# Model 1: Default Centrum\n",
    "\n",
    "* Probably need to figure out the distribution of `dataset_processed[\"test\"][\"abstracts\"]` so that we can estimate the best `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "328b32ec-684e-4756-94f4-2ee1b01e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"ratishsp/Centrum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4d3644-e019-42a3-b375-2ff7636ed94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d4aae9-cdfa-4e85-b494-616ba2f336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(DOC_SEP, special_tokens=True)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "docsep_token_id = tokenizer.convert_tokens_to_ids(DOC_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3acc1cce-7af5-43a4-8bb7-8994bb88bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = {}\n",
    "\n",
    "dataset_tokenized[\"test\"] = tokenizer(\n",
    "    dataset_processed[\"test\"][\"abstracts\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c84fa5fc-415b-4103-a5b1-a01cf2d570bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized[\"test\"][\"global_attention_mask\"] = np.array([\n",
    "    [\n",
    "        1 if token in (tokenizer.cls_token_id, docsep_token_id) else 0 \n",
    "        for token in each\n",
    "    ]\n",
    "    for each in dataset_tokenized[\"test\"][\"input_ids\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c00845a7-0df2-46a0-ba63-6cd5714a764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eae6bf4d-e55e-4c52-9da3-9b4477010af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6968c4ce-2231-493e-b5d1-8fa8168c1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample 0.\n",
      "Time elapsed: 74.9034059047699s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m dataset_tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m      9\u001b[0m global_attention_mask \u001b[38;5;241m=\u001b[39m dataset_tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m     11\u001b[0m led_output_model1\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m BATCH_SIZE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/generation/utils.py:2722\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2722\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2730\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:2436\u001b[0m, in \u001b[0;36mLEDForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2432\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2433\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2434\u001b[0m         )\n\u001b[0;32m-> 2436\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mled\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2437\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2447\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2449\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2454\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   2456\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:2300\u001b[0m, in \u001b[0;36mLEDModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, global_attention_mask, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2292\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m LEDEncoderBaseModelOutput(\n\u001b[1;32m   2293\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   2294\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2295\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2296\u001b[0m         global_attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2297\u001b[0m     )\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2300\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2301\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2308\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2309\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2311\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:2163\u001b[0m, in \u001b[0;36mLEDDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   2153\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   2154\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2161\u001b[0m     )\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:1084\u001b[0m, in \u001b[0;36mLEDDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m   1093\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:855\u001b[0m, in \u001b[0;36mLEDDecoderAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    852\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cross_attention:\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# cross_attentions\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(key_value_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp_project/lib/python3.8/site-packages/transformers/models/led/modeling_led.py:828\u001b[0m, in \u001b[0;36mLEDDecoderAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "led_output_model1 = []\n",
    "# for i in range(0, len(dataset_tokenized[\"test\"][\"input_ids\"]), BATCH_SIZE):\n",
    "for i in range(0, 100, BATCH_SIZE):\n",
    "    \n",
    "    input_ids = dataset_tokenized[\"test\"][\"input_ids\"][i:i+BATCH_SIZE]\n",
    "    attention_mask = dataset_tokenized[\"test\"][\"attention_mask\"][i:i+BATCH_SIZE]\n",
    "    global_attention_mask = dataset_tokenized[\"test\"][\"global_attention_mask\"][i:i+BATCH_SIZE]\n",
    "\n",
    "    led_output_model1.append(\n",
    "        model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask,\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if i % BATCH_SIZE == 0:\n",
    "        print(f\"Generating sample {i}.\")\n",
    "        print(f\"Time elapsed: {time.time() - start_time}s\")\n",
    "        \n",
    "led_output_model1 = torch.cat(led_output_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8104ad4-368c-4597-b7fd-870edbc0fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    torch.cat(led_output_model1),\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a610a7bb-d9e0-43a6-96bd-f507154efd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5208799d-1f41-4035-8ad3-a0d1e941c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of Intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice. The article presents experimental results illustrating the agents' dynamic behavior. I. Introduction, 488. — II. The model with automobiles as an example, 489. — III. Examples and applications, 492. — IV. Counter\",\n",
       " '“Interaction in virtual reality (VR) environments (e.g. grasping and manipulating virtual objects) is essential to ensure a pleasant and immersive experience. In this work, we propose a visually realistic, flexible and robust grasping system that enables real-time interactions in virtual environments. Resulting grasps are visually realistic because hand is automatically fitted to the object shape from a position and orientation determined by the user using the Oculus Touch motion controllers. Our approach is flexible because it can be adapted to different hand meshes (e-g. human or robotic hands) and it is also easily customizable. Moreover, it enables interaction with']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "415f1b9a-17e4-451f-a61a-94392acae155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.29331846458195443,\n",
       " 'rouge2': 0.050375425158183273,\n",
       " 'rougeL': 0.15445020608642712,\n",
       " 'rougeLsum': 0.15443973495826438}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"][:128],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196c644-b949-448c-847d-3adad17f5ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeefe6-db00-4dc9-96a3-a6cf23045246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512dd50-d7e1-4f5c-90f2-ded6717a7c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1820-10e4-42d9-b079-292b9ee5c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32720f9-1878-4fac-83dc-89b2506f3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "led_output_model1 = model.generate(\n",
    "    **dataset_tokenized[\"test\"],\n",
    "    # input_ids=dataset_tokenized[\"test\"][\"input_ids\"][:n],\n",
    "    # attention_mask=dataset_tokenized[\"test\"][\"attention_mask\"][:n],\n",
    "    # global_attention_mask=dataset_tokenized[\"test\"][\"global_attention_mask\"][:n],\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=128,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0cf01-daf9-4c65-b181-f603f7ab18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = tokenizer.batch_decode(\n",
    "    led_output_model1,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7a908-5932-44ba-b98e-1f632f2c4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1 = [each.strip() for each in test_pred_model1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda02cb-2dc3-4b70-afd8-1afea54ca09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_model1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610ce66-aef8-4838-934f-0b7bc99956c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=test_pred_model1,\n",
    "    references=dataset_processed[\"test\"][\"related_work\"],\n",
    "    use_stemmer=True,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61600e50-f3e4-49d8-b70b-a0ba42839bf3",
   "metadata": {},
   "source": [
    "# Model 1: Fine-tune Centrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143abd45-2e28-4c48-be98-35a8f615a244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c9c7f-ba05-4328-872a-78ac9b4e596c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d765c-4f93-4e85-8673-6241b4c8acc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
