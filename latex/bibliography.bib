@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer},
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{puduppully2022multidocument,
      title={Multi-Document Summarization with Centroid-Based Pretraining},
      author={Ratish Puduppully and Mark Steedman},
      year={2022},
      eprint={2208.01006},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lu-etal-2020-multi-xscience,
    title = "Multi-{XS}cience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles",
    author = "Lu, Yao  and
      Dong, Yue  and
      Charlin, Laurent",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.648",
    doi = "10.18653/v1/2020.emnlp-main.648",
    pages = "8068--8074",
    abstract = "Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results{---}using several state-of-the-art models trained on the Multi-XScience dataset{---}reveal that Multi-XScience is well suited for abstractive models.",
}

@misc{xiao2022primera,
      title={PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization},
      author={Wen Xiao and Iz Beltagy and Giuseppe Carenini and Arman Cohan},
      year={2022},
      eprint={2110.08499},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{fabbri-etal-2019-multi,
    title = "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    author = "Fabbri, Alexander  and
      Li, Irene  and
      She, Tianwei  and
      Li, Suyi  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1102",
    doi = "10.18653/v1/P19-1102",
    pages = "1074--1084",
    abstract = "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.",
}

@misc{liu2018generating,
      title={Generating Wikipedia by Summarizing Long Sequences},
      author={Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
      year={2018},
      eprint={1801.10198},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
